{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af79f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-30 16:09:27.292564: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14d9c3ef0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch import nn,optim\n",
    "import torch\n",
    "\n",
    "import algo\n",
    "import json \n",
    "import attack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8747262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  ../datasets/dataset_purchase\n",
      "Classes in classification task:  100\n"
     ]
    }
   ],
   "source": [
    "raw_data_path = '../datasets/dataset_purchase'\n",
    "raw_data = pd.read_csv(raw_data_path)\n",
    "y=raw_data['63']\n",
    "X_raw =raw_data.drop('63', axis=1)\n",
    "y_raw =  y.replace(100, 0)\n",
    "print('Dataset: ', raw_data_path)\n",
    "print('Classes in classification task: ', y.nunique())\n",
    "n_classes = y.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd768ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39464, 600) (157859, 600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/0p0lnj8d6zj532q0yvm1zt580000gn/T/ipykernel_52662/3339399080.py:8: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_target_train = np.array(y_train[:X_train_size])\n",
      "/var/folders/66/0p0lnj8d6zj532q0yvm1zt580000gn/T/ipykernel_52662/3339399080.py:10: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_target_test = np.array(y_train[X_train_size:X_train_size+X_test_size])\n"
     ]
    }
   ],
   "source": [
    "X_train, x_shadow, y_train, y_shadow = train_test_split(X_raw, y_raw, train_size=0.2, random_state=rand_seed)\n",
    "print(X_train.shape, x_shadow.shape)\n",
    "\n",
    "#Target model\n",
    "X_train_size = 10000\n",
    "X_test_size = 10000\n",
    "x_target_train = np.array(X_train[:X_train_size])\n",
    "y_target_train = np.array(y_train[:X_train_size])\n",
    "x_target_test = np.array(X_train[X_train_size:X_train_size+X_test_size])\n",
    "y_target_test = np.array(y_train[X_train_size:X_train_size+X_test_size])\n",
    "if y_target_test.shape[0]<X_test_size or y_target_train.shape[0]<X_train_size:\n",
    "    raise ValueError(\n",
    "            \"Not enough traning or test data for the target model\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e45007",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed=2\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "\n",
    "model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "model.n_classes      = n_classes\n",
    "model.alpha          = 0.001\n",
    "model.max_iter       = 100\n",
    "model.lambda_        = 1e-5\n",
    "model.tolerance      = 1e-5\n",
    "model.DP             = False\n",
    "\n",
    "X,y = model.init_theta(x_target_train, y_target_train)\n",
    "model.train(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8666f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_acc = model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "model.test_acc = model.evaluate(x_target_test, y_target_test, acc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e36771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = dict(model.__dict__)\n",
    "model_dict['rand_seed'] = rand_seed\n",
    "model_dict.pop('theta', None)\n",
    "model_dict.pop('pred_func', None)\n",
    "file_name = 'out_dp/baseline_tm'+''.join([str(i)+'_' for i in model_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(file_name + 'model', model.theta)\n",
    "with open(file_name + 'params.json', 'w') as file:\n",
    "    json.dump(model_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_model = algo.LogisticRegression_DPSGD()\n",
    "noise_model.DP_out = True\n",
    "noise_model.epsilon_out = 10\n",
    "noise_model.delta_out = 10e-5\n",
    "noise_model.lambda_ = 1e-1\n",
    "noise_model.sensitivity = 2/(X_train_size*model.lambda_)  #lambda=.0001\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4251f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_params(model, params):\n",
    "\n",
    "    model.n_classes      = params['n_classes']\n",
    "    model.alpha          = params['alpha']\n",
    "    model.max_iter       = params['max_iter']\n",
    "    model.lambda_        = params['lambda_']\n",
    "    model.tolerance      = params['tolerance']\n",
    "    model.DP             = params['DP']\n",
    "    model.L              = params['L']\n",
    "    model.C              = params['C']\n",
    "    model.epsilon        = params['epsilon']\n",
    "    model.delta          = params['delta']\n",
    "\n",
    "def output_DP(model,  X_train_size, epsilon_out_DP, delta_out_DP=1e-5):\n",
    "    #gaussian mechanism \n",
    "    noise_model = algo.LogisticRegression_DPSGD()\n",
    "    set_model_params(noise_model, target_model.__dict__)\n",
    "    \n",
    "    noise_model.DP_out = True\n",
    "    noise_model.epsilon_out = epsilon_out_DP\n",
    "    noise_model.delta_out = delta_out_DP\n",
    "    noise_model.sensitivity = 2/(X_train_size*model.lambda_) \n",
    "\n",
    "    sigma = np.sqrt(2 * np.log(1.25 / noise_model.delta_out)) * (noise_model.sensitivity / noise_model.epsilon_out)\n",
    "\n",
    "    ns=np.random.normal(loc=0.0, scale=sigma, size=model.theta.shape)\n",
    "\n",
    "    noise_model.theta = model.theta + ns\n",
    "    \n",
    "    return noise_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c912b04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model : 82.69999999999999 %\n",
      "The accuracy of the model : 52.6 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5255"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'tm/rs42_lr0.001_iter100_reg0.005_DPFalse_target_model_params.json'\n",
    "with open(file) as json_file:\n",
    "    tm_params = json.load(json_file)\n",
    "            \n",
    "x_target_train, y_target_train, x_target_test, y_target_test = attack.data_shuffle(rand_seed, X_raw, y_raw)\n",
    "target_model = algo.LogisticRegression_DPSGD()\n",
    "scripts.set_model_params(target_model, tm_params)\n",
    "target_model.theta = np.load('tm/rs42_lr0.001_iter100_reg0.005_DPFalse_target_model.npy')\n",
    "target_model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "target_model.evaluate(x_target_test, y_target_test, acc=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b41b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "452d2d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model : 77.5 %\n",
      "The accuracy of the model : 48.6 %\n"
     ]
    }
   ],
   "source": [
    "noise_model = scripts.output_DP(target_model, epsilon_out_DP=10, delta_out_DP=1e-5, X_train_size=X_train_size)\n",
    "noise_model.train_acc = noise_model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "noise_model.test_acc = noise_model.evaluate(x_target_test, y_target_test, acc=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b5a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "target_model.evaluate(x_target_test, y_target_test, acc=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc266343",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_model = algo.LogisticRegression_DPSGD()\n",
    "noise_model.DP_out = True\n",
    "noise_model.epsilon_out = 10\n",
    "noise_model.delta_out = 10e-5\n",
    "noise_model.lambda_ = 1e-1\n",
    "noise_model.sensitivity = 2/(X_train_size*model.lambda_)  #lambda=.0001\n",
    "    \n",
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "\n",
    "sigma = np.sqrt(2 * np.log(1.25 / noise_model.delta_out)) * (noise_model.sensitivity / noise_model.epsilon_out)\n",
    "\n",
    "ns=np.random.normal(loc=0.0, scale=sigma, size=model.theta.shape)\n",
    "\n",
    "noise_model.theta = model.theta + ns\n",
    "noise_model.train_acc = noise_model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "noise_model.test_acc = noise_model.evaluate(x_target_test, y_target_test, acc=True)\n",
    "\n",
    "ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = dict(noise_model.__dict__)\n",
    "new_dict['rand_seed'] = rand_seed\n",
    "new_dict.pop('theta', None)\n",
    "file_name = 'out_dp/output_dp'+''.join([str(i)+'_' for i in new_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(file_name + 'model', model.theta)\n",
    "with open(file_name + 'params.json', 'w') as file:\n",
    "    json.dump(new_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b9774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_eps = [0.1, 0.5, 1, 5, 10, 50, 100,500, 1000, 5000, 10000]\n",
    "out_eps = [0.001, 0.005, 0.1, 0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,5,10,50,100,200,500,1000]\n",
    "lambdas = [1000, 100, 10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "for rand_seed in [2, 4, 23]:\n",
    "    for eps in out_eps:\n",
    "        for lam in lambdas:\n",
    "            params_list.append((rand_seed,eps, lam))\n",
    "            \n",
    "for rand_seed, eps, lam in params_list:\n",
    "\n",
    "    np.random.seed(rand_seed)\n",
    "    torch.manual_seed(rand_seed)\n",
    "\n",
    "    noise_model = algo.LogisticRegression_DPSGD()\n",
    "    noise_model.DP_out = True\n",
    "    noise_model.epsilon_out = eps\n",
    "    noise_model.delta_out = 10e-5\n",
    "    noise_model.lambda_ = lam\n",
    "    noise_model.sensitivity = 2/(X_train_size*noise_model.lambda_)  #lambda=.0001\n",
    "     \n",
    "    #gaussian mechanism \n",
    "    sigma = np.sqrt(2 * np.log(1.25 / noise_model.delta_out)) * (noise_model.sensitivity / noise_model.epsilon_out)\n",
    "    ns=np.random.normal(loc=0.0, scale=sigma, size=model.theta.shape)\n",
    "\n",
    "    noise_model.theta = model.theta + ns\n",
    "    noise_model.train_acc = noise_model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "    noise_model.test_acc = noise_model.evaluate(x_target_test, y_target_test, acc=True)\n",
    "\n",
    "    new_dict = dict(noise_model.__dict__)\n",
    "    new_dict['rand_seed'] = rand_seed\n",
    "    new_dict.pop('theta', None)\n",
    "    file_name = 'out_dp/output_dp'+''.join([str(i)+'_' for i in new_dict.values()])\n",
    "    \n",
    "    np.save(file_name + 'model', model.theta)\n",
    "    with open(file_name + 'params.json', 'w') as file:\n",
    "        json.dump(new_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de2ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_eps = [0.1, 0.5, 1, 5, 10, 50, 100,500, 1000, 5000, 10000]\n",
    "lambdas = [1000, 100, 10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "params_list = []\n",
    "for rand_seed in [1,13, 42]:\n",
    "    for eps in out_eps:\n",
    "        for lam in lambdas:\n",
    "            params_list.append((rand_seed,eps, lam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c8c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rand_seed, eps, lam in params_list:\n",
    "    print(rand_seed, eps, lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68936b26",
   "metadata": {},
   "source": [
    "# Output DP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7246961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import algo\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ff1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'tm'\n",
    "tms_params = {}\n",
    "tms = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if 'DPFalse' in file and 'lr0.001_iter100_' in file:\n",
    "            if '.npy' in file:\n",
    "                tms[file.replace('.npy','')] = np.load(path+'/'+file)\n",
    "            elif '.json' in file:\n",
    "                with open(path+'/'+file) as json_file:\n",
    "                    tms_params[file.replace('_params.json','')]= json.load(json_file)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8bd5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(tms_params, orient='index')\n",
    "df.reset_index(inplace=True)\n",
    "df = df.rename(columns = {'index':'file_name'})\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f7cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_res = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5dcaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed=42 \n",
    "\n",
    "rs_res[rand_seed] = []\n",
    "for i in tms:\n",
    "    if 'rs'+str(rand_seed)+'_' in i:\n",
    "        rs_res[rand_seed].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "\n",
    "X_train, x_shadow, y_train, y_shadow = train_test_split(X_raw, y_raw, train_size=0.2, random_state=rand_seed)\n",
    "\n",
    "#Target model\n",
    "X_train_size = 10000\n",
    "X_test_size = 10000\n",
    "x_target_train = np.array(X_train[:X_train_size])\n",
    "y_target_train = np.array(y_train[:X_train_size])\n",
    "x_target_test = np.array(X_train[X_train_size:X_train_size+X_test_size])\n",
    "y_target_test = np.array(y_train[X_train_size:X_train_size+X_test_size])\n",
    "if y_target_test.shape[0]<X_test_size or y_target_train.shape[0]<X_train_size:\n",
    "    raise ValueError(\n",
    "            \"Not enough traning or test data for the target model\")        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c845924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in rs_res[rand_seed]:\n",
    "#     model = algo.LogisticRegression_DPSGD()\n",
    "#     model.n_classes  = n_classes\n",
    "\n",
    "#     model.theta = tms[i]\n",
    "#     model.train_acc = model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "#     model.test_acc = model.evaluate(x_target_test, y_target_test, acc=True)\n",
    "# #     print(df.loc[df['file_name']==i]['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334a594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_eps = [0.1, 0.5, 1, 5, 10, 50, 100,500, 1000]\n",
    "# out_eps = [0.1, 0.3, 0.5, 0.7, 1, 5, 10, 30 , 50 , 70, 100] \n",
    "out_eps = [i/10 for i in range(1,10)] + [i for i in range(1,11)] + [i*10 for i in range(2,11)] + [i*100 for i in range(2,11)]+ [i*1000 for i in range(2,11)] + [i*10000 for i in range(2,11)]\n",
    "model = {}\n",
    "noised_models = {}\n",
    "results_test = {}\n",
    "results_train = {}\n",
    "output_dp_res = {}\n",
    "for key in rs_res[rand_seed]:\n",
    "\n",
    "    model['theta'] = tms[key]\n",
    "    model['lambda_'] = df.loc[df['file_name']==key]['lambda_'].item()\n",
    "\n",
    "    results_test[model['lambda_']] = []\n",
    "    results_train[model['lambda_']] = []\n",
    "    noised_models[model['lambda_']] = {}\n",
    "    \n",
    "    for eps in out_eps:\n",
    "        \n",
    "        if os.path.exists(f'out_dp/{key}_outEps{eps}params.json'):\n",
    "            continue\n",
    "        \n",
    "        noise_model = algo.LogisticRegression_DPSGD()\n",
    "        noise_model.DP_out = True\n",
    "        noise_model.epsilon_out = eps\n",
    "        noise_model.delta_out = 10e-5\n",
    "        noise_model.sensitivity = 2/(X_train_size*model['lambda_'])  #lambda=.0001\n",
    "\n",
    "        #gaussian mechanism \n",
    "        sigma = np.sqrt(2 * np.log(1.25 / noise_model.delta_out)) * (noise_model.sensitivity / noise_model.epsilon_out)\n",
    "        ns=np.random.normal(loc=0.0, scale=sigma, size=model['theta'].shape)\n",
    "\n",
    "        noise_model.theta = model['theta'] + ns\n",
    "        noised_models[model['lambda_']][eps] = noise_model.theta\n",
    "        output_dp_res[f'{key}_outEps{eps}'] = tms_params[key]\n",
    "        output_dp_res[f'{key}_outEps{eps}']['outDP_epsilon'] = eps\n",
    "        noise_model.train_acc = noise_model.evaluate(x_target_train, y_target_train, acc=False)\n",
    "        noise_model.test_acc = noise_model.evaluate(x_target_test, y_target_test, acc=False)\n",
    "        \n",
    "        output_dp_res[f'{key}_outEps{eps}']['outDP_train_acc'] = noise_model.train_acc\n",
    "        output_dp_res[f'{key}_outEps{eps}']['outDP_test_acc'] = noise_model.test_acc\n",
    "        \n",
    "        results_train[model['lambda_']].append(noise_model.train_acc)\n",
    "        results_test[model['lambda_']].append(noise_model.test_acc)\n",
    "        \n",
    "        np.save(f'out_dp/{key}_outEps{eps}', noise_model.theta)\n",
    "        with open(f'out_dp/{key}_outEps{eps}params.json', 'w') as file:\n",
    "            json.dump(output_dp_res[f'{key}_outEps{eps}'], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acf1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_keys = list(results_test.keys())\n",
    "sort_keys.sort()\n",
    "sort_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e328e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2 reg and target model accuracy with output DP\n",
    "\n",
    "for key in sort_keys:\n",
    "    \n",
    "    if max(results_test[key])>0.:\n",
    "        print(key)\n",
    "        plt.plot(out_eps, results_test[key], label='l2reg: '+str(key))\n",
    "#         print(lambda_res['epsilon_out'])\n",
    "#         plt.errorbar(lambda_res['epsilon_out'], lambda_res['train_acc_mean'], lambda_res['train_acc_std'], label='lam'+str(lam)+' train')  \n",
    "#         plt.errorbar(lambda_res['epsilon_out'], lambda_res['test_acc_mean'], lambda_res['test_acc_std'], label='lam'+str(lam)+' test')\n",
    "        plt.fill_between(out_eps, results_train[key], results_test[key], alpha=0.4,) #label='lam'+str(lam))\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.xlabel('epsilon')\n",
    "# plt.ylabel('accuracy')   \n",
    "# plt.savefig('figures/outputDP_train_test_lambda_epsilon100_all.png')\n",
    "plt.plot([out_eps[0],out_eps[-1]], [0.56, 0.56], label='noDP baseline')\n",
    "\n",
    "plt.legend(loc='lower right', bbox_to_anchor=(0.27, 0.6), fontsize=8)\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('accuracy')   \n",
    "plt.xscale('log')\n",
    "# plt.title('Target model accuracy in output DP centralised with different l2 reg')\n",
    "# plt.savefig('figures/outputDP_l2_best_target_model_epsilon_labda.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32abfb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save = {}\n",
    "for key in sort_keys:\n",
    "    \n",
    "    data_save['epsilon'] = out_eps\n",
    "    data_save[f'{key}l2_train_acc'] = results_train[key]\n",
    "    data_save[f'{key}l2_test_acc'] = results_test[key]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5193915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data_save['epsilon'] \n",
    "# d = pd.DataFrame.from_dict(data_save)\n",
    "# # d.index = d.index.rename('epsilon')\n",
    "# d.to_csv('figures/outputDP_target_model_epsilon_labda.csv')\n",
    "# # d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e75b50d",
   "metadata": {},
   "source": [
    "# FL output DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f5ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'fl'\n",
    "params = {}\n",
    "results = {}\n",
    "models = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \"params.json\" in file:\n",
    "            with open(r+'/'+file) as json_file:\n",
    "                params[r] = json.load(json_file)\n",
    "        elif \"results.csv\" in file:\n",
    "            results[r] = pd.read_csv(r+'/'+file)\n",
    "            it = []\n",
    "            client = []\n",
    "            for k in results[r]['Unnamed: 0']:\n",
    "                it.append(k[k.find('i')+1:k.find('_')])\n",
    "                client.append(k[k.find('_')+1:])\n",
    "            results[r]['it'] = it\n",
    "            results[r]['client'] = client\n",
    "        elif '.npy' in file:\n",
    "            if r not in models:\n",
    "                models[r] = {}\n",
    "            models[r][file] = np.load(r+'/'+file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f6f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'fl/rs42_ncl2_fiter5_lr0.01_iter50_reg0.005_DPFalse'\n",
    "# file = 'fl/rs42_ncl4_fiter5_lr0.01_iter50_reg1e-05_DPFalse'\n",
    "# file = 'fl/rs42_ncl8_fiter5_lr0.001_iter300_reg1e-05_DPFalse'\n",
    "nodes = int(file[file.find('_ncl')+4:file.find('_fiter')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2fecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_models = {}\n",
    "for i in models[file]:\n",
    "    if 'i0_' in i:\n",
    "        fl_models[i[i.find('i0_c')+4:i.find('.npy')]] = models[file][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ca10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for m in fl_models:\n",
    "    model = algo.LogisticRegression_DPSGD()\n",
    "    model.theta = fl_models[m]\n",
    "    model.test_acc = model.evaluate(x_target_test, y_target_test, acc=True)\n",
    "    nodp_global = model.test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a5c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b744259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_eps = [i/10 for i in range(1,10)] + [i for i in range(1,11)] #+ [i*10 for i in range(2,11)] \n",
    "\n",
    "fl_noised_eps = {}\n",
    "for eps in out_eps :\n",
    "    fl_noised_eps[eps] = {}\n",
    "    for m in fl_models:\n",
    "        model = algo.LogisticRegression_DPSGD()\n",
    "        model.theta = fl_models[m]\n",
    "\n",
    "        noise_model = algo.LogisticRegression_DPSGD()\n",
    "        noise_model.DP_out = True\n",
    "        noise_model.epsilon_out = eps\n",
    "        noise_model.delta_out = 10e-5\n",
    "        noise_model.sensitivity = 2/(X_train_size*.005)#params[file]['lambda_'])  #lambda=.005\n",
    "\n",
    "        #gaussian mechanism \n",
    "        sigma = np.sqrt(2 * np.log(1.25 / noise_model.delta_out)) * (noise_model.sensitivity / noise_model.epsilon_out)\n",
    "        ns=np.random.normal(loc=0.0, scale=sigma, size=model.theta.shape)\n",
    "\n",
    "        noise_model.theta = model.theta + ns\n",
    "    #     noise_model.train_acc = noise_model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "        noise_model.test_acc = noise_model.evaluate(x_target_test, y_target_test)\n",
    "        fl_noised_eps[eps][m] = noise_model\n",
    "\n",
    "    gm = fl_noised_eps[eps]['0'].theta\n",
    "    for n, m in enumerate(fl_noised_eps[eps]):\n",
    "        if m != '0' and m != 'g':\n",
    "            gm += fl_noised_eps[eps][m].theta\n",
    "\n",
    "    fl_noised_eps[eps]['g'] = algo.LogisticRegression_DPSGD()\n",
    "    fl_noised_eps[eps]['g'].theta = gm/n\n",
    "    fl_noised_eps[eps]['g'].test_acc = fl_noised_eps[eps]['g'].evaluate(x_target_test, y_target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for eps in fl_noised_eps:\n",
    "    results[eps] = []\n",
    "    for m in fl_noised_eps[eps]:\n",
    "        results[eps].append(fl_noised_eps[eps][m].test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f10b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([out_eps[0],out_eps[-1]], [0.56, 0.56], label='noDP centr baseline')\n",
    "plt.plot([out_eps[0],out_eps[-1]], [ nodp_global,  nodp_global], label='noDP fl global baseline')\n",
    "\n",
    "data = pd.DataFrame.from_dict(results, orient='index')\n",
    "for i, m in enumerate(fl_noised_eps[eps]):\n",
    "    plt.plot(data.index, data[i], label=f'client {m}')\n",
    "\n",
    "plt.legend(loc='lower right', bbox_to_anchor=(1, 0), fontsize=8)\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('accuracy')   \n",
    "plt.title(f'Output DP Federated {nodes} nodes')\n",
    "# plt.savefig(f'figures/fl/outputDP_{nodes}nodes.png')\n",
    "# data.to_csv(f'figures/fl/outputDP_{nodes}nodes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class Net_attack(nn.Module):\n",
    "\n",
    "    def __init__(self, h_neurons, do, input_size):\n",
    "        super(Net_attack, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.h_neurons = h_neurons\n",
    "        self.do = do\n",
    "        self.fc1 = nn.Linear(input_size, h_neurons)\n",
    "        self.fc2 = nn.Linear(h_neurons, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(do)\n",
    "        self.softmax = nn.Softmax(dim=1)   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "def test_mi_attack(attack_models, target_model, x_target_train, y_target_train, x_target_test, y_target_test):\n",
    "    \n",
    "    results = {}\n",
    "    at_acc = []\n",
    "    at_rec = []\n",
    "    at_pre = []\n",
    "    for am in attack_models:\n",
    "        a_model = attack_models[am] \n",
    "        attack_acc, attack_pre, attack_rec = attack.mi_attack_test(target_model, a_model, x_target_train, y_target_train, x_target_test, y_target_test)\n",
    "        at_acc.append(attack_acc)\n",
    "        at_pre.append(attack_pre)\n",
    "        at_rec.append(attack_rec)\n",
    "    results['attack_acc_mean'] = np.mean(at_acc)\n",
    "    results['attack_acc_std'] = np.std(at_acc)\n",
    "    results['attack_pre_mean'] = np.mean(at_pre)\n",
    "    results['attack_pre_std'] = np.std(at_pre)\n",
    "    results['attack_rec_mean'] = np.mean(at_rec)\n",
    "    results['attack_rec_std'] = np.std(at_rec)\n",
    "    \n",
    "    return results\n",
    "path = 'mia'\n",
    "ams = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \"best_mi_model\" in file:\n",
    "            ams[file] = Net_attack(h_neurons=64, do=0, input_size=200)\n",
    "            ams[file] = torch.load(r+'/'+file)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_target_train = np.load('data/rs'+str(rand_seed)+'_x_target_train.npy')\n",
    "y_target_train = np.load('data/rs'+str(rand_seed)+'_y_target_train.npy')\n",
    "x_target_test = np.load('data/rs'+str(rand_seed)+'_x_target_test.npy')\n",
    "y_target_test = np.load('data/rs'+str(rand_seed)+'_y_target_test.npy')\n",
    "n_classes = len(np.unique(y_target_train))\n",
    "\n",
    "number_of_clients = nodes\n",
    "data_per_client = int(x_target_train.shape[0]/number_of_clients)\n",
    "\n",
    "clients = {}\n",
    "for i in range(number_of_clients):\n",
    "    \n",
    "    clients[str(i)] = algo.LogisticRegression_DPSGD()\n",
    "    clients[str(i)].n_classes = n_classes\n",
    "    \n",
    "    clients[str(i)].x = x_target_train[i*data_per_client:(i+1)*data_per_client]\n",
    "    clients[str(i)].y = y_target_train[i*data_per_client:(i+1)*data_per_client]\n",
    "    clients[str(i)].theta = fl_models[str(i)]\n",
    "\n",
    "clients['g'] = algo.LogisticRegression_DPSGD()\n",
    "clients['g'].n_classes = n_classes\n",
    "clients['g'].theta = fl_models['g']\n",
    "clients['g'].x = x_target_train\n",
    "clients['g'].y = y_target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5619b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_results = {}\n",
    "attack_acc = []\n",
    "for eps in fl_noised_eps:\n",
    "    mi_results[eps] = {}\n",
    "    for i in fl_noised_eps[eps]:\n",
    "        fl_noised_eps[eps][i].n_classes = n_classes\n",
    "        mi_results[eps][i] = attack.test_mi_attack(ams, fl_noised_eps[eps][i], clients[i].x, clients[i].y, x_target_test, y_target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_acc = {}\n",
    "for eps in fl_noised_eps:\n",
    "    attack_acc[eps] = []\n",
    "    for i in fl_noised_eps[eps]:\n",
    "        attack_acc[eps].append(mi_results[eps][i]['attack_acc_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([out_eps[0],out_eps[-1]], [0.655836, 0.655836], label='noDP centr MI baseline')\n",
    "# plt.plot([out_eps[0],out_eps[-1]], [ nodp_global,  nodp_global], label='noDP fl global baseline')\n",
    "\n",
    "data = pd.DataFrame.from_dict(attack_acc, orient='index')\n",
    "for i, m in enumerate(fl_noised_eps[eps]):\n",
    "    plt.plot(data.index, data[i], label=f'client {m}')\n",
    "    plt.plot([out_eps[0],out_eps[-1]], [fl_baseline[nodes][m]['attack_acc_mean'],fl_baseline[nodes][m]['attack_acc_mean']], label=f'client {m} no DP')\n",
    "\n",
    "plt.legend(loc='lower right', bbox_to_anchor=(1, 0), fontsize=8)\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('accuracy')   \n",
    "plt.title(f'Membershi Inference Output DP Federated {nodes} nodes')\n",
    "# plt.savefig(f'figures/fl/MI_outputDP_{nodes}nodes.png')\n",
    "# data.to_csv(f'figures/fl/MI_outputDP_{nodes}nodes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06184c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_baseline = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_baseline[nodes][m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_baseline[2] = {'0': {'attack_acc_mean': 0.6645666666666666,\n",
    "  'attack_acc_std': 0.0028941191969104794,\n",
    "  'attack_pre_mean': 0.3725942915392457,\n",
    "  'attack_pre_std': 0.041945700055508545,\n",
    "  'attack_rec_mean': 0.005966666666666666,\n",
    "  'attack_rec_std': 0.008488750726042608},\n",
    " '1': {'attack_acc_mean': 0.6645333333333333,\n",
    "  'attack_acc_std': 0.0030871297948693783,\n",
    "  'attack_pre_mean': 0.4379919989676087,\n",
    "  'attack_pre_std': 0.08777678967598833,\n",
    "  'attack_rec_mean': 0.005866666666666667,\n",
    "  'attack_rec_std': 0.0077645919975803555},\n",
    " 'g': {'attack_acc_mean': 0.4998916666666666,\n",
    "  'attack_acc_std': 0.00042270622843240793,\n",
    "  'attack_pre_mean': 0.5766793409378961,\n",
    "  'attack_pre_std': 0.06930078663902585,\n",
    "  'attack_rec_mean': 0.005916666666666666,\n",
    "  'attack_rec_std': 0.008117145776413995}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f31cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_baseline[4] = {'0': {'attack_acc_mean': 0.7827466666666667,\n",
    "  'attack_acc_std': 0.02855516921485302,\n",
    "  'attack_pre_mean': 0.12404040674785337,\n",
    "  'attack_pre_std': 0.15032659664100007,\n",
    "  'attack_rec_mean': 0.17113333333333336,\n",
    "  'attack_rec_std': 0.36985246908583547},\n",
    " '1': {'attack_acc_mean': 0.7826533333333333,\n",
    "  'attack_acc_std': 0.029852058927689076,\n",
    "  'attack_pre_mean': 0.33998973470304833,\n",
    "  'attack_pre_std': 0.090603241237165,\n",
    "  'attack_rec_mean': 0.17353333333333332,\n",
    "  'attack_rec_std': 0.3693644571723465},\n",
    " '2': {'attack_acc_mean': 0.7819600000000001,\n",
    "  'attack_acc_std': 0.031078180984950424,\n",
    "  'attack_pre_mean': 0.38269610973773016,\n",
    "  'attack_pre_std': 0.13809448542080138,\n",
    "  'attack_rec_mean': 0.17253333333333334,\n",
    "  'attack_rec_std': 0.3688802997661376},\n",
    " '3': {'attack_acc_mean': 0.78232,\n",
    "  'attack_acc_std': 0.029695557467967072,\n",
    "  'attack_pre_mean': 0.1288500866036267,\n",
    "  'attack_pre_std': 0.15065452542844787,\n",
    "  'attack_rec_mean': 0.17153333333333332,\n",
    "  'attack_rec_std': 0.369677583247288},\n",
    " 'g': {'attack_acc_mean': 0.5271083333333334,\n",
    "  'attack_acc_std': 0.060952880822448055,\n",
    "  'attack_pre_mean': 0.5744581818345609,\n",
    "  'attack_pre_std': 0.06791620590175466,\n",
    "  'attack_rec_mean': 0.15499999999999997,\n",
    "  'attack_rec_std': 0.3310287298709887}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2958ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_baseline[8] = {'0': {'attack_acc_mean': 0.8910518518518519,\n",
    "  'attack_acc_std': 0.019347286011079507,\n",
    "  'attack_pre_mean': 0.12887480893856348,\n",
    "  'attack_pre_std': 0.2247570112117821,\n",
    "  'attack_rec_mean': 0.1636,\n",
    "  'attack_rec_std': 0.3554892966039906},\n",
    " '1': {'attack_acc_mean': 0.8923703703703705,\n",
    "  'attack_acc_std': 0.021520195000671836,\n",
    "  'attack_pre_mean': 0.14358680674134058,\n",
    "  'attack_pre_std': 0.23247423509235346,\n",
    "  'attack_rec_mean': 0.16466666666666666,\n",
    "  'attack_rec_std': 0.3529225536699077},\n",
    " '2': {'attack_acc_mean': 0.892725925925926,\n",
    "  'attack_acc_std': 0.02178439304918229,\n",
    "  'attack_pre_mean': 0.27626076164377017,\n",
    "  'attack_pre_std': 0.17979309640432037,\n",
    "  'attack_rec_mean': 0.16533333333333333,\n",
    "  'attack_rec_std': 0.35015293166780076},\n",
    " '3': {'attack_acc_mean': 0.8929777777777779,\n",
    "  'attack_acc_std': 0.02254468265577016,\n",
    "  'attack_pre_mean': 0.2738904422528505,\n",
    "  'attack_pre_std': 0.1855714981246906,\n",
    "  'attack_rec_mean': 0.1650666666666667,\n",
    "  'attack_rec_std': 0.3513210624043041},\n",
    " '4': {'attack_acc_mean': 0.8931703703703704,\n",
    "  'attack_acc_std': 0.022875172685189273,\n",
    "  'attack_pre_mean': 0.27598034224776385,\n",
    "  'attack_pre_std': 0.1843304074562958,\n",
    "  'attack_rec_mean': 0.16733333333333333,\n",
    "  'attack_rec_std': 0.35567668214576875},\n",
    " '5': {'attack_acc_mean': 0.8933037037037037,\n",
    "  'attack_acc_std': 0.023586767116126724,\n",
    "  'attack_pre_mean': 0.3437877398523385,\n",
    "  'attack_pre_std': 0.20216409307432917,\n",
    "  'attack_rec_mean': 0.16453333333333334,\n",
    "  'attack_rec_std': 0.35257911956829713},\n",
    " '6': {'attack_acc_mean': 0.8929185185185187,\n",
    "  'attack_acc_std': 0.022709598654671145,\n",
    "  'attack_pre_mean': 0.14686515137505105,\n",
    "  'attack_pre_std': 0.2363595774603635,\n",
    "  'attack_rec_mean': 0.166,\n",
    "  'attack_rec_std': 0.35588994553560144},\n",
    " '7': {'attack_acc_mean': 0.8940444444444445,\n",
    "  'attack_acc_std': 0.025480585686235305,\n",
    "  'attack_pre_mean': 0.1460447908388099,\n",
    "  'attack_pre_std': 0.2505324248748163,\n",
    "  'attack_rec_mean': 0.1633333333333333,\n",
    "  'attack_rec_std': 0.3527641453184014},\n",
    " 'g': {'attack_acc_mean': 0.5195083333333333,\n",
    "  'attack_acc_std': 0.04395930330683396,\n",
    "  'attack_pre_mean': 0.5726381161688232,\n",
    "  'attack_pre_std': 0.06703511936618912,\n",
    "  'attack_rec_mean': 0.12289999999999997,\n",
    "  'attack_rec_std': 0.25927604980020813}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b27a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
