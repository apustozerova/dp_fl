{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fccda9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14d463f10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch import nn,optim\n",
    "import torch\n",
    "\n",
    "import algo\n",
    "import attack\n",
    "import scripts\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "rand_seed=24\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bb23201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# purchase\n",
    "# load data with different seeds\n",
    "data_seed = {}\n",
    "for rand_seed in [1,3,13,24,42]:\n",
    "    data_seed[rand_seed] =  {}\n",
    "    \n",
    "    data_seed[rand_seed]['x_target_train'] = np.load('data/rs'+str(rand_seed)+'_x_target_train.npy')\n",
    "    data_seed[rand_seed]['y_target_train'] = np.load('data/rs'+str(rand_seed)+'_y_target_train.npy')\n",
    "    data_seed[rand_seed]['x_target_test'] = np.load('data/rs'+str(rand_seed)+'_x_target_test.npy')\n",
    "    data_seed[rand_seed]['y_target_test'] = np.load('data/rs'+str(rand_seed)+'_y_target_test.npy')\n",
    "    data_seed[rand_seed]['n_classes'] = len(np.unique(data_seed[rand_seed]['y_target_train']))\n",
    "    data_seed[rand_seed]['X_train_size'] = data_seed[rand_seed]['x_target_train'].shape[0]\n",
    "    data_seed[rand_seed]['X_test_size ']= data_seed[rand_seed]['x_target_test'].shape[0]\n",
    "\n",
    "def set_the_seed_and_data(seed):\n",
    "    np.random.seed(rand_seed)\n",
    "    torch.manual_seed(rand_seed)\n",
    "    random.seed(rand_seed)\n",
    "    \n",
    "    return data_seed[seed]['x_target_train'], data_seed[rand_seed]['y_target_train'], data_seed[rand_seed]['x_target_test'], data_seed[rand_seed]['y_target_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d372b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack models\n",
    "from torch import nn\n",
    "\n",
    "class Net_attack(nn.Module):\n",
    "\n",
    "    def __init__(self, h_neurons, do, input_size):\n",
    "        super(Net_attack, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.h_neurons = h_neurons\n",
    "        self.do = do\n",
    "        self.fc1 = nn.Linear(input_size, h_neurons)\n",
    "        self.fc2 = nn.Linear(h_neurons, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(do)\n",
    "        self.softmax = nn.Softmax(dim=1)   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "path = 'mia'\n",
    "ams = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \"best_mi_model\" in file:\n",
    "            ams[file] = Net_attack(h_neurons=64, do=0, input_size=200)\n",
    "            ams[file] = torch.load(r+'/'+file)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88aa666",
   "metadata": {},
   "source": [
    "# MI on centralized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3882dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'tm'\n",
    "tms_params = {}\n",
    "tms = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \"target_model_params.json\" in file:\n",
    "            with open(path+'/'+file) as json_file:\n",
    "                tms_params[file.replace('_params.json', '')] = json.load(json_file)\n",
    "        if \"target_model.npy\" in file:\n",
    "            tms[file.replace('.npy', '')] = np.load(path+'/'+file)\n",
    "            \n",
    "df = pd.DataFrame.from_dict(tms_params, orient='index')\n",
    "df.shape            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b212250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack every centralized target model\n",
    "for file in tms_params:\n",
    "    \n",
    "    if 'attack_acc_mean' in tms_params[file]:\n",
    "        continue\n",
    "    # set random seed and load the data    \n",
    "    rand_seed=int(file[file.find('rs')+2:file.find('_lr')])\n",
    "    x_target_train, y_target_train, x_target_test, y_target_test = set_the_seed_and_data(rand_seed)\n",
    "    \n",
    "    #attack\n",
    "    target_model = algo.LogisticRegression_DPSGD()\n",
    "    target_model.theta = tms[file]\n",
    "    params = tms_params[file]\n",
    "    scripts.set_model_params(target_model, params)\n",
    "    attack_dict = attack.test_mi_attack(ams, target_model, x_target_train, y_target_train, x_target_test, y_target_test)\n",
    "    \n",
    "    params.update(attack_dict)\n",
    "    if 'attack_acc' in params:\n",
    "        params.pop('attack_acc')\n",
    "        params.pop('attack_pre')\n",
    "        params.pop('attack_rec')\n",
    "    #write a new parameters file with attack results\n",
    "    with open('tm/'+file+'_params.json', 'w') as file:\n",
    "        json.dump(params, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd254e1",
   "metadata": {},
   "source": [
    "# Membership inference Federated Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c9a88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'fl/'\n",
    "params = {}\n",
    "results = {}\n",
    "models = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \"params.json\" in file:\n",
    "            with open(r+'/'+file) as json_file:\n",
    "                params[r] = json.load(json_file)\n",
    "        elif \"results.csv\" in file:\n",
    "\n",
    "#             with open(r+'/'+file, 'r') as f_open:\n",
    "#                 lines = f_open.readlines()\n",
    "#                 if 'HEAD' in lines[0]:   \n",
    "#                     for i,l in enumerate(lines):\n",
    "#                         if 'attack_acc_mean' in l:\n",
    "#                             index = i\n",
    "#                             break\n",
    "#                     with open(r+'/'+file, 'w') as f_write:\n",
    "#                         f_write.writelines(lines[i:-1])\n",
    "\n",
    "            results[r] = pd.read_csv(r+'/'+file)\n",
    "            it = []\n",
    "            client = []\n",
    "            model_filenames = []\n",
    "            if 'Unnamed: 0' in results[r].keys():\n",
    "                for k in results[r]['Unnamed: 0']:\n",
    "                    it.append(k[k.find('i')+1:k.find('_')])\n",
    "                    client.append(k[k.find('_')+1:])\n",
    "                    model_filenames.append(k+'.npy')\n",
    "                results[r]['it'] = it\n",
    "                results[r]['client'] = client\n",
    "                results[r]['file_name'] = model_filenames\n",
    "                results[r].pop('Unnamed: 0')\n",
    "        elif '.npy' in file:\n",
    "            if r not in models:\n",
    "                models[r] = {}\n",
    "#             print(r+'/'+file)\n",
    "            models[r][file] = np.load(r+'/'+file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d6fd90ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_files = results.keys()\n",
    "len(selected_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b45f198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fl/rs42_ncl16_fiter15_lr0.01_iter300_reg0.0001_sgdDPTrue_eps10000_L20_C2\n",
      "fl/rs42_ncl16_fiter20_lr0.01_iter200_reg0.0001_sgdDPTrue_eps10000_L20_C2\n"
     ]
    }
   ],
   "source": [
    "# attack every federated local/global model\n",
    "for file in selected_files:\n",
    "    \n",
    "    if 'attack_acc_mean' in results[file].keys():\n",
    "        continue\n",
    "        \n",
    "    # set random seed and load the data    \n",
    "    rand_seed=int(file[file.find('rs')+2:file.find('_ncl')])\n",
    "    x_target_train, y_target_train, x_target_test, y_target_test = set_the_seed_and_data(rand_seed)\n",
    "    #set number of client for following split of the training data between clients\n",
    "    number_of_clients = len(results[file]['client'].unique())-1\n",
    "    data_per_client = int(x_target_train.shape[0]/number_of_clients)\n",
    "\n",
    "    #attack\n",
    "    attack_results = {}\n",
    "    print(file)\n",
    "    for tm in results[file]['file_name']:\n",
    "        target_model = algo.LogisticRegression_DPSGD()\n",
    "        target_model.theta = models[file][tm]\n",
    "        tm_params = params[file]\n",
    "        scripts.set_model_params(target_model, tm_params)\n",
    "        if 'g' in tm:\n",
    "            target_model.x = x_target_train\n",
    "            target_model.y = y_target_train\n",
    "        else:\n",
    "            i = int(tm[tm.find('_c')+2:tm.find('.npy')])\n",
    "            target_model.x = x_target_train[i*data_per_client:(i+1)*data_per_client]\n",
    "            target_model.y = y_target_train[i*data_per_client:(i+1)*data_per_client]\n",
    "        attack_dict = attack.test_mi_attack(ams, target_model, target_model.x, target_model.y, x_target_test, y_target_test)\n",
    "        attack_results[tm] = attack_dict\n",
    "    attack_df = pd.DataFrame.from_dict(attack_results, orient='index')\n",
    "    result_df = results[file].set_index('file_name')\n",
    "    new_df = pd.merge(result_df, attack_df, left_index=True, right_index=True)\n",
    "    #save attack results in old results file\n",
    "    new_df.to_csv(file+'/results.csv')\n",
    "    results[file] = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d068ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6e06e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "968750ad",
   "metadata": {},
   "source": [
    "# MI LOAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ace5cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'mia/loan/best_ams/'\n",
    "aparams = {}\n",
    "ams = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \".json\" in file:\n",
    "            with open(r+'/'+file) as json_file:\n",
    "                aparams[r+file.replace('_params.json', '')] = json.load(json_file)\n",
    "            \n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \".json\" not in file and '.DS_Store' not in file:\n",
    "            ams[r+file] = Net_attack(h_neurons=aparams[r+file]['h_neurons'], do=aparams[r+file]['do'], input_size=14)\n",
    "            ams[r+file] = torch.load(r+'/'+file)      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a7d4354",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'loan/centr/'\n",
    "tms_params = {}\n",
    "tms = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \".json\" in file:\n",
    "            with open(r+'/'+file) as json_file:\n",
    "                tms_params[file.replace('_params.json', '')] = json.load(json_file)\n",
    "        if \"target_model.npy\" in file:\n",
    "            tms[file.replace('.npy', '')] = np.load(path+'/'+file)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73bcaa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack every centralized target model\n",
    "for file in tms_params:\n",
    "    \n",
    "    if 'attack_acc_mean' in tms_params[file]:\n",
    "        continue\n",
    "    # set random seed and load the data    \n",
    "    rand_seed=int(file[file.find('rs')+2:file.find('_lr')])\n",
    "    x_target_train, y_target_train, x_target_test, y_target_test = scripts.load_loan(rand_seed, tr_size=10000)\n",
    "    \n",
    "    #attack\n",
    "    target_model = algo.LogisticRegression_DPSGD()\n",
    "    target_model.theta = tms[file]\n",
    "    params = tms_params[file]\n",
    "    scripts.set_model_params(target_model, params)\n",
    "    attack_dict = attack.test_mi_attack(ams, target_model, x_target_train, y_target_train, x_target_test, y_target_test)\n",
    "    \n",
    "    params.update(attack_dict)\n",
    "    if 'attack_acc' in params:\n",
    "        params.pop('attack_acc')\n",
    "        params.pop('attack_pre')\n",
    "        params.pop('attack_rec')\n",
    "    #write a new parameters file with attack results\n",
    "    with open('loan/centr/'+file+'_params.json', 'w') as file:\n",
    "        json.dump(params, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c2bd5",
   "metadata": {},
   "source": [
    "# Loan FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b287edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'loan/fl'\n",
    "params = {}\n",
    "results = {}\n",
    "models = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \"params.json\" in file:\n",
    "            with open(r+'/'+file) as json_file:\n",
    "                params[r] = json.load(json_file)\n",
    "        elif \"results.csv\" in file:\n",
    "            results[r] = pd.read_csv(r+'/'+file)\n",
    "            it = []\n",
    "            client = []\n",
    "            model_filenames = []\n",
    "            if 'Unnamed: 0' in results[r].keys():\n",
    "                for k in results[r]['Unnamed: 0']:\n",
    "#                     print(k)\n",
    "#                     break\n",
    "                    it.append(k[k.find('i')+1:k.find('_')])\n",
    "                    client.append(k[k.find('_')+1:])\n",
    "                    model_filenames.append(k+'.npy')\n",
    "                results[r]['it'] = it\n",
    "                results[r]['client'] = client\n",
    "                results[r]['file_name'] = model_filenames\n",
    "                results[r].pop('Unnamed: 0')\n",
    "        elif '.npy' in file:\n",
    "            if r not in models:\n",
    "                models[r] = {}\n",
    "            models[r][file] = np.load(r+'/'+file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e93edc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_files = results.keys()\n",
    "len(selected_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d777f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L1_C1.5'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f3a6d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0bf7cf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L1_C1.5\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L20_C10\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.001_iter200_reg0.0001\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L30_C2\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L30_C5\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.001_iter200_reg1e-06\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L10_C3\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L10_C4\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L20_C5\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L20_C2\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L30_C4\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L30_C3\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L20_C3\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L20_C4\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L10_C5\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L10_C2\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L5_C4\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L5_C3\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L1_C5\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L1_C2\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter300_reg0.0001\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L1_C3\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L1_C4\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L5_C2\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L5_C5\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.001_iter100_reg0.0001\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter300_reg1e-06\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.001_iter300_reg1e-06\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter100_reg0.0001\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L5_C1.5\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L10_C7\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.001_iter300_reg0.0001\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L20_C8\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L20_C1\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L10_C9\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L20_C6\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L10_C1.5\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L30_C1\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L20_C7\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L10_C8\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L10_C1\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L20_C9\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L30_C1.5\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L10_C6\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.001_iter100_reg1e-06\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L5_C7\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L1_C8\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L1_C1\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L1_C6\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L5_C9\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L1_C10\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L1_C7\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L5_C8\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L5_C6\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L1_C9\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L5_C1\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L20_C1.5\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L10_C10\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg1e-06_sgdDPTrue_eps10000_L5_C10\n",
      "loan/fl/rs42_ncl2_fiter10_lr0.01_iter200_reg0.0001\n"
     ]
    }
   ],
   "source": [
    "# attack every federated local/global model\n",
    "for file in selected_files:\n",
    "    \n",
    "    if 'attack_acc_mean' in results[file].keys():\n",
    "        continue\n",
    "        \n",
    "    # set random seed and load the data    \n",
    "    rand_seed=int(file[file.find('rs')+2:file.find('_ncl')])\n",
    "    x_target_train, y_target_train, x_target_test, y_target_test = scripts.load_loan(rand_seed, tr_size=10000)\n",
    "    #set number of client for following split of the training data between clients\n",
    "    number_of_clients = len(results[file]['client'].unique())-1\n",
    "    data_per_client = int(x_target_train.shape[0]/number_of_clients)\n",
    "\n",
    "    #attack\n",
    "    attack_results = {}\n",
    "    print(file)\n",
    "    for tm in results[file]['file_name']:\n",
    "        target_model = algo.LogisticRegression_DPSGD()\n",
    "        target_model.theta = models[file][tm]\n",
    "        tm_params = params[file]\n",
    "        scripts.set_model_params(target_model, tm_params)\n",
    "        if 'g' in tm:\n",
    "            target_model.x = x_target_train\n",
    "            target_model.y = y_target_train\n",
    "        else:\n",
    "            i = int(tm[tm.find('_c')+2:tm.find('.npy')])\n",
    "            target_model.x = x_target_train[i*data_per_client:(i+1)*data_per_client]\n",
    "            target_model.y = y_target_train[i*data_per_client:(i+1)*data_per_client]\n",
    "        attack_dict = attack.test_mi_attack(ams, target_model, target_model.x, target_model.y, x_target_test, y_target_test)\n",
    "        attack_results[tm] = attack_dict\n",
    "    attack_df = pd.DataFrame.from_dict(attack_results, orient='index')\n",
    "    result_df = results[file].set_index('file_name')\n",
    "    new_df = pd.merge(result_df, attack_df, left_index=True, right_index=True)\n",
    "    #save attack results in old results file\n",
    "    new_df.to_csv(file+'/results.csv')\n",
    "    results[file] = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a4865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
