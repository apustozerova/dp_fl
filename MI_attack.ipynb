{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fccda9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 20:51:56.595400: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14db67f10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch import nn,optim\n",
    "import torch\n",
    "\n",
    "import algo\n",
    "import attack\n",
    "import scripts\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "rand_seed=24\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bb23201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# purchase\n",
    "# load data with different seeds\n",
    "data_seed = {}\n",
    "for rand_seed in [1,3,13,24,42]:\n",
    "    data_seed[rand_seed] =  {}\n",
    "    \n",
    "    data_seed[rand_seed]['x_target_train'] = np.load('data/rs'+str(rand_seed)+'_x_target_train.npy')\n",
    "    data_seed[rand_seed]['y_target_train'] = np.load('data/rs'+str(rand_seed)+'_y_target_train.npy')\n",
    "    data_seed[rand_seed]['x_target_test'] = np.load('data/rs'+str(rand_seed)+'_x_target_test.npy')\n",
    "    data_seed[rand_seed]['y_target_test'] = np.load('data/rs'+str(rand_seed)+'_y_target_test.npy')\n",
    "    data_seed[rand_seed]['n_classes'] = len(np.unique(data_seed[rand_seed]['y_target_train']))\n",
    "    data_seed[rand_seed]['X_train_size'] = data_seed[rand_seed]['x_target_train'].shape[0]\n",
    "    data_seed[rand_seed]['X_test_size ']= data_seed[rand_seed]['x_target_test'].shape[0]\n",
    "\n",
    "def set_the_seed_and_data(seed):\n",
    "    np.random.seed(rand_seed)\n",
    "    torch.manual_seed(rand_seed)\n",
    "    random.seed(rand_seed)\n",
    "    \n",
    "    return data_seed[seed]['x_target_train'], data_seed[rand_seed]['y_target_train'], data_seed[rand_seed]['x_target_test'], data_seed[rand_seed]['y_target_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d372b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack models\n",
    "from torch import nn\n",
    "\n",
    "class Net_attack(nn.Module):\n",
    "\n",
    "    def __init__(self, h_neurons, do, input_size):\n",
    "        super(Net_attack, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.h_neurons = h_neurons\n",
    "        self.do = do\n",
    "        self.fc1 = nn.Linear(input_size, h_neurons)\n",
    "        self.fc2 = nn.Linear(h_neurons, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(do)\n",
    "        self.softmax = nn.Softmax(dim=1)   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "path = 'mia'\n",
    "ams = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \"best_mi_model\" in file:\n",
    "            ams[file] = Net_attack(h_neurons=64, do=0, input_size=200)\n",
    "            ams[file] = torch.load(r+'/'+file)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88aa666",
   "metadata": {},
   "source": [
    "# MI on centralized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3882dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'tm'\n",
    "tms_params = {}\n",
    "tms = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \"target_model_params.json\" in file:\n",
    "            with open(path+'/'+file) as json_file:\n",
    "                tms_params[file.replace('_params.json', '')] = json.load(json_file)\n",
    "        if \"target_model.npy\" in file:\n",
    "            tms[file.replace('.npy', '')] = np.load(path+'/'+file)\n",
    "            \n",
    "df = pd.DataFrame.from_dict(tms_params, orient='index')\n",
    "df.shape            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b212250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack every centralized target model\n",
    "for file in tms_params:\n",
    "    \n",
    "    if 'attack_acc_mean' in tms_params[file]:\n",
    "        continue\n",
    "    # set random seed and load the data    \n",
    "    rand_seed=int(file[file.find('rs')+2:file.find('_lr')])\n",
    "    x_target_train, y_target_train, x_target_test, y_target_test = set_the_seed_and_data(rand_seed)\n",
    "    \n",
    "    #attack\n",
    "    target_model = algo.LogisticRegression_DPSGD()\n",
    "    target_model.theta = tms[file]\n",
    "    params = tms_params[file]\n",
    "    scripts.set_model_params(target_model, params)\n",
    "    attack_dict = attack.test_mi_attack(ams, target_model, x_target_train, y_target_train, x_target_test, y_target_test)\n",
    "    \n",
    "    params.update(attack_dict)\n",
    "    if 'attack_acc' in params:\n",
    "        params.pop('attack_acc')\n",
    "        params.pop('attack_pre')\n",
    "        params.pop('attack_rec')\n",
    "    #write a new parameters file with attack results\n",
    "    with open('tm/'+file+'_params.json', 'w') as file:\n",
    "        json.dump(params, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd254e1",
   "metadata": {},
   "source": [
    "# Membership inference Federated Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'fl/'\n",
    "params = {}\n",
    "results = {}\n",
    "models = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \"params.json\" in file:\n",
    "            with open(r+'/'+file) as json_file:\n",
    "                params[r] = json.load(json_file)\n",
    "        elif \"results.csv\" in file:\n",
    "\n",
    "#             with open(r+'/'+file, 'r') as f_open:\n",
    "#                 lines = f_open.readlines()\n",
    "#                 if 'HEAD' in lines[0]:   \n",
    "#                     for i,l in enumerate(lines):\n",
    "#                         if 'attack_acc_mean' in l:\n",
    "#                             index = i\n",
    "#                             break\n",
    "#                     with open(r+'/'+file, 'w') as f_write:\n",
    "#                         f_write.writelines(lines[i:-1])\n",
    "\n",
    "            results[r] = pd.read_csv(r+'/'+file)\n",
    "            it = []\n",
    "            client = []\n",
    "            model_filenames = []\n",
    "            if 'Unnamed: 0' in results[r].keys():\n",
    "                for k in results[r]['Unnamed: 0']:\n",
    "                    it.append(k[k.find('i')+1:k.find('_')])\n",
    "                    client.append(k[k.find('_')+1:])\n",
    "                    model_filenames.append(k+'.npy')\n",
    "                results[r]['it'] = it\n",
    "                results[r]['client'] = client\n",
    "                results[r]['file_name'] = model_filenames\n",
    "                results[r].pop('Unnamed: 0')\n",
    "        elif '.npy' in file:\n",
    "            if r not in models:\n",
    "                models[r] = {}\n",
    "#             print(r+'/'+file)\n",
    "            models[r][file] = np.load(r+'/'+file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_files = results.keys()\n",
    "len(selected_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack every federated local/global model\n",
    "for file in selected_files:\n",
    "    \n",
    "    if 'attack_acc_mean' in results[file].keys():\n",
    "        continue\n",
    "        \n",
    "    # set random seed and load the data    \n",
    "    rand_seed=int(file[file.find('rs')+2:file.find('_ncl')])\n",
    "    x_target_train, y_target_train, x_target_test, y_target_test = set_the_seed_and_data(rand_seed)\n",
    "    #set number of client for following split of the training data between clients\n",
    "    number_of_clients = len(results[file]['client'].unique())-1\n",
    "    data_per_client = int(x_target_train.shape[0]/number_of_clients)\n",
    "\n",
    "    #attack\n",
    "    attack_results = {}\n",
    "    print(file)\n",
    "    for tm in results[file]['file_name']:\n",
    "        target_model = algo.LogisticRegression_DPSGD()\n",
    "        target_model.theta = models[file][tm]\n",
    "        tm_params = params[file]\n",
    "        scripts.set_model_params(target_model, tm_params)\n",
    "        if 'g' in tm:\n",
    "            target_model.x = x_target_train\n",
    "            target_model.y = y_target_train\n",
    "        else:\n",
    "            i = int(tm[tm.find('_c')+2:tm.find('.npy')])\n",
    "            target_model.x = x_target_train[i*data_per_client:(i+1)*data_per_client]\n",
    "            target_model.y = y_target_train[i*data_per_client:(i+1)*data_per_client]\n",
    "        attack_dict = attack.test_mi_attack(ams, target_model, target_model.x, target_model.y, x_target_test, y_target_test)\n",
    "        attack_results[tm] = attack_dict\n",
    "    attack_df = pd.DataFrame.from_dict(attack_results, orient='index')\n",
    "    result_df = results[file].set_index('file_name')\n",
    "    new_df = pd.merge(result_df, attack_df, left_index=True, right_index=True)\n",
    "    #save attack results in old results file\n",
    "    new_df.to_csv(file+'/results.csv')\n",
    "    results[file] = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d068ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6e06e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "968750ad",
   "metadata": {},
   "source": [
    "# MI LOAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace5cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'mia/loan/best_ams/'\n",
    "aparams = {}\n",
    "ams = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \".json\" in file:\n",
    "            with open(r+'/'+file) as json_file:\n",
    "                aparams[r+file.replace('_params.json', '')] = json.load(json_file)\n",
    "            \n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \".json\" not in file and '.DS_Store' not in file:\n",
    "            ams[r+file] = Net_attack(h_neurons=aparams[r+file]['h_neurons'], do=aparams[r+file]['do'], input_size=14)\n",
    "            ams[r+file] = torch.load(r+'/'+file)      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7d4354",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'loan/centr/'\n",
    "tms_params = {}\n",
    "tms = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \".json\" in file:\n",
    "            with open(r+'/'+file) as json_file:\n",
    "                tms_params[file.replace('_params.json', '')] = json.load(json_file)\n",
    "        if \"target_model.npy\" in file:\n",
    "            tms[file.replace('.npy', '')] = np.load(path+'/'+file)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bcaa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack every centralized target model\n",
    "for file in tms_params:\n",
    "    \n",
    "    if 'attack_acc_mean' in tms_params[file]:\n",
    "        continue\n",
    "    # set random seed and load the data    \n",
    "    rand_seed=int(file[file.find('rs')+2:file.find('_lr')])\n",
    "    x_target_train, y_target_train, x_target_test, y_target_test = scripts.load_loan(rand_seed, tr_size=10000)\n",
    "    \n",
    "    #attack\n",
    "    target_model = algo.LogisticRegression_DPSGD()\n",
    "    target_model.theta = tms[file]\n",
    "    params = tms_params[file]\n",
    "    scripts.set_model_params(target_model, params)\n",
    "    attack_dict = attack.test_mi_attack(ams, target_model, x_target_train, y_target_train, x_target_test, y_target_test)\n",
    "    \n",
    "    params.update(attack_dict)\n",
    "    if 'attack_acc' in params:\n",
    "        params.pop('attack_acc')\n",
    "        params.pop('attack_pre')\n",
    "        params.pop('attack_rec')\n",
    "    #write a new parameters file with attack results\n",
    "    with open('loan/centr/'+file+'_params.json', 'w') as file:\n",
    "        json.dump(params, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c2bd5",
   "metadata": {},
   "source": [
    "# Loan FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b287edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'loan/fl'\n",
    "params = {}\n",
    "results = {}\n",
    "models = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \"params.json\" in file:\n",
    "            with open(r+'/'+file) as json_file:\n",
    "                params[r] = json.load(json_file)\n",
    "        elif \"results.csv\" in file:\n",
    "            results[r] = pd.read_csv(r+'/'+file)\n",
    "            it = []\n",
    "            client = []\n",
    "            model_filenames = []\n",
    "            if 'Unnamed: 0' in results[r].keys():\n",
    "                for k in results[r]['Unnamed: 0']:\n",
    "#                     print(k)\n",
    "#                     break\n",
    "                    it.append(k[k.find('i')+1:k.find('_')])\n",
    "                    client.append(k[k.find('_')+1:])\n",
    "                    model_filenames.append(k+'.npy')\n",
    "                results[r]['it'] = it\n",
    "                results[r]['client'] = client\n",
    "                results[r]['file_name'] = model_filenames\n",
    "                results[r].pop('Unnamed: 0')\n",
    "        elif '.npy' in file:\n",
    "            if r not in models:\n",
    "                models[r] = {}\n",
    "            models[r][file] = np.load(r+'/'+file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c726a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'mia/loan/best_ams/'\n",
    "aparams = {}\n",
    "ams = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \".json\" in file:\n",
    "            with open(r+'/'+file) as json_file:\n",
    "                aparams[r+file.replace('_params.json', '')] = json.load(json_file)\n",
    "            \n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \".json\" not in file and '.DS_Store' not in file:\n",
    "            ams[r+file] = Net_attack(h_neurons=aparams[r+file]['h_neurons'], do=aparams[r+file]['do'], input_size=14)\n",
    "            ams[r+file] = torch.load(r+'/'+file)      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e93edc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_files = results.keys()\n",
    "len(selected_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf7cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack every federated local/global model\n",
    "for file in selected_files:\n",
    "    \n",
    "    if 'attack_acc_mean' in results[file].keys():\n",
    "        continue\n",
    "        \n",
    "    # set random seed and load the data    \n",
    "    rand_seed=int(file[file.find('rs')+2:file.find('_ncl')])\n",
    "    x_target_train, y_target_train, x_target_test, y_target_test = scripts.load_loan(rand_seed, tr_size=10000)\n",
    "    #set number of client for following split of the training data between clients\n",
    "    number_of_clients = len(results[file]['client'].unique())-1\n",
    "    data_per_client = int(x_target_train.shape[0]/number_of_clients)\n",
    "\n",
    "    #attack\n",
    "    attack_results = {}\n",
    "    print(file)\n",
    "    for tm in results[file]['file_name']:\n",
    "        target_model = algo.LogisticRegression_DPSGD()\n",
    "        target_model.theta = models[file][tm]\n",
    "        tm_params = params[file]\n",
    "        scripts.set_model_params(target_model, tm_params)\n",
    "        if 'g' in tm:\n",
    "            target_model.x = x_target_train\n",
    "            target_model.y = y_target_train\n",
    "        else:\n",
    "            i = int(tm[tm.find('_c')+2:tm.find('.npy')])\n",
    "            target_model.x = x_target_train[i*data_per_client:(i+1)*data_per_client]\n",
    "            target_model.y = y_target_train[i*data_per_client:(i+1)*data_per_client]\n",
    "        attack_dict = attack.test_mi_attack(ams, target_model, target_model.x, target_model.y, x_target_test, y_target_test)\n",
    "        attack_results[tm] = attack_dict\n",
    "    attack_df = pd.DataFrame.from_dict(attack_results, orient='index')\n",
    "    result_df = results[file].set_index('file_name')\n",
    "    new_df = pd.merge(result_df, attack_df, left_index=True, right_index=True)\n",
    "    #save attack results in old results file\n",
    "    new_df.to_csv(file+'/results.csv')\n",
    "    results[file] = new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7959e2ce",
   "metadata": {},
   "source": [
    "# Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ef1e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'mia/texas/best_mia/'\n",
    "aparams = {}\n",
    "ams = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \".json\" in file:\n",
    "            with open(r+'/'+file) as json_file:\n",
    "                aparams[r+file.replace('.json', '')] = json.load(json_file)\n",
    "            \n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \".json\" not in file and '.DS_Store' not in file:\n",
    "            ams[r+file] = Net_attack(h_neurons=aparams[r+file]['h_neurons'], do=aparams[r+file]['do'], input_size=100)\n",
    "            ams[r+file] = torch.load(r+'/'+file)      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65c8a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'texas/centr/'\n",
    "tms_params = {}\n",
    "tms = {}\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \".json\" in file:\n",
    "            with open(r+'/'+file) as json_file:\n",
    "                tms_params[file.replace('_params.json', '')] = json.load(json_file)\n",
    "        if \"target_model.npy\" in file:\n",
    "            tms[file.replace('.npy', '')] = np.load(path+'/'+file)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00cd1cba",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m params \u001b[38;5;241m=\u001b[39m tms_params[file]\n\u001b[1;32m     14\u001b[0m scripts\u001b[38;5;241m.\u001b[39mset_model_params(target_model, params)\n\u001b[0;32m---> 15\u001b[0m attack_dict \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_mi_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_target_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_target_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_target_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_target_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(attack_dict)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattack_acc\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n",
      "File \u001b[0;32m~/Documents/Projects/dp_fl/attack.py:180\u001b[0m, in \u001b[0;36mtest_mi_attack\u001b[0;34m(attack_models, target_model, x_target_train, y_target_train, x_target_test, y_target_test)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m am \u001b[38;5;129;01min\u001b[39;00m attack_models:\n\u001b[1;32m    179\u001b[0m     a_model \u001b[38;5;241m=\u001b[39m attack_models[am] \n\u001b[0;32m--> 180\u001b[0m     attack_acc, attack_pre, attack_rec \u001b[38;5;241m=\u001b[39m \u001b[43mmi_attack_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_target_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_target_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_target_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_target_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     at_acc\u001b[38;5;241m.\u001b[39mappend(attack_acc)\n\u001b[1;32m    182\u001b[0m     at_pre\u001b[38;5;241m.\u001b[39mappend(attack_pre)\n",
      "File \u001b[0;32m~/Documents/Projects/dp_fl/attack.py:124\u001b[0m, in \u001b[0;36mmi_attack_test\u001b[0;34m(model, a_model, x_target_train, y_target_train, x_target_test, y_target_test)\u001b[0m\n\u001b[1;32m    121\u001b[0m x_target_test, y_target_test \u001b[38;5;241m=\u001b[39m x_target_test[:set_size], y_target_test[:set_size]\n\u001b[1;32m    123\u001b[0m train_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_target_train, y_target_train)\n\u001b[0;32m--> 124\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_target_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_target_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_ohe(model, y_target_train)\n\u001b[1;32m    127\u001b[0m y_test \u001b[38;5;241m=\u001b[39m y_ohe(model, y_target_test)\n",
      "File \u001b[0;32m~/Documents/Projects/dp_fl/algo.py:87\u001b[0m, in \u001b[0;36mLogisticRegression_DPSGD.predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    Predicted class labels\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 87\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#add column to the data for bias\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/numpy/lib/function_base.py:5392\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5390\u001b[0m     values \u001b[38;5;241m=\u001b[39m ravel(values)\n\u001b[1;32m   5391\u001b[0m     axis \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# attack every centralized target model\n",
    "for file in tms_params:\n",
    "    \n",
    "    if 'attack_acc_mean' in tms_params[file]:\n",
    "        continue\n",
    "    # set random seed and load the data\n",
    "    print(file)\n",
    "    rand_seed=int(file[file.find('rs')+2:file.find('_lr')])\n",
    "    x_target_train, y_target_train, x_target_test, y_target_test = scripts.load_texas()\n",
    "    \n",
    "    #attack\n",
    "    target_model = algo.LogisticRegression_DPSGD()\n",
    "    target_model.theta = tms[file]\n",
    "    params = tms_params[file]\n",
    "    scripts.set_model_params(target_model, params)\n",
    "    attack_dict = attack.test_mi_attack(ams, target_model, x_target_train, y_target_train, x_target_test, y_target_test)\n",
    "    \n",
    "    params.update(attack_dict)\n",
    "    if 'attack_acc' in params:\n",
    "        params.pop('attack_acc')\n",
    "        params.pop('attack_pre')\n",
    "        params.pop('attack_rec')\n",
    "    #write a new parameters file with attack results\n",
    "    with open('texas/centr/'+file+'_params.json', 'w') as file:\n",
    "        json.dump(params, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c63f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
