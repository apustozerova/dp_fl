{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe2a2510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "2022-11-16 13:10:51.673138: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14cc1bed0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch import nn,optim\n",
    "import torch\n",
    "\n",
    "import algo\n",
    "\n",
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b00bee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_attack(nn.Module):\n",
    "\n",
    "    def __init__(self, h_neurons, do, input_size):\n",
    "        super(Net_attack, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.h_neurons = h_neurons\n",
    "        self.do = do\n",
    "        self.fc1 = nn.Linear(input_size, h_neurons)\n",
    "        self.fc2 = nn.Linear(h_neurons, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(do)\n",
    "        self.softmax = nn.Softmax(dim=1)   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class Train_args():\n",
    "\n",
    "    def __init__(self, learning_rate, weight_decay, epoch):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epoch = epoch\n",
    "         \n",
    "def train_attack_model(model, train_data, train_target, train_args):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=train_args.learning_rate, weight_decay=train_args.weight_decay)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_data)\n",
    "    loss = nn.CrossEntropyLoss()(output, train_target.to(torch.long))\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "def attack_evaluation(model, x, y, dev=\"cpu\", extended=False):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output =  model(x)\n",
    "        out_target = output.argmax(1, keepdim=True)\n",
    "        correct = out_target.to(dev).eq(y.to(dev).view_as(out_target.to(dev))).sum().item()\n",
    "        acc = correct/y.shape[0]\n",
    "\n",
    "        predicted_positive = output.argmax(1, keepdim=True) == 1\n",
    "        labeled_positive = y == 1\n",
    "        tp = predicted_positive.to(dev) * labeled_positive.to(dev).view_as(out_target)\n",
    "        tp_count = tp.to(dev).sum().item()\n",
    "        \n",
    "        if predicted_positive.to(dev).sum().item() != 0:\n",
    "            pre = tp_count / predicted_positive.to(dev).sum().item()\n",
    "        else:\n",
    "            pre = 0\n",
    "        if labeled_positive.to(dev).sum().item() !=0:\n",
    "            rec = tp_count / labeled_positive.to(dev).sum().item()\n",
    "        else:\n",
    "            rec = 0\n",
    "    if extended:\n",
    "        predicted_negative = output.argmax(1, keepdim=True) == 0\n",
    "        labeled_negative = y == 0\n",
    "        tn = predicted_negative.to(dev) * labeled_negative.to(dev).view_as(out_target)\n",
    "        tn_count = tn.to(dev).sum().item()\n",
    "\n",
    "        fp_count = predicted_positive.to(dev).sum().item() - tp.to(dev).sum().item()\n",
    "        fn_count = labeled_positive.to(dev).sum().item() - tp.to(dev).sum().item()\n",
    "        \n",
    "        return acc, pre, rec, tp_count, tn_count, fp_count, fn_count\n",
    "    else:\n",
    "        return acc, pre, rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cc39b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "(19732, 600) (177591, 600)\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('../datasets/dataset_purchase', )\n",
    "y=raw_data['63']\n",
    "X=raw_data.drop('63', axis=1)\n",
    "y =  y.replace(100, 0)\n",
    "print(y.nunique())\n",
    "\n",
    "X_train, x_shadow, y_train, y_shadow = train_test_split(X, y, train_size=0.1, random_state=42)\n",
    "print(X_train.shape, x_shadow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "580e8d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0e99c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60caadc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/0p0lnj8d6zj532q0yvm1zt580000gn/T/ipykernel_87725/3594789665.py:5: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_target_train = np.array(y_train[:X_train_size])\n",
      "/var/folders/66/0p0lnj8d6zj532q0yvm1zt580000gn/T/ipykernel_87725/3594789665.py:7: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_target_test = np.array(y_train[X_train_size:])\n"
     ]
    }
   ],
   "source": [
    "# (x_target, y_target), (x_shadow, y_shadow), _, _ = load_nursery(test_set=0.75)\n",
    "\n",
    "X_train_size = 10000\n",
    "x_target_train = np.array(X_train[:X_train_size])\n",
    "y_target_train = np.array(y_train[:X_train_size])\n",
    "x_target_test = np.array(X_train[X_train_size:])\n",
    "y_target_test = np.array(y_train[X_train_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff81e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = algo.LogisticRegression_DPSGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5219365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.__dict__\n",
    "p2 = dict(model.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe728475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_classes': 2,\n",
       " 'alpha': 0.1,\n",
       " 'max_iter': 100,\n",
       " 'lambda_': 0.1,\n",
       " 'tolerance': 1e-06,\n",
       " 'DP': False,\n",
       " 'L': 1,\n",
       " 'C': 1,\n",
       " 'epsilon': 0.1,\n",
       " 'delta': 1e-05,\n",
       " 'new': 2}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.new = 2\n",
    "model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fe3990f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_classes': 2,\n",
       " 'alpha': 0.1,\n",
       " 'max_iter': 100,\n",
       " 'lambda_': 0.1,\n",
       " 'tolerance': 1e-06,\n",
       " 'DP': False,\n",
       " 'L': 1,\n",
       " 'C': 1,\n",
       " 'epsilon': 0.1,\n",
       " 'delta': 1e-05}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2['train_acc'] =="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67b0e6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_classes': 2,\n",
       " 'alpha': 0.1,\n",
       " 'max_iter': 100,\n",
       " 'lambda_': 0.1,\n",
       " 'tolerance': 1e-06,\n",
       " 'DP': False,\n",
       " 'L': 1,\n",
       " 'C': 1,\n",
       " 'epsilon': 0.1,\n",
       " 'delta': 1e-05}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('test_model_param_save.json', 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9938913",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "model.n_classes      = 100\n",
    "model.alpha          = 0.001\n",
    "model.max_iter       = 100*X_train_size \n",
    "model.lambda_        = 1e-3\n",
    "model.tolerance      = 10e-5\n",
    "model.DP             = True\n",
    "model.L              = L\n",
    "model.C              = C\n",
    "model.epsilon        = 1\n",
    "model.delta          = delta\n",
    "\n",
    "# X,y = model.init_theta(x_target_train, y_target_train)\n",
    "# model.SGD(X,y)\n",
    "# model.evaluate(x_target_train, y_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89b53a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0,1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18920960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model : 99.6 %\n",
      "The accuracy of the model : 53.900000000000006 %\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "model.evaluate(x_target_test, y_target_test, acc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a56f5839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tm/lr0.001_iter100_reg0.001_DPTrue_eps1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm_path = f'tm/lr{model.alpha}_iter{int(model.max_iter/X_train_size)}_reg{model.lambda_}_DP{model.DP}'\n",
    "if model.DP:\n",
    "    tm_path += f'_eps{model.epsilon}'\n",
    "tm_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d91ec17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(tm_path+'_target_model', model.theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e91b03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(tm_path+'_target_model.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dce2b523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59170522, 0.10471406, 0.57134466, ..., 0.50253035, 0.577948  ,\n",
       "        0.54217507],\n",
       "       [0.36787926, 0.36787926, 0.36787926, ..., 0.36787926, 0.36787926,\n",
       "        0.36787926],\n",
       "       [0.35040232, 0.46746699, 0.450361  , ..., 0.18051937, 0.52409756,\n",
       "        0.52919821],\n",
       "       ...,\n",
       "       [0.27549225, 0.41194471, 0.36066505, ..., 0.40835602, 0.49085802,\n",
       "        0.43304051],\n",
       "       [0.40733633, 0.3410984 , 0.48951214, ..., 0.30980288, 0.64702545,\n",
       "        0.35198058],\n",
       "       [0.54315534, 0.49553767, 0.33415502, ..., 0.49521893, 0.4018193 ,\n",
       "        0.29335622]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13d44d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/0p0lnj8d6zj532q0yvm1zt580000gn/T/ipykernel_6751/3818404637.py:7: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_shadow_train = np.array(y_shadow[:shadow_size])\n",
      "/var/folders/66/0p0lnj8d6zj532q0yvm1zt580000gn/T/ipykernel_6751/3818404637.py:9: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_shadow_test = np.array(y_shadow[shadow_size:2*shadow_size])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shadow model:  0\n",
      "The accuracy of the model : 93.5 %\n",
      "Confusion Matrix:\n",
      " [[41  0  0 ...  0  0  0]\n",
      " [ 0 34  0 ...  0  0  0]\n",
      " [ 0  0 28 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 41  0  0]\n",
      " [ 0  0  0 ...  0 56  0]\n",
      " [ 0  0  0 ...  0  0 45]]\n",
      "The accuracy of the model : 44.1 %\n",
      "Confusion Matrix:\n",
      " [[12  0  0 ...  0  0  1]\n",
      " [ 0 12  0 ...  0  0  0]\n",
      " [ 0  0 13 ...  0  0  0]\n",
      " ...\n",
      " [ 2  0  0 ... 15  0  0]\n",
      " [ 0  0  0 ...  0 28  1]\n",
      " [ 0  0  0 ...  1  3  9]]\n",
      "Shadow model:  1\n",
      "The accuracy of the model : 94.19999999999999 %\n",
      "Confusion Matrix:\n",
      " [[54  0  0 ...  0  0  0]\n",
      " [ 0 23  0 ...  0  0  0]\n",
      " [ 0  0 31 ...  0  0  0]\n",
      " ...\n",
      " [ 3  0  0 ... 19  0  0]\n",
      " [ 0  0  0 ...  0 46  0]\n",
      " [ 0  0  0 ...  0  0 35]]\n",
      "The accuracy of the model : 45.300000000000004 %\n",
      "Confusion Matrix:\n",
      " [[29  0  0 ...  0  1  0]\n",
      " [ 0 19  0 ...  0  0  0]\n",
      " [ 0  0 12 ...  0  0  0]\n",
      " ...\n",
      " [ 6  0  0 ...  0  0  0]\n",
      " [ 1  0  0 ...  0 39  0]\n",
      " [ 1  0  0 ...  0  2 12]]\n",
      "Shadow model:  2\n",
      "The accuracy of the model : 94.3 %\n",
      "Confusion Matrix:\n",
      " [[34  0  0 ...  0  0  0]\n",
      " [ 0 39  0 ...  0  0  0]\n",
      " [ 0  0 30 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 21  0  0]\n",
      " [ 0  0  0 ...  0 64  0]\n",
      " [ 0  0  0 ...  0  1 42]]\n",
      "The accuracy of the model : 45.300000000000004 %\n",
      "Confusion Matrix:\n",
      " [[11  0  0 ...  0  2  0]\n",
      " [ 0 18  0 ...  0  0  0]\n",
      " [ 0  0 16 ...  0  0  0]\n",
      " ...\n",
      " [ 4  0  0 ...  3  1  1]\n",
      " [ 0  0  0 ...  0 47  0]\n",
      " [ 0  0  0 ...  0  9 17]]\n",
      "Shadow model:  3\n",
      "The accuracy of the model : 93.60000000000001 %\n",
      "Confusion Matrix:\n",
      " [[33  0  0 ...  0  0  0]\n",
      " [ 0 33  0 ...  0  0  0]\n",
      " [ 0  0  5 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 18  0  0]\n",
      " [ 0  0  0 ...  0 60  0]\n",
      " [ 0  0  0 ...  0  0 40]]\n",
      "The accuracy of the model : 42.9 %\n",
      "Confusion Matrix:\n",
      " [[ 3  0  0 ...  0  0  0]\n",
      " [ 0 18  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  1  0]\n",
      " [ 0  0  0 ...  0 55  0]\n",
      " [ 0  0  0 ...  0  9  7]]\n",
      "Shadow model:  4\n",
      "The accuracy of the model : 93.60000000000001 %\n",
      "Confusion Matrix:\n",
      " [[46  0  0 ...  0  0  0]\n",
      " [ 0 19  0 ...  0  0  0]\n",
      " [ 0  0 12 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 28  0  0]\n",
      " [ 0  0  0 ...  0 66  0]\n",
      " [ 0  0  0 ...  0  1 59]]\n",
      "The accuracy of the model : 44.3 %\n",
      "Confusion Matrix:\n",
      " [[23  0  0 ...  0  4  2]\n",
      " [ 0  9  0 ...  0  0  0]\n",
      " [ 0  0  6 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  5  0  0]\n",
      " [ 0  0  0 ...  0 54  1]\n",
      " [ 0  0  0 ...  0  9 28]]\n",
      "Shadow model:  5\n",
      "The accuracy of the model : 95.0 %\n",
      "Confusion Matrix:\n",
      " [[43  0  0 ...  0  0  0]\n",
      " [ 0 49  0 ...  0  0  0]\n",
      " [ 0  0  9 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 32  0  0]\n",
      " [ 0  0  0 ...  0 60  0]\n",
      " [ 0  0  0 ...  0  0 41]]\n",
      "The accuracy of the model : 45.300000000000004 %\n",
      "Confusion Matrix:\n",
      " [[18  0  0 ...  0  1  0]\n",
      " [ 0 11  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 1  0  0 ...  3  0  0]\n",
      " [ 0  0  0 ...  0 27  1]\n",
      " [ 0  0  0 ...  0  2 10]]\n",
      "Shadow model:  6\n",
      "The accuracy of the model : 93.7 %\n",
      "Confusion Matrix:\n",
      " [[18  0  0 ...  0  1  0]\n",
      " [ 0 15  0 ...  0  0  0]\n",
      " [ 0  0 20 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 22  0  0]\n",
      " [ 0  0  0 ...  0 62  0]\n",
      " [ 0  0  0 ...  0  0 42]]\n",
      "The accuracy of the model : 44.2 %\n",
      "Confusion Matrix:\n",
      " [[ 1  0  0 ...  0  5  0]\n",
      " [ 0 12  0 ...  0  0  0]\n",
      " [ 0  0 15 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  2  0  0]\n",
      " [ 0  0  0 ...  0 48  1]\n",
      " [ 0  0  0 ...  0 13 15]]\n",
      "Shadow model:  7\n",
      "The accuracy of the model : 93.4 %\n",
      "Confusion Matrix:\n",
      " [[37  0  0 ...  0  1  0]\n",
      " [ 0 32  0 ...  0  0  0]\n",
      " [ 0  0 20 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 21  0  0]\n",
      " [ 0  0  0 ...  0 64  0]\n",
      " [ 0  0  0 ...  0  2 35]]\n",
      "The accuracy of the model : 44.0 %\n",
      "Confusion Matrix:\n",
      " [[ 3  0  0 ...  0  6  0]\n",
      " [ 0 17  0 ...  0  0  0]\n",
      " [ 0  0  2 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  1  1  0]\n",
      " [ 0  0  0 ...  0 54  0]\n",
      " [ 0  0  0 ...  0 12  8]]\n",
      "Shadow model:  8\n",
      "The accuracy of the model : 93.8 %\n",
      "Confusion Matrix:\n",
      " [[47  0  0 ...  0  0  0]\n",
      " [ 0 33  0 ...  0  0  0]\n",
      " [ 0  0 30 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 23  0  1]\n",
      " [ 0  0  0 ...  0 59  0]\n",
      " [ 0  0  0 ...  0  0 43]]\n",
      "The accuracy of the model : 44.6 %\n",
      "Confusion Matrix:\n",
      " [[15  0  0 ...  0  0  0]\n",
      " [ 0 24  0 ...  0  0  0]\n",
      " [ 0  0 20 ...  0  0  0]\n",
      " ...\n",
      " [ 6  0  0 ...  3  0  1]\n",
      " [ 0  0  0 ...  0 52  1]\n",
      " [ 0  0  0 ...  0 12 14]]\n",
      "Shadow model:  9\n",
      "The accuracy of the model : 93.8 %\n",
      "Confusion Matrix:\n",
      " [[56  0  0 ...  0  0  0]\n",
      " [ 0 44  0 ...  0  0  0]\n",
      " [ 0  0 14 ...  0  0  0]\n",
      " ...\n",
      " [ 1  0  0 ... 24  0  0]\n",
      " [ 0  0  0 ...  0 72  0]\n",
      " [ 0  0  0 ...  0  2 46]]\n",
      "The accuracy of the model : 43.1 %\n",
      "Confusion Matrix:\n",
      " [[26  0  0 ...  0  1  0]\n",
      " [ 0 20  0 ...  0  0  0]\n",
      " [ 0  0  2 ...  0  0  0]\n",
      " ...\n",
      " [ 7  0  0 ...  3  0  0]\n",
      " [ 0  0  0 ...  0 58  1]\n",
      " [ 1  0  0 ...  0 17 15]]\n"
     ]
    }
   ],
   "source": [
    "s_ms = {}\n",
    "number_of_sms = 10\n",
    "shadow_size = 50000\n",
    "shadow_batch_size = int(shadow_size/number_of_sms)\n",
    "\n",
    "x_shadow_train = np.array(x_shadow[:shadow_size])\n",
    "y_shadow_train = np.array(y_shadow[:shadow_size])\n",
    "x_shadow_test = np.array(x_shadow[shadow_size:2*shadow_size])\n",
    "y_shadow_test = np.array(y_shadow[shadow_size:2*shadow_size])\n",
    "\n",
    "for i in range(number_of_sms):\n",
    "    batch_start = i*shadow_batch_size\n",
    "    batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "    shadow_model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "    shadow_model.n_classes      = 100\n",
    "    shadow_model.alpha          = 0.001\n",
    "    shadow_model.max_iter       = 100*shadow_batch_size\n",
    "    shadow_model.lambda_        = 10e-3\n",
    "    shadow_model.tolerance      = 10e-5\n",
    "    shadow_model.DP             = False\n",
    "\n",
    "    X,y = shadow_model.init_theta(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end] )\n",
    "    shadow_model.SGD(X,y)\n",
    "    print('Shadow model: ', i)\n",
    "    shadow_model.evaluate(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end])\n",
    "    shadow_model.evaluate(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end])\n",
    "    s_ms[i] = shadow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27297dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_ms[0].evaluate(x_shadow_train, y_shadow_train)\n",
    "# s_ms[0].evaluate(x_shadow_test, y_shadow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f14b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_train_pred = []\n",
    "shadow_test_pred = []\n",
    "\n",
    "for i in range(number_of_sms): \n",
    "    batch_start = i*shadow_batch_size\n",
    "    batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "    train_prediciton = s_ms[i].predict(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end])\n",
    "    test_prediciton = s_ms[i].predict(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end])\n",
    "    \n",
    "    shadow_train_pred.append(train_prediciton)\n",
    "    shadow_test_pred.append(test_prediciton)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "161a1a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_shadow_train_ohe = OneHotEncoder(sparse=False).fit_transform(y_shadow_train.reshape(-1,1)) #encoode the target values\n",
    "y_shadow_test_ohe = OneHotEncoder(sparse=False).fit_transform(y_shadow_test.reshape(-1,1)) #encoode the target values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0853eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_train_pred = np.concatenate(shadow_train_pred)\n",
    "sh_test_pred = np.concatenate(shadow_test_pred)\n",
    "\n",
    "# members\n",
    "labels = np.ones(sh_train_pred.shape[0])\n",
    "# non-members\n",
    "test_labels = np.zeros(sh_test_pred.shape[0])\n",
    "\n",
    "x_1 = np.concatenate((sh_train_pred, sh_test_pred))\n",
    "x_2 = np.concatenate((y_shadow_train_ohe, y_shadow_test_ohe))#.reshape((-1, 1))\n",
    "y_new = np.concatenate((labels, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4f766dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_train_data = np.concatenate((x_1,x_2),axis=1)\n",
    "attack_train_target = y_new\n",
    "df = pd.DataFrame(attack_train_data)\n",
    "df['a_target'] = attack_train_target\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)\n",
    "attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7df18553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)   \n",
    "# attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)\n",
    "attack_train_args = Train_args(learning_rate=0.001, weight_decay=1e-6, epoch=200)\n",
    "attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "for epoch in range(attack_train_args.epoch):\n",
    "            \n",
    "    attack_model = train_model(attack_model, attack_train_data, attack_train_target, attack_train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c896104",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_model.n_classes      = 100\n",
    "shadow_model.alpha          = 0.001\n",
    "shadow_model.max_iter       = 100*shadow_batch_size\n",
    "shadow_model.lambda_        = 10e-3\n",
    "shadow_model.tolerance      = 10e-5\n",
    "shadow_model.DP             = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a72152f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c737ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_path = f'mia/shms{number_of_sms}_shtrsize{shadow_size}_shlr{shadow_model.alpha}_shiter{int(shadow_model.max_iter/shadow_batch_size)}_shreg{shadow_model.lambda_}/'      \n",
    "os.mkdir(sh_path)\n",
    "\n",
    "torch.save(attack_train_data, sh_path+'attack_train_data.pt')\n",
    "torch.save(attack_train_target, sh_path+'attack_train_target.pt')\n",
    "\n",
    "at_path = sh_path+f'attack_model_aneur{attack_model.h_neurons}_ado{attack_model.do}_alr{attack_train_args.learning_rate}_alreg{attack_train_args.weight_decay}_aepoch{attack_train_args.epoch}'\n",
    "torch.save(attack_model, at_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6f404bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.59217, 0.5930014328093153, 0.5877)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)\n",
    "train_acc, train_pre, train_rec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3c6f7e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6260389215487533, 0.5935737236701178, 0.8313)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, test_pre, test_rec = attack_evaluation(attack_model, attack_test_data, attack_test_target)\n",
    "test_acc, test_pre, test_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09df009",
   "metadata": {},
   "outputs": [],
   "source": [
    "ams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "07b5b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "# random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "\n",
    "epo = [200]\n",
    "wd = [1e-5, 1e-6]\n",
    "ams = {}\n",
    "for ep in epo:\n",
    "    for w_d in wd:\n",
    "        attack_train_args = Train_args(learning_rate=l_r, weight_decay=w_d, epoch=ep)\n",
    "        attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "        for epoch in range(attack_train_args.epoch):\n",
    "\n",
    "            attack_model = train_model(attack_model, attack_train_data, attack_train_target, attack_train_args)\n",
    "            ams[(l_r,w_d)] = attack_model\n",
    "        at_path = sh_path+f'attack_model_aneur{attack_model.h_neurons}_ado{attack_model.do}_alr{attack_train_args.learning_rate}_alreg{attack_train_args.weight_decay}_aepoch{attack_train_args.epoch}'\n",
    "        torch.save(attack_model, at_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c6b59ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 1e-05\n",
      "Train:  0.8081 0.8207236842105263 0.78842\n",
      "Test:  0.6895398337725522 0.6209717711716213 0.9943\n",
      "200 1e-06\n",
      "Train:  0.80328 0.8273606493674712 0.7665\n",
      "Test:  0.7008919521589296 0.6304929308368361 0.99\n",
      "500 1e-05\n",
      "Train:  0.8081 0.8207236842105263 0.78842\n",
      "Test:  0.6895398337725522 0.6209717711716213 0.9943\n",
      "500 1e-06\n",
      "Train:  0.80328 0.8273606493674712 0.7665\n",
      "Test:  0.7008919521589296 0.6304929308368361 0.99\n"
     ]
    }
   ],
   "source": [
    "epo = [200,500]\n",
    "wd = [1e-5, 1e-6]\n",
    "\n",
    "for ep in epo:\n",
    "    for w_d in wd:\n",
    "        print(ep, w_d)\n",
    "        attack_train_args = Train_args(learning_rate=l_r, weight_decay=w_d, epoch=500)\n",
    "        attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "        at_path = sh_path+f'attack_model_aneur{attack_model.h_neurons}_ado{attack_model.do}_alr{attack_train_args.learning_rate}_alreg{attack_train_args.weight_decay}_aepoch{attack_train_args.epoch}'\n",
    "        attack_model = torch.load(at_path)\n",
    "        \n",
    "        train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)\n",
    "        print('Train: ', train_acc, train_pre, train_rec)\n",
    "        test_acc, test_pre, test_rec = attack_evaluation(attack_model, attack_test_data, attack_test_target)\n",
    "        print('Test: ' , test_acc, test_pre, test_rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l_r in lr:\n",
    "    for w_d in wd:\n",
    "        ams[(l_r,w_d)]\n",
    "        train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)\n",
    "        print(\"Train acc pre rec: \", train_acc, train_pre, train_rec)\n",
    "        test_acc, test_pre, test_rec = attack_evaluation(attack_model, attack_test_data, attack_test_target)\n",
    "        print(\"Test acc pre rec: \", test_acc, test_pre, test_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9eafae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32530f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_size = 20000\n",
    "shadow_clf = LogisticRegression(random_state=1).fit(x_shadow[:shadow_size], y_shadow[:shadow_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03307bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shadow_clf.score(x_shadow[:shadow_size], y_shadow[:shadow_size]))\n",
    "print(shadow_clf.score(x_shadow[shadow_size:shadow_size*2], y_shadow[shadow_size:shadow_size*2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789976e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_train_pred = shadow_clf.predict_proba(x_shadow[:shadow_size])\n",
    "shadow_test_pred = shadow_clf.predict_proba(x_shadow[shadow_size:shadow_size*2])\n",
    "y_shadow_train = y_shadow[:shadow_size]\n",
    "y_shadow_test = y_shadow[shadow_size:shadow_size*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe53777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# members\n",
    "labels = np.ones(shadow_train_pred.shape[0])\n",
    "# non-members\n",
    "test_labels = np.zeros(shadow_test_pred.shape[0])\n",
    "\n",
    "x_1 = np.concatenate((shadow_train_pred, shadow_test_pred))\n",
    "x_2 = np.concatenate((y_shadow_train, y_shadow_test)).reshape((-1, 1))\n",
    "y_new = np.concatenate((labels, test_labels))\n",
    "\n",
    "attack_train_data = np.concatenate((x_1,x_2),axis=1)\n",
    "attack_train_target = y_new\n",
    "df = pd.DataFrame(attack_train_data)\n",
    "df['a_target'] = attack_train_target\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "attack_train_data = np.array(df.drop(['a_target'], axis=1))\n",
    "attack_train_target = np.array(df['a_target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3072ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)   \n",
    "attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)\n",
    "attack_train_args = Train_args(learning_rate=0.001, weight_decay=1e-3, epoch=200)\n",
    "attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d0277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)   \n",
    "attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)\n",
    "attack_train_args = Train_args(learning_rate=0.001, weight_decay=1e-3, epoch=500)\n",
    "attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "for epoch in range(attack_train_args.epoch):\n",
    "            \n",
    "    attack_model = train_model(attack_model, attack_train_data, attack_train_target, attack_train_args)\n",
    "    train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262101cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, train_pre, train_rec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb726788",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(x_target_train, y_target_train)\n",
    "test_pred = model.predict(x_target_test, y_target_test)\n",
    "\n",
    "y_target_train_ohe = OneHotEncoder(sparse=False).fit_transform(y_target_train.reshape(-1,1)) #encoode the target values\n",
    "y_target_test_ohe = OneHotEncoder(sparse=False).fit_transform(y_target_test.reshape(-1,1)) #encoode the target values\n",
    "    \n",
    "# members\n",
    "labels = np.ones(train_pred.shape[0])\n",
    "# non-members\n",
    "test_labels = np.zeros(test_pred.shape[0])\n",
    "\n",
    "x_1 = np.concatenate((train_pred, test_pred))\n",
    "x_2 = np.concatenate((y_target_train_ohe, y_target_test_ohe))#.reshape((-1, 1))\n",
    "y_new = np.concatenate((labels, test_labels))\n",
    "\n",
    "attack_test_data = np.concatenate((x_1,x_2),axis=1)\n",
    "attack_test_target = y_new\n",
    "df = pd.DataFrame(attack_test_data)\n",
    "df['a_target'] = attack_test_target\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "attack_test_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)\n",
    "attack_test_target = torch.tensor(np.array(df['a_target']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f31e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_pre, test_rec = attack_evaluation(a_model, attack_test_data, attack_test_target)\n",
    "test_acc, test_pre, test_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc65931",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_model = Net_attack(h_neurons=64, do=0)\n",
    "a_model = torch.load('attack_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45652ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "# random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "\n",
    "lr = [0.001]\n",
    "wd = [1e-4, 1e-6]\n",
    "tms = {}\n",
    "for l_r in lr:\n",
    "    for w_d in wd:\n",
    "        model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "        model.n_classes      = 100\n",
    "        model.alpha          = 0.001\n",
    "        model.max_iter       = 100*X_train_size \n",
    "        model.lambda_        = 1e-3\n",
    "        model.tolerance      = 10e-5\n",
    "        model.DP             = True\n",
    "        model.epsilon\n",
    "\n",
    "        X,y = model.init_theta(x_target_train, y_target_train)\n",
    "        model.SGD(X,y)\n",
    "        model.evaluate(x_target_train, y_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a690be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import algo\n",
    "# import attack\n",
    "\n",
    "# from torch import nn,optim\n",
    "import torch\n",
    "\n",
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "        \n",
    "\n",
    "raw_data_path = '../datasets/dataset_purchase'\n",
    "raw_data = pd.read_csv(raw_data_path)\n",
    "y=raw_data['63']\n",
    "X=raw_data.drop('63', axis=1)\n",
    "y =  y.replace(100, 0)\n",
    "print('Dataset: ', raw_data_path)\n",
    "print('Classes in classification task: ', y.nunique())\n",
    "n_classes = y.nunique()\n",
    "\n",
    "X_train, x_shadow, y_train, y_shadow = train_test_split(X, y, train_size=0.2, random_state=rand_seed)\n",
    "print(X_train.shape, x_shadow.shape)\n",
    "\n",
    "#Target model\n",
    "X_train_size = 10000\n",
    "X_test_size = 10000\n",
    "x_target_train = np.array(X_train[:X_train_size])\n",
    "y_target_train = np.array(y_train[:X_train_size])\n",
    "x_target_test = np.array(X_train[X_train_size:X_train_size+X_test_size])\n",
    "y_target_test = np.array(y_train[X_train_size:X_train_size+X_test_size])\n",
    "if y_target_test.shape[0]<X_test_size or y_target_train.shape[0]<X_train_size:\n",
    "    raise ValueError(\n",
    "            \"Not enough traning or test data for the target model\")\n",
    "\n",
    "for L in [1,10,100]:\n",
    "    for epsilon in np.arange(0,1,0.1):\n",
    "        model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "        model.n_classes      = n_classes\n",
    "        model.alpha          = 0.001\n",
    "        model.max_iter       = 100*X_train_size\n",
    "        model.lambda_        = 1e-3\n",
    "        model.tolerance      = 10e-5\n",
    "        model.DP             = True\n",
    "        model.L              = L\n",
    "        model.epsilon        = epsilon\n",
    "\n",
    "\n",
    "        X,y = model.init_theta(x_target_train, y_target_train)\n",
    "        model.train(X,y)\n",
    "        model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "        model.evaluate(x_target_test, y_target_test, acc=True)\n",
    "\n",
    "        tm_path = f'tm/lr{model.alpha}_iter{int(model.max_iter/X_train_size)}_reg{model.lambda_}_DP{model.DP}'\n",
    "        if model.DP:\n",
    "            tm_path += f'_eps{model.epsilon}_L{model.L}'\n",
    "        np.save(tm_path+'_target_model', model.theta)\n",
    "\n",
    "        print(tm_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0fb92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shadow models\n",
    "# s_ms = {}\n",
    "# number_of_sms = 10\n",
    "# shadow_size = 50000\n",
    "# shadow_batch_size = int(shadow_size/number_of_sms)\n",
    "\n",
    "# x_shadow_train = np.array(x_shadow[:shadow_size])\n",
    "# y_shadow_train = np.array(y_shadow[:shadow_size])\n",
    "# x_shadow_test = np.array(x_shadow[shadow_size:2*shadow_size])\n",
    "# y_shadow_test = np.array(y_shadow[shadow_size:2*shadow_size])\n",
    "\n",
    "# attack.train_shadow_models(number_of_sms,)\n",
    "\n",
    "# for i in range(number_of_sms):  \n",
    "#     batch_start = i*shadow_batch_size\n",
    "#     batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "#     shadow_model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "#     shadow_model.n_classes      = n_classes\n",
    "#     shadow_model.alpha          = 0.001\n",
    "#     shadow_model.max_iter       = 100*shadow_batch_size\n",
    "#     shadow_model.lambda_        = 10e-3\n",
    "#     shadow_model.tolerance      = 10e-5\n",
    "#     shadow_model.DP             = False\n",
    "\n",
    "#     X,y = shadow_model.init_theta(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end] )\n",
    "#     shadow_model.SGD(X,y)\n",
    "#     print('Shadow model: ', i)\n",
    "#     shadow_model.evaluate(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end])\n",
    "#     shadow_model.evaluate(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end])\n",
    "#     s_ms[i] = shadow_model\n",
    "\n",
    "# #Attack model\n",
    "\n",
    "# shadow_train_pred = []\n",
    "# shadow_test_pred = []\n",
    "\n",
    "# for i in range(number_of_sms): \n",
    "#     batch_start = i*shadow_batch_size\n",
    "#     batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "#     train_prediciton = s_ms[i].predict(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end])\n",
    "#     test_prediciton = s_ms[i].predict(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end])\n",
    "    \n",
    "#     shadow_train_pred.append(train_prediciton)\n",
    "#     shadow_test_pred.append(test_prediciton)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "007e1fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/10000 == 1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad90969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0,1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a0b2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
