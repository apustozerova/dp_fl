{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf2f0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 18:39:38.510437: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch import nn,optim\n",
    "import torch\n",
    "\n",
    "import algo\n",
    "\n",
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa0caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_attack(nn.Module):\n",
    "\n",
    "    def __init__(self, h_neurons, do, input_size):\n",
    "        super(Net_attack, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.h_neurons = h_neurons\n",
    "        self.do = do\n",
    "        self.fc1 = nn.Linear(input_size, h_neurons)\n",
    "        self.fc2 = nn.Linear(h_neurons, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(do)\n",
    "        self.softmax = nn.Softmax(dim=1)   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class Train_args():\n",
    "\n",
    "    def __init__(self, learning_rate, weight_decay, epoch):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epoch = epoch\n",
    "         \n",
    "def train_attack_model(model, train_data, train_target, train_args):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=train_args.learning_rate, weight_decay=train_args.weight_decay)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_data)\n",
    "    loss = nn.CrossEntropyLoss()(output, train_target.to(torch.long))\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "def attack_evaluation(model, x, y, dev=\"cpu\", extended=False):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output =  model(x)\n",
    "        out_target = output.argmax(1, keepdim=True)\n",
    "        correct = out_target.to(dev).eq(y.to(dev).view_as(out_target.to(dev))).sum().item()\n",
    "        acc = correct/y.shape[0]\n",
    "\n",
    "        predicted_positive = output.argmax(1, keepdim=True) == 1\n",
    "        labeled_positive = y == 1\n",
    "        tp = predicted_positive.to(dev) * labeled_positive.to(dev).view_as(out_target)\n",
    "        tp_count = tp.to(dev).sum().item()\n",
    "        \n",
    "        if predicted_positive.to(dev).sum().item() != 0:\n",
    "            pre = tp_count / predicted_positive.to(dev).sum().item()\n",
    "        else:\n",
    "            pre = 0\n",
    "        if labeled_positive.to(dev).sum().item() !=0:\n",
    "            rec = tp_count / labeled_positive.to(dev).sum().item()\n",
    "        else:\n",
    "            rec = 0\n",
    "    if extended:\n",
    "        predicted_negative = output.argmax(1, keepdim=True) == 0\n",
    "        labeled_negative = y == 0\n",
    "        tn = predicted_negative.to(dev) * labeled_negative.to(dev).view_as(out_target)\n",
    "        tn_count = tn.to(dev).sum().item()\n",
    "\n",
    "        fp_count = predicted_positive.to(dev).sum().item() - tp.to(dev).sum().item()\n",
    "        fn_count = labeled_positive.to(dev).sum().item() - tp.to(dev).sum().item()\n",
    "        \n",
    "        return acc, pre, rec, tp_count, tn_count, fp_count, fn_count\n",
    "    else:\n",
    "        return acc, pre, rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "684c6361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "(19732, 600) (177591, 600)\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('../datasets/dataset_purchase', )\n",
    "y=raw_data['63']\n",
    "X=raw_data.drop('63', axis=1)\n",
    "y =  y.replace(100, 0)\n",
    "print(y.nunique())\n",
    "\n",
    "X_train, x_shadow, y_train, y_shadow = train_test_split(X, y, train_size=0.1, random_state=42)\n",
    "print(X_train.shape, x_shadow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cce3a11",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_target_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43my_target_train\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_target_train' is not defined"
     ]
    }
   ],
   "source": [
    "y_target_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee07ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec427b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_target, y_target), (x_shadow, y_shadow), _, _ = load_nursery(test_set=0.75)\n",
    "\n",
    "X_train_size = 10000\n",
    "x_target_train = np.array(X_train[:X_train_size])\n",
    "y_target_train = np.array(y_train[:X_train_size])\n",
    "x_target_test = np.array(X_train[X_train_size:])\n",
    "y_target_test = np.array(y_train[X_train_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "828cb498",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = algo.LogisticRegression_DPSGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94cf190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.__dict__\n",
    "p2 = dict(model.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6586dd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_classes': 2,\n",
       " 'alpha': 0.1,\n",
       " 'max_iter': 100,\n",
       " 'lambda_': 0.1,\n",
       " 'tolerance': 1e-06,\n",
       " 'DP': False,\n",
       " 'L': 1,\n",
       " 'C': 1,\n",
       " 'epsilon': 0.1,\n",
       " 'delta': 1e-05,\n",
       " 'new': 2}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.new = 2\n",
    "model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b82c26f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_classes': 2,\n",
       " 'alpha': 0.1,\n",
       " 'max_iter': 100,\n",
       " 'lambda_': 0.1,\n",
       " 'tolerance': 1e-06,\n",
       " 'DP': False,\n",
       " 'L': 1,\n",
       " 'C': 1,\n",
       " 'epsilon': 0.1,\n",
       " 'delta': 1e-05}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2['train_acc'] =="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6305f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_classes': 2,\n",
       " 'alpha': 0.1,\n",
       " 'max_iter': 100,\n",
       " 'lambda_': 0.1,\n",
       " 'tolerance': 1e-06,\n",
       " 'DP': False,\n",
       " 'L': 1,\n",
       " 'C': 1,\n",
       " 'epsilon': 0.1,\n",
       " 'delta': 1e-05}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('test_model_param_save.json', 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77aad817",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "model.n_classes      = 100\n",
    "model.alpha          = 0.001\n",
    "model.max_iter       = 100*X_train_size \n",
    "model.lambda_        = 1e-3\n",
    "model.tolerance      = 10e-5\n",
    "model.DP             = True\n",
    "model.L              = L\n",
    "model.C              = C\n",
    "model.epsilon        = 1\n",
    "model.delta          = delta\n",
    "\n",
    "# X,y = model.init_theta(x_target_train, y_target_train)\n",
    "# model.SGD(X,y)\n",
    "# model.evaluate(x_target_train, y_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc6fa43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0,1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3727bc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model : 99.6 %\n",
      "The accuracy of the model : 53.900000000000006 %\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "model.evaluate(x_target_test, y_target_test, acc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0535ab1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tm_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtm/lr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39malpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_iter\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(model\u001b[38;5;241m.\u001b[39mmax_iter\u001b[38;5;241m/\u001b[39mX_train_size)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_reg\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mlambda_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_DP\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mDP\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mDP:\n\u001b[1;32m      3\u001b[0m     tm_path \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_eps\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mepsilon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "tm_path = f'tm/lr{model.alpha}_iter{int(model.max_iter/X_train_size)}_reg{model.lambda_}_DP{model.DP}'\n",
    "if model.DP:\n",
    "    tm_path += f'_eps{model.epsilon}'\n",
    "tm_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2c2a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(tm_path+'_target_model', model.theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c4b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('tm/rs13_lr0.001_iter100_reg1_DPFalse_target_model.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dec47683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.25885028e-03, -3.50712779e-03, -3.16824117e-03, ...,\n",
       "        -2.35133277e-03,  2.66860303e-03,  2.36332287e-03],\n",
       "       [ 3.53852769e-44,  3.53852769e-44,  3.53852769e-44, ...,\n",
       "         3.53852769e-44,  3.53852769e-44,  3.53852769e-44],\n",
       "       [-2.11209516e-03,  1.15879588e-03, -2.26797171e-03, ...,\n",
       "        -2.66929941e-03,  7.06403689e-04,  1.72851183e-03],\n",
       "       ...,\n",
       "       [-5.93230586e-04,  2.09928426e-03, -4.49838616e-04, ...,\n",
       "        -5.19930773e-04, -1.61702619e-04,  1.78996053e-04],\n",
       "       [-3.92912490e-04,  1.71878823e-03, -2.67540245e-04, ...,\n",
       "        -5.07596203e-04,  1.40477244e-03, -3.68554401e-05],\n",
       "       [ 1.66235219e-03,  3.19956322e-03, -7.49122078e-04, ...,\n",
       "         8.07593929e-05, -5.07559610e-04,  1.25629144e-04]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b14812a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/0p0lnj8d6zj532q0yvm1zt580000gn/T/ipykernel_6751/3818404637.py:7: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_shadow_train = np.array(y_shadow[:shadow_size])\n",
      "/var/folders/66/0p0lnj8d6zj532q0yvm1zt580000gn/T/ipykernel_6751/3818404637.py:9: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_shadow_test = np.array(y_shadow[shadow_size:2*shadow_size])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shadow model:  0\n",
      "The accuracy of the model : 93.5 %\n",
      "Confusion Matrix:\n",
      " [[41  0  0 ...  0  0  0]\n",
      " [ 0 34  0 ...  0  0  0]\n",
      " [ 0  0 28 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 41  0  0]\n",
      " [ 0  0  0 ...  0 56  0]\n",
      " [ 0  0  0 ...  0  0 45]]\n",
      "The accuracy of the model : 44.1 %\n",
      "Confusion Matrix:\n",
      " [[12  0  0 ...  0  0  1]\n",
      " [ 0 12  0 ...  0  0  0]\n",
      " [ 0  0 13 ...  0  0  0]\n",
      " ...\n",
      " [ 2  0  0 ... 15  0  0]\n",
      " [ 0  0  0 ...  0 28  1]\n",
      " [ 0  0  0 ...  1  3  9]]\n",
      "Shadow model:  1\n",
      "The accuracy of the model : 94.19999999999999 %\n",
      "Confusion Matrix:\n",
      " [[54  0  0 ...  0  0  0]\n",
      " [ 0 23  0 ...  0  0  0]\n",
      " [ 0  0 31 ...  0  0  0]\n",
      " ...\n",
      " [ 3  0  0 ... 19  0  0]\n",
      " [ 0  0  0 ...  0 46  0]\n",
      " [ 0  0  0 ...  0  0 35]]\n",
      "The accuracy of the model : 45.300000000000004 %\n",
      "Confusion Matrix:\n",
      " [[29  0  0 ...  0  1  0]\n",
      " [ 0 19  0 ...  0  0  0]\n",
      " [ 0  0 12 ...  0  0  0]\n",
      " ...\n",
      " [ 6  0  0 ...  0  0  0]\n",
      " [ 1  0  0 ...  0 39  0]\n",
      " [ 1  0  0 ...  0  2 12]]\n",
      "Shadow model:  2\n",
      "The accuracy of the model : 94.3 %\n",
      "Confusion Matrix:\n",
      " [[34  0  0 ...  0  0  0]\n",
      " [ 0 39  0 ...  0  0  0]\n",
      " [ 0  0 30 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 21  0  0]\n",
      " [ 0  0  0 ...  0 64  0]\n",
      " [ 0  0  0 ...  0  1 42]]\n",
      "The accuracy of the model : 45.300000000000004 %\n",
      "Confusion Matrix:\n",
      " [[11  0  0 ...  0  2  0]\n",
      " [ 0 18  0 ...  0  0  0]\n",
      " [ 0  0 16 ...  0  0  0]\n",
      " ...\n",
      " [ 4  0  0 ...  3  1  1]\n",
      " [ 0  0  0 ...  0 47  0]\n",
      " [ 0  0  0 ...  0  9 17]]\n",
      "Shadow model:  3\n",
      "The accuracy of the model : 93.60000000000001 %\n",
      "Confusion Matrix:\n",
      " [[33  0  0 ...  0  0  0]\n",
      " [ 0 33  0 ...  0  0  0]\n",
      " [ 0  0  5 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 18  0  0]\n",
      " [ 0  0  0 ...  0 60  0]\n",
      " [ 0  0  0 ...  0  0 40]]\n",
      "The accuracy of the model : 42.9 %\n",
      "Confusion Matrix:\n",
      " [[ 3  0  0 ...  0  0  0]\n",
      " [ 0 18  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  1  0]\n",
      " [ 0  0  0 ...  0 55  0]\n",
      " [ 0  0  0 ...  0  9  7]]\n",
      "Shadow model:  4\n",
      "The accuracy of the model : 93.60000000000001 %\n",
      "Confusion Matrix:\n",
      " [[46  0  0 ...  0  0  0]\n",
      " [ 0 19  0 ...  0  0  0]\n",
      " [ 0  0 12 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 28  0  0]\n",
      " [ 0  0  0 ...  0 66  0]\n",
      " [ 0  0  0 ...  0  1 59]]\n",
      "The accuracy of the model : 44.3 %\n",
      "Confusion Matrix:\n",
      " [[23  0  0 ...  0  4  2]\n",
      " [ 0  9  0 ...  0  0  0]\n",
      " [ 0  0  6 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  5  0  0]\n",
      " [ 0  0  0 ...  0 54  1]\n",
      " [ 0  0  0 ...  0  9 28]]\n",
      "Shadow model:  5\n",
      "The accuracy of the model : 95.0 %\n",
      "Confusion Matrix:\n",
      " [[43  0  0 ...  0  0  0]\n",
      " [ 0 49  0 ...  0  0  0]\n",
      " [ 0  0  9 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 32  0  0]\n",
      " [ 0  0  0 ...  0 60  0]\n",
      " [ 0  0  0 ...  0  0 41]]\n",
      "The accuracy of the model : 45.300000000000004 %\n",
      "Confusion Matrix:\n",
      " [[18  0  0 ...  0  1  0]\n",
      " [ 0 11  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 1  0  0 ...  3  0  0]\n",
      " [ 0  0  0 ...  0 27  1]\n",
      " [ 0  0  0 ...  0  2 10]]\n",
      "Shadow model:  6\n",
      "The accuracy of the model : 93.7 %\n",
      "Confusion Matrix:\n",
      " [[18  0  0 ...  0  1  0]\n",
      " [ 0 15  0 ...  0  0  0]\n",
      " [ 0  0 20 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 22  0  0]\n",
      " [ 0  0  0 ...  0 62  0]\n",
      " [ 0  0  0 ...  0  0 42]]\n",
      "The accuracy of the model : 44.2 %\n",
      "Confusion Matrix:\n",
      " [[ 1  0  0 ...  0  5  0]\n",
      " [ 0 12  0 ...  0  0  0]\n",
      " [ 0  0 15 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  2  0  0]\n",
      " [ 0  0  0 ...  0 48  1]\n",
      " [ 0  0  0 ...  0 13 15]]\n",
      "Shadow model:  7\n",
      "The accuracy of the model : 93.4 %\n",
      "Confusion Matrix:\n",
      " [[37  0  0 ...  0  1  0]\n",
      " [ 0 32  0 ...  0  0  0]\n",
      " [ 0  0 20 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 21  0  0]\n",
      " [ 0  0  0 ...  0 64  0]\n",
      " [ 0  0  0 ...  0  2 35]]\n",
      "The accuracy of the model : 44.0 %\n",
      "Confusion Matrix:\n",
      " [[ 3  0  0 ...  0  6  0]\n",
      " [ 0 17  0 ...  0  0  0]\n",
      " [ 0  0  2 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  1  1  0]\n",
      " [ 0  0  0 ...  0 54  0]\n",
      " [ 0  0  0 ...  0 12  8]]\n",
      "Shadow model:  8\n",
      "The accuracy of the model : 93.8 %\n",
      "Confusion Matrix:\n",
      " [[47  0  0 ...  0  0  0]\n",
      " [ 0 33  0 ...  0  0  0]\n",
      " [ 0  0 30 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 23  0  1]\n",
      " [ 0  0  0 ...  0 59  0]\n",
      " [ 0  0  0 ...  0  0 43]]\n",
      "The accuracy of the model : 44.6 %\n",
      "Confusion Matrix:\n",
      " [[15  0  0 ...  0  0  0]\n",
      " [ 0 24  0 ...  0  0  0]\n",
      " [ 0  0 20 ...  0  0  0]\n",
      " ...\n",
      " [ 6  0  0 ...  3  0  1]\n",
      " [ 0  0  0 ...  0 52  1]\n",
      " [ 0  0  0 ...  0 12 14]]\n",
      "Shadow model:  9\n",
      "The accuracy of the model : 93.8 %\n",
      "Confusion Matrix:\n",
      " [[56  0  0 ...  0  0  0]\n",
      " [ 0 44  0 ...  0  0  0]\n",
      " [ 0  0 14 ...  0  0  0]\n",
      " ...\n",
      " [ 1  0  0 ... 24  0  0]\n",
      " [ 0  0  0 ...  0 72  0]\n",
      " [ 0  0  0 ...  0  2 46]]\n",
      "The accuracy of the model : 43.1 %\n",
      "Confusion Matrix:\n",
      " [[26  0  0 ...  0  1  0]\n",
      " [ 0 20  0 ...  0  0  0]\n",
      " [ 0  0  2 ...  0  0  0]\n",
      " ...\n",
      " [ 7  0  0 ...  3  0  0]\n",
      " [ 0  0  0 ...  0 58  1]\n",
      " [ 1  0  0 ...  0 17 15]]\n"
     ]
    }
   ],
   "source": [
    "s_ms = {}\n",
    "number_of_sms = 10\n",
    "shadow_size = 50000\n",
    "shadow_batch_size = int(shadow_size/number_of_sms)\n",
    "\n",
    "x_shadow_train = np.array(x_shadow[:shadow_size])\n",
    "y_shadow_train = np.array(y_shadow[:shadow_size])\n",
    "x_shadow_test = np.array(x_shadow[shadow_size:2*shadow_size])\n",
    "y_shadow_test = np.array(y_shadow[shadow_size:2*shadow_size])\n",
    "\n",
    "for i in range(number_of_sms):\n",
    "    batch_start = i*shadow_batch_size\n",
    "    batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "    shadow_model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "    shadow_model.n_classes      = 100\n",
    "    shadow_model.alpha          = 0.001\n",
    "    shadow_model.max_iter       = 100*shadow_batch_size\n",
    "    shadow_model.lambda_        = 10e-3\n",
    "    shadow_model.tolerance      = 10e-5\n",
    "    shadow_model.DP             = False\n",
    "\n",
    "    X,y = shadow_model.init_theta(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end] )\n",
    "    shadow_model.SGD(X,y)\n",
    "    print('Shadow model: ', i)\n",
    "    shadow_model.evaluate(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end])\n",
    "    shadow_model.evaluate(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end])\n",
    "    s_ms[i] = shadow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967039a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_ms[0].evaluate(x_shadow_train, y_shadow_train)\n",
    "# s_ms[0].evaluate(x_shadow_test, y_shadow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "600e22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_train_pred = []\n",
    "shadow_test_pred = []\n",
    "\n",
    "for i in range(number_of_sms): \n",
    "    batch_start = i*shadow_batch_size\n",
    "    batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "    train_prediciton = s_ms[i].predict(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end])\n",
    "    test_prediciton = s_ms[i].predict(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end])\n",
    "    \n",
    "    shadow_train_pred.append(train_prediciton)\n",
    "    shadow_test_pred.append(test_prediciton)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49a69604",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_shadow_train_ohe = OneHotEncoder(sparse=False).fit_transform(y_shadow_train.reshape(-1,1)) #encoode the target values\n",
    "y_shadow_test_ohe = OneHotEncoder(sparse=False).fit_transform(y_shadow_test.reshape(-1,1)) #encoode the target values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41420b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_train_pred = np.concatenate(shadow_train_pred)\n",
    "sh_test_pred = np.concatenate(shadow_test_pred)\n",
    "\n",
    "# members\n",
    "labels = np.ones(sh_train_pred.shape[0])\n",
    "# non-members\n",
    "test_labels = np.zeros(sh_test_pred.shape[0])\n",
    "\n",
    "x_1 = np.concatenate((sh_train_pred, sh_test_pred))\n",
    "x_2 = np.concatenate((y_shadow_train_ohe, y_shadow_test_ohe))#.reshape((-1, 1))\n",
    "y_new = np.concatenate((labels, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e11035f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_train_data = np.concatenate((x_1,x_2),axis=1)\n",
    "attack_train_target = y_new\n",
    "df = pd.DataFrame(attack_train_data)\n",
    "df['a_target'] = attack_train_target\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)\n",
    "attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "399bf1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)   \n",
    "# attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)\n",
    "attack_train_args = Train_args(learning_rate=0.001, weight_decay=1e-6, epoch=200)\n",
    "attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "for epoch in range(attack_train_args.epoch):\n",
    "            \n",
    "    attack_model = train_model(attack_model, attack_train_data, attack_train_target, attack_train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f58b1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_model.n_classes      = 100\n",
    "shadow_model.alpha          = 0.001\n",
    "shadow_model.max_iter       = 100*shadow_batch_size\n",
    "shadow_model.lambda_        = 10e-3\n",
    "shadow_model.tolerance      = 10e-5\n",
    "shadow_model.DP             = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "017c62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b459d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_path = f'mia/shms{number_of_sms}_shtrsize{shadow_size}_shlr{shadow_model.alpha}_shiter{int(shadow_model.max_iter/shadow_batch_size)}_shreg{shadow_model.lambda_}/'      \n",
    "os.mkdir(sh_path)\n",
    "\n",
    "torch.save(attack_train_data, sh_path+'attack_train_data.pt')\n",
    "torch.save(attack_train_target, sh_path+'attack_train_target.pt')\n",
    "\n",
    "at_path = sh_path+f'attack_model_aneur{attack_model.h_neurons}_ado{attack_model.do}_alr{attack_train_args.learning_rate}_alreg{attack_train_args.weight_decay}_aepoch{attack_train_args.epoch}'\n",
    "torch.save(attack_model, at_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6a77dac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.59217, 0.5930014328093153, 0.5877)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)\n",
    "train_acc, train_pre, train_rec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "61f91b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6260389215487533, 0.5935737236701178, 0.8313)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, test_pre, test_rec = attack_evaluation(attack_model, attack_test_data, attack_test_target)\n",
    "test_acc, test_pre, test_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc286901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ffcaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac16b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "# random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "\n",
    "epo = [200]\n",
    "wd = [1e-5, 1e-6]\n",
    "ams = {}\n",
    "for ep in epo:\n",
    "    for w_d in wd:\n",
    "        attack_train_args = Train_args(learning_rate=l_r, weight_decay=w_d, epoch=ep)\n",
    "        attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "        for epoch in range(attack_train_args.epoch):\n",
    "\n",
    "            attack_model = train_model(attack_model, attack_train_data, attack_train_target, attack_train_args)\n",
    "            ams[(l_r,w_d)] = attack_model\n",
    "        at_path = sh_path+f'attack_model_aneur{attack_model.h_neurons}_ado{attack_model.do}_alr{attack_train_args.learning_rate}_alreg{attack_train_args.weight_decay}_aepoch{attack_train_args.epoch}'\n",
    "        torch.save(attack_model, at_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "df4d74e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 1e-05\n",
      "Train:  0.8081 0.8207236842105263 0.78842\n",
      "Test:  0.6895398337725522 0.6209717711716213 0.9943\n",
      "200 1e-06\n",
      "Train:  0.80328 0.8273606493674712 0.7665\n",
      "Test:  0.7008919521589296 0.6304929308368361 0.99\n",
      "500 1e-05\n",
      "Train:  0.8081 0.8207236842105263 0.78842\n",
      "Test:  0.6895398337725522 0.6209717711716213 0.9943\n",
      "500 1e-06\n",
      "Train:  0.80328 0.8273606493674712 0.7665\n",
      "Test:  0.7008919521589296 0.6304929308368361 0.99\n"
     ]
    }
   ],
   "source": [
    "epo = [200,500]\n",
    "wd = [1e-5, 1e-6]\n",
    "\n",
    "for ep in epo:\n",
    "    for w_d in wd:\n",
    "        print(ep, w_d)\n",
    "        attack_train_args = Train_args(learning_rate=l_r, weight_decay=w_d, epoch=500)\n",
    "        attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "        at_path = sh_path+f'attack_model_aneur{attack_model.h_neurons}_ado{attack_model.do}_alr{attack_train_args.learning_rate}_alreg{attack_train_args.weight_decay}_aepoch{attack_train_args.epoch}'\n",
    "        attack_model = torch.load(at_path)\n",
    "        \n",
    "        train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)\n",
    "        print('Train: ', train_acc, train_pre, train_rec)\n",
    "        test_acc, test_pre, test_rec = attack_evaluation(attack_model, attack_test_data, attack_test_target)\n",
    "        print('Test: ' , test_acc, test_pre, test_rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l_r in lr:\n",
    "    for w_d in wd:\n",
    "        ams[(l_r,w_d)]\n",
    "        train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)\n",
    "        print(\"Train acc pre rec: \", train_acc, train_pre, train_rec)\n",
    "        test_acc, test_pre, test_rec = attack_evaluation(attack_model, attack_test_data, attack_test_target)\n",
    "        print(\"Test acc pre rec: \", test_acc, test_pre, test_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf9f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_size = 20000\n",
    "shadow_clf = LogisticRegression(random_state=1).fit(x_shadow[:shadow_size], y_shadow[:shadow_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eca16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shadow_clf.score(x_shadow[:shadow_size], y_shadow[:shadow_size]))\n",
    "print(shadow_clf.score(x_shadow[shadow_size:shadow_size*2], y_shadow[shadow_size:shadow_size*2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee911ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_train_pred = shadow_clf.predict_proba(x_shadow[:shadow_size])\n",
    "shadow_test_pred = shadow_clf.predict_proba(x_shadow[shadow_size:shadow_size*2])\n",
    "y_shadow_train = y_shadow[:shadow_size]\n",
    "y_shadow_test = y_shadow[shadow_size:shadow_size*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ab2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# members\n",
    "labels = np.ones(shadow_train_pred.shape[0])\n",
    "# non-members\n",
    "test_labels = np.zeros(shadow_test_pred.shape[0])\n",
    "\n",
    "x_1 = np.concatenate((shadow_train_pred, shadow_test_pred))\n",
    "x_2 = np.concatenate((y_shadow_train, y_shadow_test)).reshape((-1, 1))\n",
    "y_new = np.concatenate((labels, test_labels))\n",
    "\n",
    "attack_train_data = np.concatenate((x_1,x_2),axis=1)\n",
    "attack_train_target = y_new\n",
    "df = pd.DataFrame(attack_train_data)\n",
    "df['a_target'] = attack_train_target\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "attack_train_data = np.array(df.drop(['a_target'], axis=1))\n",
    "attack_train_target = np.array(df['a_target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b1987",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)   \n",
    "attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)\n",
    "attack_train_args = Train_args(learning_rate=0.001, weight_decay=1e-3, epoch=200)\n",
    "attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86acb25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49914cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)   \n",
    "attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)\n",
    "attack_train_args = Train_args(learning_rate=0.001, weight_decay=1e-3, epoch=500)\n",
    "attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "for epoch in range(attack_train_args.epoch):\n",
    "            \n",
    "    attack_model = train_model(attack_model, attack_train_data, attack_train_target, attack_train_args)\n",
    "    train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, train_pre, train_rec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2234ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(x_target_train, y_target_train)\n",
    "test_pred = model.predict(x_target_test, y_target_test)\n",
    "\n",
    "y_target_train_ohe = OneHotEncoder(sparse=False).fit_transform(y_target_train.reshape(-1,1)) #encoode the target values\n",
    "y_target_test_ohe = OneHotEncoder(sparse=False).fit_transform(y_target_test.reshape(-1,1)) #encoode the target values\n",
    "    \n",
    "# members\n",
    "labels = np.ones(train_pred.shape[0])\n",
    "# non-members\n",
    "test_labels = np.zeros(test_pred.shape[0])\n",
    "\n",
    "x_1 = np.concatenate((train_pred, test_pred))\n",
    "x_2 = np.concatenate((y_target_train_ohe, y_target_test_ohe))#.reshape((-1, 1))\n",
    "y_new = np.concatenate((labels, test_labels))\n",
    "\n",
    "attack_test_data = np.concatenate((x_1,x_2),axis=1)\n",
    "attack_test_target = y_new\n",
    "df = pd.DataFrame(attack_test_data)\n",
    "df['a_target'] = attack_test_target\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "attack_test_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)\n",
    "attack_test_target = torch.tensor(np.array(df['a_target']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_pre, test_rec = attack_evaluation(a_model, attack_test_data, attack_test_target)\n",
    "test_acc, test_pre, test_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2bcabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_model = Net_attack(h_neurons=64, do=0, input_size=100)\n",
    "a_model = torch.load('attack_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41bca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "# random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "\n",
    "lr = [0.001]\n",
    "wd = [1e-4, 1e-6]\n",
    "tms = {}\n",
    "for l_r in lr:\n",
    "    for w_d in wd:\n",
    "        model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "        model.n_classes      = 100\n",
    "        model.alpha          = 0.001\n",
    "        model.max_iter       = 100*X_train_size \n",
    "        model.lambda_        = 1e-3\n",
    "        model.tolerance      = 10e-5\n",
    "        model.DP             = True\n",
    "        model.epsilon\n",
    "\n",
    "        X,y = model.init_theta(x_target_train, y_target_train)\n",
    "        model.SGD(X,y)\n",
    "        model.evaluate(x_target_train, y_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import algo\n",
    "# import attack\n",
    "\n",
    "# from torch import nn,optim\n",
    "import torch\n",
    "\n",
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "        \n",
    "\n",
    "raw_data_path = '../datasets/dataset_purchase'\n",
    "raw_data = pd.read_csv(raw_data_path)\n",
    "y=raw_data['63']\n",
    "X=raw_data.drop('63', axis=1)\n",
    "y =  y.replace(100, 0)\n",
    "print('Dataset: ', raw_data_path)\n",
    "print('Classes in classification task: ', y.nunique())\n",
    "n_classes = y.nunique()\n",
    "\n",
    "X_train, x_shadow, y_train, y_shadow = train_test_split(X, y, train_size=0.2, random_state=rand_seed)\n",
    "print(X_train.shape, x_shadow.shape)\n",
    "\n",
    "#Target model\n",
    "X_train_size = 10000\n",
    "X_test_size = 10000\n",
    "x_target_train = np.array(X_train[:X_train_size])\n",
    "y_target_train = np.array(y_train[:X_train_size])\n",
    "x_target_test = np.array(X_train[X_train_size:X_train_size+X_test_size])\n",
    "y_target_test = np.array(y_train[X_train_size:X_train_size+X_test_size])\n",
    "if y_target_test.shape[0]<X_test_size or y_target_train.shape[0]<X_train_size:\n",
    "    raise ValueError(\n",
    "            \"Not enough traning or test data for the target model\")\n",
    "\n",
    "for L in [1,10,100]:\n",
    "    for epsilon in np.arange(0,1,0.1):\n",
    "        model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "        model.n_classes      = n_classes\n",
    "        model.alpha          = 0.001\n",
    "        model.max_iter       = 100*X_train_size\n",
    "        model.lambda_        = 1e-3\n",
    "        model.tolerance      = 10e-5\n",
    "        model.DP             = True\n",
    "        model.L              = L\n",
    "        model.epsilon        = epsilon\n",
    "\n",
    "\n",
    "        X,y = model.init_theta(x_target_train, y_target_train)\n",
    "        model.train(X,y)\n",
    "        model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "        model.evaluate(x_target_test, y_target_test, acc=True)\n",
    "\n",
    "        tm_path = f'tm/lr{model.alpha}_iter{int(model.max_iter/X_train_size)}_reg{model.lambda_}_DP{model.DP}'\n",
    "        if model.DP:\n",
    "            tm_path += f'_eps{model.epsilon}_L{model.L}'\n",
    "        np.save(tm_path+'_target_model', model.theta)\n",
    "\n",
    "        print(tm_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee18d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shadow models\n",
    "# s_ms = {}\n",
    "# number_of_sms = 10\n",
    "# shadow_size = 50000\n",
    "# shadow_batch_size = int(shadow_size/number_of_sms)\n",
    "\n",
    "# x_shadow_train = np.array(x_shadow[:shadow_size])\n",
    "# y_shadow_train = np.array(y_shadow[:shadow_size])\n",
    "# x_shadow_test = np.array(x_shadow[shadow_size:2*shadow_size])\n",
    "# y_shadow_test = np.array(y_shadow[shadow_size:2*shadow_size])\n",
    "\n",
    "# attack.train_shadow_models(number_of_sms,)\n",
    "\n",
    "# for i in range(number_of_sms):  \n",
    "#     batch_start = i*shadow_batch_size\n",
    "#     batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "#     shadow_model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "#     shadow_model.n_classes      = n_classes\n",
    "#     shadow_model.alpha          = 0.001\n",
    "#     shadow_model.max_iter       = 100*shadow_batch_size\n",
    "#     shadow_model.lambda_        = 10e-3\n",
    "#     shadow_model.tolerance      = 10e-5\n",
    "#     shadow_model.DP             = False\n",
    "\n",
    "#     X,y = shadow_model.init_theta(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end] )\n",
    "#     shadow_model.SGD(X,y)\n",
    "#     print('Shadow model: ', i)\n",
    "#     shadow_model.evaluate(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end])\n",
    "#     shadow_model.evaluate(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end])\n",
    "#     s_ms[i] = shadow_model\n",
    "\n",
    "# #Attack model\n",
    "\n",
    "# shadow_train_pred = []\n",
    "# shadow_test_pred = []\n",
    "\n",
    "# for i in range(number_of_sms): \n",
    "#     batch_start = i*shadow_batch_size\n",
    "#     batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "#     train_prediciton = s_ms[i].predict(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end])\n",
    "#     test_prediciton = s_ms[i].predict(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end])\n",
    "    \n",
    "#     shadow_train_pred.append(train_prediciton)\n",
    "#     shadow_test_pred.append(test_prediciton)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f530940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/10000 == 1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7622903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0,1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca6fe0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
