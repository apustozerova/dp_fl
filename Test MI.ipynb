{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf2f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from torch import nn,optim\n",
    "import torch\n",
    "\n",
    "import algo\n",
    "\n",
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import attack\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa0caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_attack(nn.Module):\n",
    "\n",
    "    def __init__(self, h_neurons, do, input_size):\n",
    "        super(Net_attack, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.h_neurons = h_neurons\n",
    "        self.do = do\n",
    "        self.fc1 = nn.Linear(input_size, h_neurons)\n",
    "        self.fc2 = nn.Linear(h_neurons, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(do)\n",
    "        self.softmax = nn.Softmax(dim=1)   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class Train_args():\n",
    "\n",
    "    def __init__(self, learning_rate, weight_decay, epoch):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epoch = epoch\n",
    "         \n",
    "def train_attack_model(model, train_data, train_target, train_args):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=train_args.learning_rate, weight_decay=train_args.weight_decay)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_data)\n",
    "    loss = nn.CrossEntropyLoss()(output, train_target.to(torch.long))\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "def attack_evaluation(model, x, y, dev=\"cpu\", extended=False):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output =  model(x)\n",
    "        out_target = output.argmax(1, keepdim=True)\n",
    "        correct = out_target.to(dev).eq(y.to(dev).view_as(out_target.to(dev))).sum().item()\n",
    "        acc = correct/y.shape[0]\n",
    "\n",
    "        predicted_positive = output.argmax(1, keepdim=True) == 1\n",
    "        labeled_positive = y == 1\n",
    "        tp = predicted_positive.to(dev) * labeled_positive.to(dev).view_as(out_target)\n",
    "        tp_count = tp.to(dev).sum().item()\n",
    "        \n",
    "        if predicted_positive.to(dev).sum().item() != 0:\n",
    "            pre = tp_count / predicted_positive.to(dev).sum().item()\n",
    "        else:\n",
    "            pre = 0\n",
    "        if labeled_positive.to(dev).sum().item() !=0:\n",
    "            rec = tp_count / labeled_positive.to(dev).sum().item()\n",
    "        else:\n",
    "            rec = 0\n",
    "    if extended:\n",
    "        predicted_negative = output.argmax(1, keepdim=True) == 0\n",
    "        labeled_negative = y == 0\n",
    "        tn = predicted_negative.to(dev) * labeled_negative.to(dev).view_as(out_target)\n",
    "        tn_count = tn.to(dev).sum().item()\n",
    "\n",
    "        fp_count = predicted_positive.to(dev).sum().item() - tp.to(dev).sum().item()\n",
    "        fn_count = labeled_positive.to(dev).sum().item() - tp.to(dev).sum().item()\n",
    "        \n",
    "        return acc, pre, rec, tp_count, tn_count, fp_count, fn_count\n",
    "    else:\n",
    "        return acc, pre, rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "684c6361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "(19732, 600) (177591, 600)\n"
     ]
    }
   ],
   "source": [
    "# raw_data = pd.read_csv('../datasets/dataset_purchase', )\n",
    "# y=raw_data['63']\n",
    "# X=raw_data.drop('63', axis=1)\n",
    "# y =  y.replace(100, 0)\n",
    "# print(y.nunique())\n",
    "\n",
    "# X_train, x_shadow, y_train, y_shadow = train_test_split(X, y, train_size=0.1, random_state=42)\n",
    "# print(X_train.shape, x_shadow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa3395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../DP-UTIL.nosync/loan_preprocessed.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "523b87ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661283, 167)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7b1c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loan\n",
    "y = data['grade']\n",
    "X = data.drop('grade', axis=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[0:100000], y[0:100000], train_size=0.5, random_state=0)\n",
    "x_target_train = np.array(X_train)\n",
    "y_target_train = np.array(y_train)\n",
    "x_target_test = np.array(X_test)\n",
    "y_target_test = np.array(y_test)\n",
    "\n",
    "x_shadow = X[100000:]\n",
    "y_shadow = y[100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "77aad817",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "model.n_classes      = len(np.unique(y_target_test))\n",
    "model.alpha          = 0.001\n",
    "model.max_iter       = 100\n",
    "model.lambda_        = 1e-5\n",
    "model.tolerance      = 1e-5\n",
    "model.DP             = False\n",
    "model.L              = 1\n",
    "model.epsilon        = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cc6fa43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model : 88.6 %\n",
      "The accuracy of the model : 88.1 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.88086"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "random.seed(rand_seed)\n",
    "\n",
    "X,y = model.init_theta(x_target_train, y_target_train)\n",
    "model.train(X,y)\n",
    "model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "model.evaluate(x_target_test, y_target_test, acc=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3727bc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model : 86.6 %\n",
      "The accuracy of the model : 86.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85972"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "model.evaluate(x_target_test, y_target_test, acc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7b14812a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/0p0lnj8d6zj532q0yvm1zt580000gn/T/ipykernel_30021/3790754660.py:7: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_shadow_train = np.array(y_shadow[:shadow_size])\n",
      "/var/folders/66/0p0lnj8d6zj532q0yvm1zt580000gn/T/ipykernel_30021/3790754660.py:9: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_shadow_test = np.array(y_shadow[shadow_size:2*shadow_size])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shadow model:  0\n",
      "The accuracy of the model : 91.60000000000001 %\n",
      "The accuracy of the model : 78.60000000000001 %\n",
      "Shadow model:  1\n",
      "The accuracy of the model : 91.8 %\n",
      "The accuracy of the model : 79.2 %\n",
      "Shadow model:  2\n",
      "The accuracy of the model : 92.7 %\n",
      "The accuracy of the model : 79.10000000000001 %\n",
      "Shadow model:  3\n",
      "The accuracy of the model : 92.4 %\n",
      "The accuracy of the model : 80.10000000000001 %\n",
      "Shadow model:  4\n",
      "The accuracy of the model : 91.10000000000001 %\n",
      "The accuracy of the model : 79.4 %\n",
      "Shadow model:  5\n",
      "The accuracy of the model : 91.8 %\n",
      "The accuracy of the model : 79.2 %\n",
      "Shadow model:  6\n",
      "The accuracy of the model : 92.2 %\n",
      "The accuracy of the model : 78.7 %\n",
      "Shadow model:  7\n",
      "The accuracy of the model : 91.60000000000001 %\n",
      "The accuracy of the model : 79.3 %\n",
      "Shadow model:  8\n",
      "The accuracy of the model : 92.30000000000001 %\n",
      "The accuracy of the model : 78.4 %\n",
      "Shadow model:  9\n",
      "The accuracy of the model : 92.2 %\n",
      "The accuracy of the model : 81.2 %\n",
      "Shadow model:  10\n",
      "The accuracy of the model : 91.3 %\n",
      "The accuracy of the model : 77.7 %\n",
      "Shadow model:  11\n",
      "The accuracy of the model : 91.7 %\n",
      "The accuracy of the model : 79.0 %\n",
      "Shadow model:  12\n",
      "The accuracy of the model : 92.30000000000001 %\n",
      "The accuracy of the model : 78.60000000000001 %\n",
      "Shadow model:  13\n",
      "The accuracy of the model : 92.30000000000001 %\n",
      "The accuracy of the model : 77.8 %\n",
      "Shadow model:  14\n",
      "The accuracy of the model : 91.4 %\n",
      "The accuracy of the model : 80.5 %\n",
      "Shadow model:  15\n",
      "The accuracy of the model : 92.30000000000001 %\n",
      "The accuracy of the model : 79.7 %\n",
      "Shadow model:  16\n",
      "The accuracy of the model : 90.5 %\n",
      "The accuracy of the model : 79.80000000000001 %\n",
      "Shadow model:  17\n",
      "The accuracy of the model : 92.4 %\n",
      "The accuracy of the model : 78.9 %\n",
      "Shadow model:  18\n",
      "The accuracy of the model : 91.4 %\n",
      "The accuracy of the model : 76.9 %\n",
      "Shadow model:  19\n",
      "The accuracy of the model : 91.60000000000001 %\n",
      "The accuracy of the model : 78.9 %\n"
     ]
    }
   ],
   "source": [
    "s_ms = {}\n",
    "number_of_sms = 20\n",
    "shadow_size = 50000\n",
    "shadow_batch_size = int(shadow_size/number_of_sms)\n",
    "\n",
    "x_shadow_train = np.array(x_shadow[:shadow_size])\n",
    "y_shadow_train = np.array(y_shadow[:shadow_size])\n",
    "x_shadow_test = np.array(x_shadow[shadow_size:2*shadow_size])\n",
    "y_shadow_test = np.array(y_shadow[shadow_size:2*shadow_size])\n",
    "\n",
    "for i in range(number_of_sms):\n",
    "    batch_start = i*shadow_batch_size\n",
    "    batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "    shadow_model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "    shadow_model.n_classes      = len(np.unique(y_target_test))\n",
    "    shadow_model.alpha          = 0.001\n",
    "    shadow_model.max_iter       = 500\n",
    "    shadow_model.lambda_        = 10e-5\n",
    "    shadow_model.tolerance      = 10e-5\n",
    "    shadow_model.DP             = False\n",
    "\n",
    "    X,y = shadow_model.init_theta(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end] )\n",
    "    shadow_model.SGD(X,y)\n",
    "    print('Shadow model: ', i)\n",
    "    shadow_model.evaluate(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end], acc=True)\n",
    "    shadow_model.evaluate(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end], acc=True)\n",
    "    s_ms[i] = shadow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "600e22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_train_pred = []\n",
    "shadow_test_pred = []\n",
    "\n",
    "for i in range(number_of_sms): \n",
    "    batch_start = i*shadow_batch_size\n",
    "    batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "    train_prediciton = s_ms[i].predict(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end])\n",
    "    test_prediciton = s_ms[i].predict(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end])\n",
    "    \n",
    "    shadow_train_pred.append(train_prediciton)\n",
    "    shadow_test_pred.append(test_prediciton)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "49a69604",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_shadow_train_ohe = OneHotEncoder(sparse=False).fit_transform(y_shadow_train.reshape(-1,1)) #encoode the target values\n",
    "y_shadow_test_ohe = OneHotEncoder(sparse=False).fit_transform(y_shadow_test.reshape(-1,1)) #encoode the target values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d261d0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 7)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shadow_train_pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "41420b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_train_pred = np.concatenate(shadow_train_pred)\n",
    "sh_test_pred = np.concatenate(shadow_test_pred)\n",
    "\n",
    "# members\n",
    "labels = np.ones(sh_train_pred.shape[0])\n",
    "# non-members\n",
    "test_labels = np.zeros(sh_test_pred.shape[0])\n",
    "\n",
    "x_1 = np.concatenate((sh_train_pred, sh_test_pred))\n",
    "x_2 = np.concatenate((y_shadow_train_ohe, y_shadow_test_ohe))#.reshape((-1, 1))\n",
    "y_new = np.concatenate((labels, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e11035f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_train_data = np.concatenate((x_1,x_2),axis=1)\n",
    "attack_train_target = y_new\n",
    "df = pd.DataFrame(attack_train_data)\n",
    "df['a_target'] = attack_train_target\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)\n",
    "attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "399bf1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: (0.49126, 0.48604368932038833, 0.30438)\n",
      "epoch 1: (0.49315, 0.49249676868140296, 0.44962)\n",
      "epoch 2: (0.49551, 0.49579154559940014, 0.52896)\n",
      "epoch 3: (0.49855, 0.4987650535711244, 0.58562)\n",
      "epoch 4: (0.50248, 0.5019040014740657, 0.65374)\n",
      "epoch 5: (0.50778, 0.5055169479506453, 0.71288)\n",
      "epoch 6: (0.51181, 0.5080920340399874, 0.74154)\n",
      "epoch 7: (0.50895, 0.5079857950996662, 0.56932)\n",
      "epoch 8: (0.51039, 0.5092709913446953, 0.57074)\n",
      "epoch 9: (0.51307, 0.511784754657097, 0.5676)\n",
      "epoch 10: (0.51587, 0.514453288646837, 0.56488)\n",
      "epoch 11: (0.51749, 0.5161355794601177, 0.55946)\n",
      "epoch 12: (0.52027, 0.5186679191762907, 0.56318)\n",
      "epoch 13: (0.5219, 0.5205394657863145, 0.55502)\n",
      "epoch 14: (0.52263, 0.5216526015653405, 0.5452)\n",
      "epoch 15: (0.52283, 0.5223363663046668, 0.53388)\n",
      "epoch 16: (0.52494, 0.5210563641889838, 0.61716)\n",
      "epoch 17: (0.52525, 0.5213819967821154, 0.6157)\n",
      "epoch 18: (0.52589, 0.5217947638690126, 0.61984)\n",
      "epoch 19: (0.53099, 0.5257473288911783, 0.6328)\n",
      "epoch 20: (0.5314, 0.5265131045663334, 0.62356)\n",
      "epoch 21: (0.53279, 0.5277200101445599, 0.62424)\n",
      "epoch 22: (0.53609, 0.529627136453938, 0.64516)\n",
      "epoch 23: (0.54298, 0.533714034702394, 0.6804)\n",
      "epoch 24: (0.5457, 0.5351657484071531, 0.69548)\n",
      "epoch 25: (0.54756, 0.5359692642787997, 0.70868)\n",
      "epoch 26: (0.54798, 0.5363209689629069, 0.70848)\n",
      "epoch 27: (0.5482, 0.5361698934413928, 0.7145)\n",
      "epoch 28: (0.54829, 0.5363876120865044, 0.71184)\n",
      "epoch 29: (0.54834, 0.5363677399939814, 0.71294)\n",
      "epoch 30: (0.54919, 0.5366438713330055, 0.72038)\n",
      "epoch 31: (0.54875, 0.5369413333737478, 0.70858)\n",
      "epoch 32: (0.54921, 0.5370495851590851, 0.71332)\n",
      "epoch 33: (0.54954, 0.5372369212266987, 0.71474)\n",
      "epoch 34: (0.55005, 0.5374586495427126, 0.71812)\n",
      "epoch 35: (0.54973, 0.5376223691576766, 0.71064)\n",
      "epoch 36: (0.5506, 0.537225590018245, 0.73024)\n",
      "epoch 37: (0.54959, 0.5376371833209369, 0.70838)\n",
      "epoch 38: (0.55074, 0.5370073226945182, 0.73628)\n",
      "epoch 39: (0.55044, 0.5379784959190435, 0.7145)\n",
      "epoch 40: (0.55128, 0.537585388020757, 0.73346)\n",
      "epoch 41: (0.55113, 0.5381709593131766, 0.72088)\n",
      "epoch 42: (0.55137, 0.5377992965519272, 0.73088)\n",
      "epoch 43: (0.55177, 0.5387297074885913, 0.72012)\n",
      "epoch 44: (0.55215, 0.5380729189481216, 0.73702)\n",
      "epoch 45: (0.552, 0.5389922015596881, 0.7188)\n",
      "epoch 46: (0.5525, 0.5382474647394802, 0.73882)\n",
      "epoch 47: (0.55299, 0.5395701719013696, 0.72256)\n",
      "epoch 48: (0.55321, 0.5388853973311507, 0.7374)\n",
      "epoch 49: (0.55364, 0.5397074499585456, 0.72908)\n",
      "epoch 50: (0.55416, 0.539857525536487, 0.73358)\n",
      "epoch 51: (0.5546, 0.5401364344732277, 0.73478)\n",
      "epoch 52: (0.55547, 0.5405880028683067, 0.7388)\n",
      "epoch 53: (0.55521, 0.5410782577640214, 0.72722)\n",
      "epoch 54: (0.55616, 0.5409496587528437, 0.74188)\n",
      "epoch 55: (0.55623, 0.5417316055869736, 0.72994)\n",
      "epoch 56: (0.55705, 0.5416368652293859, 0.74214)\n",
      "epoch 57: (0.55734, 0.542376138110441, 0.7339)\n",
      "epoch 58: (0.55885, 0.5429894663023945, 0.74332)\n",
      "epoch 59: (0.55881, 0.5435236304968843, 0.73442)\n",
      "epoch 60: (0.56045, 0.5441092771770063, 0.74568)\n",
      "epoch 61: (0.5606, 0.5444777171041043, 0.74184)\n",
      "epoch 62: (0.56151, 0.5447755761643397, 0.74838)\n",
      "epoch 63: (0.56128, 0.5453227619667476, 0.73732)\n",
      "epoch 64: (0.56275, 0.5455641237891924, 0.75134)\n",
      "epoch 65: (0.56231, 0.5460321212748038, 0.73912)\n",
      "epoch 66: (0.56333, 0.5461151969708002, 0.74998)\n",
      "epoch 67: (0.56316, 0.546435713445477, 0.74324)\n",
      "epoch 68: (0.5638, 0.5465829439252337, 0.7486)\n",
      "epoch 69: (0.56387, 0.5469197655113645, 0.7445)\n",
      "epoch 70: (0.5645, 0.5471160589060309, 0.74898)\n",
      "epoch 71: (0.56398, 0.5471203417292679, 0.74288)\n",
      "epoch 72: (0.56457, 0.5471858055275427, 0.74878)\n",
      "epoch 73: (0.56423, 0.5473623667173005, 0.7423)\n",
      "epoch 74: (0.56436, 0.5471308474179092, 0.74714)\n",
      "epoch 75: (0.56439, 0.5476504107156072, 0.74004)\n",
      "epoch 76: (0.56455, 0.5473164152409435, 0.74666)\n",
      "epoch 77: (0.56476, 0.5479433800230981, 0.74014)\n",
      "epoch 78: (0.56474, 0.5475595780317946, 0.74536)\n",
      "epoch 79: (0.56505, 0.5483133049122859, 0.73826)\n",
      "epoch 80: (0.56496, 0.5479140850888062, 0.74284)\n",
      "epoch 81: (0.56507, 0.5483540164969904, 0.73792)\n",
      "epoch 82: (0.56526, 0.5481112323434874, 0.74348)\n",
      "epoch 83: (0.56513, 0.5485038502211829, 0.73652)\n",
      "epoch 84: (0.56568, 0.5484608800873595, 0.74334)\n",
      "epoch 85: (0.56533, 0.5487122895447157, 0.7359)\n",
      "epoch 86: (0.56573, 0.5485730332096777, 0.74234)\n",
      "epoch 87: (0.56514, 0.5487108159846853, 0.73378)\n",
      "epoch 88: (0.56608, 0.5488685105753587, 0.74218)\n",
      "epoch 89: (0.56538, 0.5490340193196136, 0.73206)\n",
      "epoch 90: (0.56627, 0.5489503774504735, 0.74318)\n",
      "epoch 91: (0.56535, 0.5491168733558812, 0.7306)\n",
      "epoch 92: (0.56659, 0.5491301332467647, 0.74428)\n",
      "epoch 93: (0.56567, 0.5493114271554508, 0.73154)\n",
      "epoch 94: (0.56638, 0.5491645434615157, 0.74146)\n",
      "epoch 95: (0.5654, 0.5492440214444912, 0.72944)\n",
      "epoch 96: (0.56626, 0.5492522225195493, 0.73892)\n",
      "epoch 97: (0.56568, 0.549260492604926, 0.73234)\n",
      "epoch 98: (0.56647, 0.5494325703151726, 0.7388)\n",
      "epoch 99: (0.5656, 0.5493648786948407, 0.73004)\n",
      "epoch 100: (0.56615, 0.5492341356673961, 0.73794)\n",
      "epoch 101: (0.56565, 0.5494508805495714, 0.72944)\n",
      "epoch 102: (0.5662, 0.5492911603526328, 0.73772)\n",
      "epoch 103: (0.56573, 0.5494530297786538, 0.7303)\n",
      "epoch 104: (0.56622, 0.5493825319174322, 0.7367)\n",
      "epoch 105: (0.56553, 0.5494439162780871, 0.7282)\n",
      "epoch 106: (0.56641, 0.5493307186046857, 0.73952)\n",
      "epoch 107: (0.56539, 0.5494218124102487, 0.72694)\n",
      "epoch 108: (0.56661, 0.5492160600552674, 0.74332)\n",
      "epoch 109: (0.56559, 0.5494906813551649, 0.72824)\n",
      "epoch 110: (0.5664, 0.5492596219472388, 0.74038)\n",
      "epoch 111: (0.56563, 0.5494954675032806, 0.72862)\n",
      "epoch 112: (0.56627, 0.549267712437737, 0.73882)\n",
      "epoch 113: (0.56553, 0.5495111594662798, 0.7273)\n",
      "epoch 114: (0.56617, 0.5491290854283296, 0.7396)\n",
      "epoch 115: (0.56568, 0.5496792932349026, 0.72672)\n",
      "epoch 116: (0.56621, 0.5489711690655463, 0.74222)\n",
      "epoch 117: (0.56597, 0.5497774088885535, 0.72862)\n",
      "epoch 118: (0.56612, 0.5492535979261643, 0.73734)\n",
      "epoch 119: (0.56595, 0.5497578126178871, 0.72866)\n",
      "epoch 120: (0.56609, 0.5490930160003565, 0.7392)\n",
      "epoch 121: (0.56576, 0.5497382990953923, 0.72682)\n",
      "epoch 122: (0.56684, 0.5491513957113863, 0.74678)\n",
      "epoch 123: (0.56622, 0.5500953187883922, 0.72716)\n",
      "epoch 124: (0.56675, 0.5491988148060792, 0.74512)\n",
      "epoch 125: (0.56622, 0.5501195846326179, 0.72684)\n",
      "epoch 126: (0.56657, 0.5491385800965499, 0.74394)\n",
      "epoch 127: (0.56633, 0.5502461934701917, 0.72638)\n",
      "epoch 128: (0.56668, 0.5492481314071317, 0.74366)\n",
      "epoch 129: (0.56638, 0.5503504353894232, 0.72556)\n",
      "epoch 130: (0.56719, 0.5494822735775411, 0.74612)\n",
      "epoch 131: (0.56637, 0.5502566976117278, 0.72668)\n",
      "epoch 132: (0.56716, 0.5493997881605273, 0.74692)\n",
      "epoch 133: (0.56623, 0.5500869696740528, 0.72738)\n",
      "epoch 134: (0.56701, 0.5494531446030317, 0.74452)\n",
      "epoch 135: (0.56635, 0.5502567753859205, 0.72646)\n",
      "epoch 136: (0.56727, 0.549346400434266, 0.74888)\n",
      "epoch 137: (0.56638, 0.5500467444735969, 0.72956)\n",
      "epoch 138: (0.56745, 0.5496240490869764, 0.74706)\n",
      "epoch 139: (0.56637, 0.5500671383956186, 0.72918)\n",
      "epoch 140: (0.56726, 0.5494704324801413, 0.74706)\n",
      "epoch 141: (0.56613, 0.5499132009963016, 0.72858)\n",
      "epoch 142: (0.56769, 0.5497464540310134, 0.74804)\n",
      "epoch 143: (0.56654, 0.5501280699111044, 0.73024)\n",
      "epoch 144: (0.56778, 0.5497227031309604, 0.74936)\n",
      "epoch 145: (0.5668, 0.5502815162737482, 0.73106)\n",
      "epoch 146: (0.5684, 0.5501539815222173, 0.7503)\n",
      "epoch 147: (0.56727, 0.5505903587275325, 0.73212)\n",
      "epoch 148: (0.56886, 0.5502539701074264, 0.75398)\n",
      "epoch 149: (0.56764, 0.5506879290189143, 0.73486)\n",
      "epoch 150: (0.56917, 0.5506094794913444, 0.75254)\n",
      "epoch 151: (0.56792, 0.5509726224783862, 0.73416)\n",
      "epoch 152: (0.56924, 0.5503680856635726, 0.75658)\n",
      "epoch 153: (0.56814, 0.5509237115867511, 0.73718)\n",
      "epoch 154: (0.56908, 0.550495599543873, 0.7531)\n",
      "epoch 155: (0.56812, 0.5509758141762452, 0.73628)\n",
      "epoch 156: (0.56924, 0.5503548987665814, 0.75676)\n",
      "epoch 157: (0.56833, 0.5510130948292595, 0.73806)\n",
      "epoch 158: (0.56931, 0.5504461621322619, 0.75628)\n",
      "epoch 159: (0.5683, 0.5510158350761877, 0.7377)\n",
      "epoch 160: (0.56934, 0.5503324526000987, 0.75816)\n",
      "epoch 161: (0.56841, 0.551005800688925, 0.73902)\n",
      "epoch 162: (0.56942, 0.5503130979300748, 0.7593)\n",
      "epoch 163: (0.56849, 0.5509651302963107, 0.74042)\n",
      "epoch 164: (0.56946, 0.5503362513768915, 0.75942)\n",
      "epoch 165: (0.56858, 0.5509645967717963, 0.7414)\n",
      "epoch 166: (0.56952, 0.550292262283697, 0.76068)\n",
      "epoch 167: (0.56854, 0.5509682025045362, 0.74092)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 168: (0.56995, 0.5504842737337433, 0.76274)\n",
      "epoch 169: (0.56873, 0.5509979965867775, 0.74258)\n",
      "epoch 170: (0.56977, 0.5503282117867705, 0.76292)\n",
      "epoch 171: (0.56871, 0.5509544220814856, 0.74294)\n",
      "epoch 172: (0.5698, 0.5502114925330187, 0.76486)\n",
      "epoch 173: (0.56864, 0.5509183703747663, 0.74266)\n",
      "epoch 174: (0.56995, 0.5503070926168319, 0.76518)\n",
      "epoch 175: (0.56865, 0.5508737087044805, 0.74336)\n",
      "epoch 176: (0.56997, 0.5503041108890391, 0.76544)\n",
      "epoch 177: (0.56861, 0.550797387943672, 0.74394)\n",
      "epoch 178: (0.56995, 0.5502868398728991, 0.76546)\n",
      "epoch 179: (0.56867, 0.5507966801295993, 0.7446)\n",
      "epoch 180: (0.57009, 0.5503397158739963, 0.76626)\n",
      "epoch 181: (0.56884, 0.5509036055488183, 0.74502)\n",
      "epoch 182: (0.57001, 0.5502865926362213, 0.76612)\n",
      "epoch 183: (0.56925, 0.5510972064400926, 0.74688)\n",
      "epoch 184: (0.56998, 0.5502210357101849, 0.7667)\n",
      "epoch 185: (0.56915, 0.5510204081632653, 0.74682)\n",
      "epoch 186: (0.57003, 0.5502057554163142, 0.76746)\n",
      "epoch 187: (0.56912, 0.5509975209538425, 0.7468)\n",
      "epoch 188: (0.57012, 0.5502522646485495, 0.7678)\n",
      "epoch 189: (0.56906, 0.5508901727288805, 0.74758)\n",
      "epoch 190: (0.57011, 0.5502386173094286, 0.76788)\n",
      "epoch 191: (0.56907, 0.5508698021770831, 0.74796)\n",
      "epoch 192: (0.5701, 0.5501861397479955, 0.7685)\n",
      "epoch 193: (0.56899, 0.5507914304645513, 0.74814)\n",
      "epoch 194: (0.57011, 0.5501279832978221, 0.76942)\n",
      "epoch 195: (0.5692, 0.5508928308769453, 0.74906)\n",
      "epoch 196: (0.5701, 0.5500771516744771, 0.77002)\n",
      "epoch 197: (0.5692, 0.5508748713424496, 0.7493)\n",
      "epoch 198: (0.57036, 0.5502112354418817, 0.771)\n",
      "epoch 199: (0.5692, 0.5508046517091507, 0.75024)\n",
      "epoch 200: (0.5705, 0.5502767001369238, 0.77162)\n",
      "epoch 201: (0.56937, 0.5508808989423345, 0.75106)\n",
      "epoch 202: (0.5706, 0.550290631410987, 0.77252)\n",
      "epoch 203: (0.56955, 0.5509576073736501, 0.75198)\n",
      "epoch 204: (0.57057, 0.5502427772003019, 0.77286)\n",
      "epoch 205: (0.5697, 0.5510293730049491, 0.75264)\n",
      "epoch 206: (0.57064, 0.5502275312855518, 0.77384)\n",
      "epoch 207: (0.56972, 0.5510201094751632, 0.75298)\n",
      "epoch 208: (0.57073, 0.5502679345585831, 0.77426)\n",
      "epoch 209: (0.56963, 0.5509117763186757, 0.75346)\n",
      "epoch 210: (0.57082, 0.5502911518250249, 0.77492)\n",
      "epoch 211: (0.56978, 0.5509447186286248, 0.75464)\n",
      "epoch 212: (0.57081, 0.5502148722821848, 0.77588)\n",
      "epoch 213: (0.56992, 0.5510439480216089, 0.75482)\n",
      "epoch 214: (0.57082, 0.5502055862753438, 0.77612)\n",
      "epoch 215: (0.56984, 0.5509661976764552, 0.755)\n",
      "epoch 216: (0.57094, 0.5502251423069299, 0.77716)\n",
      "epoch 217: (0.56987, 0.5509382791653908, 0.7557)\n",
      "epoch 218: (0.57082, 0.5500721174241353, 0.778)\n",
      "epoch 219: (0.56992, 0.5509873698334451, 0.75558)\n",
      "epoch 220: (0.57089, 0.5501294072722644, 0.77796)\n",
      "epoch 221: (0.57004, 0.5509960391425909, 0.75676)\n",
      "epoch 222: (0.57101, 0.5501631839952529, 0.7788)\n",
      "epoch 223: (0.57001, 0.5509541623604419, 0.757)\n",
      "epoch 224: (0.57099, 0.5501193148924753, 0.7792)\n",
      "epoch 225: (0.57008, 0.5509732041546652, 0.7575)\n",
      "epoch 226: (0.57105, 0.5501545933277802, 0.77936)\n",
      "epoch 227: (0.57001, 0.5508948952441879, 0.7578)\n",
      "epoch 228: (0.57116, 0.5501961005614966, 0.77998)\n",
      "epoch 229: (0.57008, 0.5509065550906556, 0.7584)\n",
      "epoch 230: (0.57126, 0.5502099715340605, 0.78088)\n",
      "epoch 231: (0.57023, 0.5509762647891413, 0.75908)\n",
      "epoch 232: (0.57127, 0.5501851930091399, 0.78134)\n",
      "epoch 233: (0.57025, 0.5509582323840474, 0.75954)\n",
      "epoch 234: (0.57129, 0.5501752509114455, 0.7817)\n",
      "epoch 235: (0.57033, 0.551002944290542, 0.7598)\n",
      "epoch 236: (0.5714, 0.5502152080344332, 0.78234)\n",
      "epoch 237: (0.57037, 0.5509920146084839, 0.76038)\n",
      "epoch 238: (0.57146, 0.5502263206724957, 0.78284)\n",
      "epoch 239: (0.5703, 0.5508948221939071, 0.76094)\n",
      "epoch 240: (0.57153, 0.5502310360809539, 0.78354)\n",
      "epoch 241: (0.57038, 0.5509350393700787, 0.76126)\n",
      "epoch 242: (0.57159, 0.5502308415542864, 0.7842)\n",
      "epoch 243: (0.57031, 0.5508674451245098, 0.76142)\n",
      "epoch 244: (0.57163, 0.5502419863926492, 0.78448)\n",
      "epoch 245: (0.57036, 0.5508587289654774, 0.76208)\n",
      "epoch 246: (0.57179, 0.5502977650108597, 0.78544)\n",
      "epoch 247: (0.57036, 0.550849907493062, 0.7622)\n",
      "epoch 248: (0.57182, 0.5502997534737786, 0.78574)\n",
      "epoch 249: (0.57047, 0.5508786622962182, 0.763)\n",
      "epoch 250: (0.57168, 0.5501609517144856, 0.78618)\n",
      "epoch 251: (0.57041, 0.5508074641728363, 0.76332)\n",
      "epoch 252: (0.57187, 0.5502664745625201, 0.78676)\n",
      "epoch 253: (0.57054, 0.5509020060614808, 0.76344)\n",
      "epoch 254: (0.57193, 0.5502845238594578, 0.78716)\n",
      "epoch 255: (0.57054, 0.5508843812216868, 0.76368)\n",
      "epoch 256: (0.57186, 0.5502236511042773, 0.78726)\n",
      "epoch 257: (0.57051, 0.5507697181780217, 0.76492)\n",
      "epoch 258: (0.57189, 0.5501948024744802, 0.788)\n",
      "epoch 259: (0.57043, 0.5507252639615113, 0.76466)\n",
      "epoch 260: (0.57193, 0.5502199259931578, 0.78808)\n",
      "epoch 261: (0.57057, 0.5508041409298374, 0.7651)\n",
      "epoch 262: (0.57187, 0.5501668272116821, 0.78818)\n",
      "epoch 263: (0.57073, 0.5508607423813153, 0.76606)\n",
      "epoch 264: (0.57194, 0.550158969210174, 0.78906)\n",
      "epoch 265: (0.57078, 0.5508988925643608, 0.76608)\n",
      "epoch 266: (0.57199, 0.550198730911373, 0.78904)\n",
      "epoch 267: (0.57081, 0.5509124113832128, 0.76622)\n",
      "epoch 268: (0.57203, 0.5502014189933232, 0.78944)\n",
      "epoch 269: (0.57087, 0.5509306503772907, 0.76662)\n",
      "epoch 270: (0.5721, 0.5502341006632853, 0.78974)\n",
      "epoch 271: (0.57092, 0.5509263248599742, 0.76722)\n",
      "epoch 272: (0.57221, 0.5502652132147183, 0.7905)\n",
      "epoch 273: (0.57087, 0.550870694976815, 0.76744)\n",
      "epoch 274: (0.57206, 0.550133578226749, 0.79074)\n",
      "epoch 275: (0.57098, 0.5508992341450821, 0.76824)\n",
      "epoch 276: (0.57236, 0.5502821246907746, 0.7919)\n",
      "epoch 277: (0.57101, 0.550905415286679, 0.76848)\n",
      "epoch 278: (0.57226, 0.5502028679413072, 0.79194)\n",
      "epoch 279: (0.57103, 0.5508920255069141, 0.76888)\n",
      "epoch 280: (0.57242, 0.550294460803378, 0.79238)\n",
      "epoch 281: (0.57115, 0.5509502599429987, 0.76938)\n",
      "epoch 282: (0.5725, 0.5503276503581941, 0.79278)\n",
      "epoch 283: (0.57109, 0.5508883448581942, 0.76958)\n",
      "epoch 284: (0.57251, 0.5503185243785652, 0.79302)\n",
      "epoch 285: (0.57122, 0.5509442060085837, 0.77022)\n",
      "epoch 286: (0.57245, 0.5502420216085768, 0.79346)\n",
      "epoch 287: (0.57119, 0.5509176477319868, 0.77026)\n",
      "epoch 288: (0.5725, 0.5502885522446035, 0.79334)\n",
      "epoch 289: (0.5713, 0.5509431266076021, 0.7711)\n",
      "epoch 290: (0.57242, 0.5502107715347496, 0.79358)\n",
      "epoch 291: (0.57137, 0.5509647381425041, 0.77156)\n",
      "epoch 292: (0.5725, 0.5502328030596974, 0.79414)\n",
      "epoch 293: (0.57133, 0.5509347195841248, 0.77154)\n",
      "epoch 294: (0.57242, 0.5501676411094794, 0.7942)\n",
      "epoch 295: (0.57134, 0.5508917106577258, 0.77224)\n",
      "epoch 296: (0.57248, 0.5501855647260843, 0.7946)\n",
      "epoch 297: (0.57136, 0.5508798448506973, 0.77262)\n",
      "epoch 298: (0.57257, 0.5502221484034381, 0.79506)\n",
      "epoch 299: (0.57131, 0.5508231772503742, 0.77286)\n",
      "epoch 300: (0.57261, 0.5501998036531575, 0.79582)\n",
      "epoch 301: (0.57137, 0.5508412999187907, 0.77326)\n",
      "epoch 302: (0.5726, 0.5501977487070276, 0.79574)\n",
      "epoch 303: (0.57154, 0.5508949659941379, 0.77436)\n",
      "epoch 304: (0.57269, 0.5502023564512342, 0.79666)\n",
      "epoch 305: (0.57145, 0.5508244298701114, 0.77436)\n",
      "epoch 306: (0.57265, 0.5501581033125751, 0.79686)\n",
      "epoch 307: (0.57151, 0.5508295068450307, 0.77494)\n",
      "epoch 308: (0.57271, 0.5501773563551544, 0.79724)\n",
      "epoch 309: (0.57148, 0.5507742577070607, 0.77538)\n",
      "epoch 310: (0.57274, 0.5501793598233996, 0.79754)\n",
      "epoch 311: (0.57155, 0.5508030503131257, 0.77574)\n",
      "epoch 312: (0.57269, 0.550128960180958, 0.79772)\n",
      "epoch 313: (0.57169, 0.5508403659314942, 0.77674)\n",
      "epoch 314: (0.57285, 0.5501977591885672, 0.79848)\n",
      "epoch 315: (0.57172, 0.5508537069601225, 0.77688)\n",
      "epoch 316: (0.57286, 0.5501915075363037, 0.79868)\n",
      "epoch 317: (0.57182, 0.5509029569359, 0.77728)\n",
      "epoch 318: (0.57295, 0.5502320521118808, 0.79908)\n",
      "epoch 319: (0.57189, 0.5508840475078213, 0.7783)\n",
      "epoch 320: (0.57301, 0.5502332429717494, 0.79972)\n",
      "epoch 321: (0.57192, 0.5508843922456488, 0.77862)\n",
      "epoch 322: (0.57307, 0.5502648414390865, 0.79992)\n",
      "epoch 323: (0.57203, 0.5509514041168565, 0.77888)\n",
      "epoch 324: (0.57299, 0.5501959975242418, 0.80004)\n",
      "epoch 325: (0.5721, 0.5509569445622368, 0.77956)\n",
      "epoch 326: (0.57306, 0.55021167802947, 0.80058)\n",
      "epoch 327: (0.57213, 0.5509514996538717, 0.77996)\n",
      "epoch 328: (0.5731, 0.5502267417891988, 0.8008)\n",
      "epoch 329: (0.57214, 0.5509376941761284, 0.78026)\n",
      "epoch 330: (0.57314, 0.5502307565518378, 0.80118)\n",
      "epoch 331: (0.57215, 0.5509095271023553, 0.78076)\n",
      "epoch 332: (0.57322, 0.5502774115578993, 0.80138)\n",
      "epoch 333: (0.57222, 0.5509409474367294, 0.78108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 334: (0.57336, 0.5503583294434224, 0.80174)\n",
      "epoch 335: (0.57218, 0.5508955013397264, 0.78128)\n",
      "epoch 336: (0.57342, 0.5503732367308853, 0.80218)\n",
      "epoch 337: (0.57222, 0.5508792199740743, 0.78194)\n",
      "epoch 338: (0.57343, 0.5503462461432979, 0.80268)\n",
      "epoch 339: (0.57228, 0.5509014084507042, 0.78228)\n",
      "epoch 340: (0.57347, 0.5503529573024467, 0.80302)\n",
      "epoch 341: (0.57227, 0.5508735868448099, 0.78256)\n",
      "epoch 342: (0.57345, 0.5503240747084698, 0.80322)\n",
      "epoch 343: (0.5723, 0.5508725021108922, 0.7829)\n",
      "epoch 344: (0.57342, 0.550282160859084, 0.8035)\n",
      "epoch 345: (0.5723, 0.5508324427695596, 0.78346)\n",
      "epoch 346: (0.57341, 0.5502498459853515, 0.80386)\n",
      "epoch 347: (0.57242, 0.5509067903838043, 0.78372)\n",
      "epoch 348: (0.57344, 0.5502669404517454, 0.80394)\n",
      "epoch 349: (0.57247, 0.5509111601309485, 0.7842)\n",
      "epoch 350: (0.57341, 0.5502237182381675, 0.80424)\n",
      "epoch 351: (0.57244, 0.5508893697136594, 0.78418)\n",
      "epoch 352: (0.57338, 0.550201132911912, 0.80424)\n",
      "epoch 353: (0.57262, 0.5509728500435186, 0.78496)\n",
      "epoch 354: (0.57347, 0.5502262814640616, 0.80486)\n",
      "epoch 355: (0.57266, 0.5509994946375428, 0.78502)\n",
      "epoch 356: (0.57348, 0.5502365520824788, 0.80482)\n",
      "epoch 357: (0.57262, 0.5509413844383961, 0.7854)\n",
      "epoch 358: (0.57349, 0.5502166099517581, 0.80522)\n",
      "epoch 359: (0.57271, 0.5510052331046481, 0.78548)\n",
      "epoch 360: (0.57352, 0.5502391690583572, 0.80522)\n",
      "epoch 361: (0.57277, 0.5510058176210836, 0.78612)\n",
      "epoch 362: (0.57356, 0.5502404108840564, 0.80564)\n",
      "epoch 363: (0.57282, 0.5510329941412273, 0.78628)\n",
      "epoch 364: (0.57366, 0.5502935955209614, 0.80596)\n",
      "epoch 365: (0.57288, 0.5510521449186023, 0.78666)\n",
      "epoch 366: (0.5737, 0.550315410033043, 0.80608)\n",
      "epoch 367: (0.57282, 0.5509715533654385, 0.78714)\n",
      "epoch 368: (0.57358, 0.5501978441806522, 0.80648)\n",
      "epoch 369: (0.57284, 0.5509670015953427, 0.78742)\n",
      "epoch 370: (0.57358, 0.5501992140596005, 0.80646)\n",
      "epoch 371: (0.57296, 0.5510124174963642, 0.78808)\n",
      "epoch 372: (0.5736, 0.5501758883041095, 0.80702)\n",
      "epoch 373: (0.57294, 0.5509813240885708, 0.7883)\n",
      "epoch 374: (0.57365, 0.5502147678461853, 0.807)\n",
      "epoch 375: (0.57305, 0.5510318136727537, 0.78878)\n",
      "epoch 376: (0.5735, 0.5500967856266529, 0.80708)\n",
      "epoch 377: (0.57304, 0.5510269666061198, 0.78874)\n",
      "epoch 378: (0.57356, 0.5501294807141883, 0.80726)\n",
      "epoch 379: (0.57298, 0.5509864744019674, 0.78866)\n",
      "epoch 380: (0.5736, 0.5501499046061598, 0.8074)\n",
      "epoch 381: (0.573, 0.5509619949177627, 0.78922)\n",
      "epoch 382: (0.57365, 0.5501641488100915, 0.80774)\n",
      "epoch 383: (0.573, 0.5509662645218946, 0.78916)\n",
      "epoch 384: (0.57366, 0.5501648097196873, 0.80784)\n",
      "epoch 385: (0.57311, 0.5510195536574132, 0.7896)\n",
      "epoch 386: (0.57371, 0.550185873605948, 0.80808)\n",
      "epoch 387: (0.5732, 0.5510531454875157, 0.7901)\n",
      "epoch 388: (0.57363, 0.550112298373375, 0.80828)\n",
      "epoch 389: (0.57317, 0.5510372054741013, 0.79)\n",
      "epoch 390: (0.57356, 0.5500626122937877, 0.80824)\n",
      "epoch 391: (0.57321, 0.5510323579026614, 0.7905)\n",
      "epoch 392: (0.57365, 0.5500931807979541, 0.80878)\n",
      "epoch 393: (0.57322, 0.5510315026484528, 0.79062)\n",
      "epoch 394: (0.57358, 0.5500571459671275, 0.80854)\n",
      "epoch 395: (0.5733, 0.5510730211817169, 0.7909)\n",
      "epoch 396: (0.57353, 0.5500170056458744, 0.80858)\n",
      "epoch 397: (0.57331, 0.5510693138279346, 0.79106)\n",
      "epoch 398: (0.57354, 0.55, 0.80894)\n",
      "epoch 399: (0.57331, 0.5510550874016297, 0.79126)\n",
      "epoch 400: (0.5735, 0.5499619337647507, 0.80906)\n",
      "epoch 401: (0.57337, 0.5510926031670869, 0.79138)\n",
      "epoch 402: (0.57361, 0.5500224254862253, 0.80938)\n",
      "epoch 403: (0.57328, 0.5510107478977557, 0.79156)\n",
      "epoch 404: (0.57371, 0.5500740479069578, 0.80972)\n",
      "epoch 405: (0.57324, 0.5509814840595851, 0.79154)\n",
      "epoch 406: (0.57371, 0.5500618046971569, 0.8099)\n",
      "epoch 407: (0.57332, 0.5510201241406185, 0.79186)\n",
      "epoch 408: (0.57369, 0.5500387054717314, 0.81002)\n",
      "epoch 409: (0.57336, 0.5510436960757028, 0.79196)\n",
      "epoch 410: (0.57367, 0.5500101826081053, 0.81022)\n",
      "epoch 411: (0.57355, 0.5511182774773773, 0.79296)\n",
      "epoch 412: (0.57367, 0.5499667656913414, 0.81086)\n",
      "epoch 413: (0.57355, 0.5511211198687741, 0.79292)\n",
      "epoch 414: (0.57377, 0.5500305188199389, 0.81102)\n",
      "epoch 415: (0.5736, 0.5511480513704342, 0.79308)\n",
      "epoch 416: (0.57377, 0.5500237336407405, 0.81112)\n",
      "epoch 417: (0.57353, 0.5511072188164644, 0.7929)\n",
      "epoch 418: (0.57382, 0.5500556022674876, 0.8112)\n",
      "epoch 419: (0.5735, 0.551062942892872, 0.7932)\n",
      "epoch 420: (0.57385, 0.5500562582184446, 0.81152)\n",
      "epoch 421: (0.57349, 0.5510567049701955, 0.79318)\n",
      "epoch 422: (0.57391, 0.5500969268100913, 0.81158)\n",
      "epoch 423: (0.5735, 0.5510544302742352, 0.79332)\n",
      "epoch 424: (0.57395, 0.5501199625879387, 0.81168)\n",
      "epoch 425: (0.57352, 0.5510385427079862, 0.79376)\n",
      "epoch 426: (0.57398, 0.5501178765953988, 0.81204)\n",
      "epoch 427: (0.57352, 0.5510342912675275, 0.79382)\n",
      "epoch 428: (0.57398, 0.5501165185345762, 0.81206)\n",
      "epoch 429: (0.57352, 0.5510328742780987, 0.79384)\n",
      "epoch 430: (0.57399, 0.5501185395922238, 0.81214)\n",
      "epoch 431: (0.57351, 0.551006813860864, 0.7941)\n",
      "epoch 432: (0.57402, 0.5501341063639565, 0.81224)\n",
      "epoch 433: (0.5735, 0.5509892610372673, 0.79424)\n",
      "epoch 434: (0.57402, 0.5501246004658974, 0.81238)\n",
      "epoch 435: (0.57349, 0.5509660596141309, 0.79446)\n",
      "epoch 436: (0.57409, 0.5501536628623263, 0.81272)\n",
      "epoch 437: (0.57355, 0.5509949386396727, 0.7947)\n",
      "epoch 438: (0.57414, 0.5501732445455038, 0.81298)\n",
      "epoch 439: (0.57354, 0.5509830564876183, 0.79476)\n",
      "epoch 440: (0.57415, 0.5501739000987915, 0.81308)\n",
      "epoch 441: (0.57354, 0.5509745750963485, 0.79488)\n",
      "epoch 442: (0.57419, 0.5501778781771207, 0.81346)\n",
      "epoch 443: (0.57352, 0.550949410949411, 0.79502)\n",
      "epoch 444: (0.57418, 0.55015822356855, 0.81364)\n",
      "epoch 445: (0.57351, 0.5509347154280013, 0.79512)\n",
      "epoch 446: (0.57413, 0.5501169598550509, 0.8137)\n",
      "epoch 447: (0.57354, 0.5509505598048997, 0.79522)\n",
      "epoch 448: (0.57418, 0.5501419494389618, 0.81388)\n",
      "epoch 449: (0.57352, 0.5509310574152073, 0.79528)\n",
      "epoch 450: (0.57414, 0.5501094920111385, 0.81392)\n",
      "epoch 451: (0.57349, 0.5509025170737113, 0.79536)\n",
      "epoch 452: (0.57414, 0.550089179548157, 0.81422)\n",
      "epoch 453: (0.57352, 0.550911306852806, 0.79556)\n",
      "epoch 454: (0.57406, 0.5500270197243988, 0.81426)\n",
      "epoch 455: (0.57354, 0.5509181045226688, 0.79568)\n",
      "epoch 456: (0.5741, 0.5500418703908804, 0.81448)\n",
      "epoch 457: (0.57353, 0.5509006077891152, 0.79582)\n",
      "epoch 458: (0.57418, 0.5500904842935472, 0.81464)\n",
      "epoch 459: (0.57356, 0.550905165254941, 0.79608)\n",
      "epoch 460: (0.57423, 0.5501073294541724, 0.81494)\n",
      "epoch 461: (0.57354, 0.5508885075287865, 0.7961)\n",
      "epoch 462: (0.5743, 0.5501498420583709, 0.81508)\n",
      "epoch 463: (0.57359, 0.5509181738925868, 0.79622)\n",
      "epoch 464: (0.57429, 0.5501329410335659, 0.81522)\n",
      "epoch 465: (0.57369, 0.5509690270995588, 0.79658)\n",
      "epoch 466: (0.57427, 0.5501072715251447, 0.81538)\n",
      "epoch 467: (0.57369, 0.5509718475479006, 0.79654)\n",
      "epoch 468: (0.57432, 0.550141681284577, 0.81542)\n",
      "epoch 469: (0.57374, 0.5509986721257054, 0.7967)\n",
      "epoch 470: (0.57422, 0.550067458175931, 0.81542)\n",
      "epoch 471: (0.57377, 0.5509947325489762, 0.79708)\n",
      "epoch 472: (0.57425, 0.5500667556742324, 0.81576)\n",
      "epoch 473: (0.5738, 0.5510189973177004, 0.79706)\n",
      "epoch 474: (0.57431, 0.5501058622038218, 0.81584)\n",
      "epoch 475: (0.57381, 0.5510252049718639, 0.79708)\n",
      "epoch 476: (0.57426, 0.550074173971679, 0.81576)\n",
      "epoch 477: (0.57378, 0.5509952999723527, 0.79718)\n",
      "epoch 478: (0.57422, 0.5500404530744336, 0.81582)\n",
      "epoch 479: (0.57382, 0.5510074348414914, 0.79744)\n",
      "epoch 480: (0.57429, 0.5500734689475741, 0.8161)\n",
      "epoch 481: (0.57377, 0.5509679558927165, 0.79746)\n",
      "epoch 482: (0.57429, 0.5500721189490854, 0.81612)\n",
      "epoch 483: (0.57377, 0.5509679558927165, 0.79746)\n",
      "epoch 484: (0.57426, 0.5500498746394198, 0.81612)\n",
      "epoch 485: (0.57381, 0.5509899553725631, 0.79758)\n",
      "epoch 486: (0.57429, 0.5500653700483873, 0.81622)\n",
      "epoch 487: (0.57386, 0.5510195623342176, 0.7977)\n",
      "epoch 488: (0.57431, 0.5500734491448902, 0.81632)\n",
      "epoch 489: (0.57372, 0.5509679203539823, 0.79692)\n",
      "epoch 490: (0.57437, 0.5501017259731336, 0.81656)\n",
      "epoch 491: (0.57392, 0.551220931844009, 0.7955)\n",
      "epoch 492: (0.57438, 0.5501010373164489, 0.81668)\n",
      "epoch 493: (0.57392, 0.5513518770667185, 0.79366)\n",
      "epoch 494: (0.57438, 0.5501023872393167, 0.81666)\n",
      "epoch 495: (0.57402, 0.551502922349012, 0.79262)\n",
      "epoch 496: (0.57439, 0.5501016985681381, 0.81678)\n",
      "epoch 497: (0.57401, 0.5515871356279537, 0.79134)\n",
      "epoch 498: (0.57435, 0.5500693630719087, 0.81682)\n",
      "epoch 499: (0.57401, 0.551659151508383, 0.79034)\n"
     ]
    }
   ],
   "source": [
    "# attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)   \n",
    "# attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)\n",
    "attack_train_args = Train_args(learning_rate=0.001, weight_decay=1e-5, epoch=500)\n",
    "attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "for epoch in range(attack_train_args.epoch):\n",
    "    attack_model = attack.train_attack_model(attack_model, attack_train_data, attack_train_target, attack_train_args)\n",
    "    print(f'epoch {epoch}: {attack_evaluation(attack_model, attack_train_data, attack_train_target)}' )        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ba78e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = attack_evaluation(attack_model, attack_train_data, attack_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c46ca467",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = attack.mi_attack_test(model, attack_model, x_target_train, y_target_train, x_target_test, y_target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2392f52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.57401, 0.551659151508383, 0.79034)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.50259, 0.5017606351882317, 0.73812)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_res)\n",
    "test_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b459d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_path = f'mia/shms{number_of_sms}_shtrsize{shadow_size}_shlr{shadow_model.alpha}_shiter{int(shadow_model.max_iter/shadow_batch_size)}_shreg{shadow_model.lambda_}/'      \n",
    "os.mkdir(sh_path)\n",
    "\n",
    "torch.save(attack_train_data, sh_path+'attack_train_data.pt')\n",
    "torch.save(attack_train_target, sh_path+'attack_train_target.pt')\n",
    "\n",
    "at_path = sh_path+f'attack_model_aneur{attack_model.h_neurons}_ado{attack_model.do}_alr{attack_train_args.learning_rate}_alreg{attack_train_args.weight_decay}_aepoch{attack_train_args.epoch}'\n",
    "torch.save(attack_model, at_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6a77dac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.59217, 0.5930014328093153, 0.5877)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)\n",
    "train_acc, train_pre, train_rec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "61f91b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6260389215487533, 0.5935737236701178, 0.8313)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, test_pre, test_rec = attack_evaluation(attack_model, attack_test_data, attack_test_target)\n",
    "test_acc, test_pre, test_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc286901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ffcaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac16b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "# random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "\n",
    "epo = [200]\n",
    "wd = [1e-5, 1e-6]\n",
    "ams = {}\n",
    "for ep in epo:\n",
    "    for w_d in wd:\n",
    "        attack_train_args = Train_args(learning_rate=l_r, weight_decay=w_d, epoch=ep)\n",
    "        attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "        for epoch in range(attack_train_args.epoch):\n",
    "\n",
    "            attack_model = train_model(attack_model, attack_train_data, attack_train_target, attack_train_args)\n",
    "            ams[(l_r,w_d)] = attack_model\n",
    "        at_path = sh_path+f'attack_model_aneur{attack_model.h_neurons}_ado{attack_model.do}_alr{attack_train_args.learning_rate}_alreg{attack_train_args.weight_decay}_aepoch{attack_train_args.epoch}'\n",
    "        torch.save(attack_model, at_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "df4d74e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 1e-05\n",
      "Train:  0.8081 0.8207236842105263 0.78842\n",
      "Test:  0.6895398337725522 0.6209717711716213 0.9943\n",
      "200 1e-06\n",
      "Train:  0.80328 0.8273606493674712 0.7665\n",
      "Test:  0.7008919521589296 0.6304929308368361 0.99\n",
      "500 1e-05\n",
      "Train:  0.8081 0.8207236842105263 0.78842\n",
      "Test:  0.6895398337725522 0.6209717711716213 0.9943\n",
      "500 1e-06\n",
      "Train:  0.80328 0.8273606493674712 0.7665\n",
      "Test:  0.7008919521589296 0.6304929308368361 0.99\n"
     ]
    }
   ],
   "source": [
    "epo = [200,500]\n",
    "wd = [1e-5, 1e-6]\n",
    "\n",
    "for ep in epo:\n",
    "    for w_d in wd:\n",
    "        print(ep, w_d)\n",
    "        attack_train_args = Train_args(learning_rate=l_r, weight_decay=w_d, epoch=500)\n",
    "        attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "        at_path = sh_path+f'attack_model_aneur{attack_model.h_neurons}_ado{attack_model.do}_alr{attack_train_args.learning_rate}_alreg{attack_train_args.weight_decay}_aepoch{attack_train_args.epoch}'\n",
    "        attack_model = torch.load(at_path)\n",
    "        \n",
    "        train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)\n",
    "        print('Train: ', train_acc, train_pre, train_rec)\n",
    "        test_acc, test_pre, test_rec = attack_evaluation(attack_model, attack_test_data, attack_test_target)\n",
    "        print('Test: ' , test_acc, test_pre, test_rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l_r in lr:\n",
    "    for w_d in wd:\n",
    "        ams[(l_r,w_d)]\n",
    "        train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)\n",
    "        print(\"Train acc pre rec: \", train_acc, train_pre, train_rec)\n",
    "        test_acc, test_pre, test_rec = attack_evaluation(attack_model, attack_test_data, attack_test_target)\n",
    "        print(\"Test acc pre rec: \", test_acc, test_pre, test_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf9f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_size = 20000\n",
    "shadow_clf = LogisticRegression(random_state=1).fit(x_shadow[:shadow_size], y_shadow[:shadow_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eca16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shadow_clf.score(x_shadow[:shadow_size], y_shadow[:shadow_size]))\n",
    "print(shadow_clf.score(x_shadow[shadow_size:shadow_size*2], y_shadow[shadow_size:shadow_size*2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee911ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_train_pred = shadow_clf.predict_proba(x_shadow[:shadow_size])\n",
    "shadow_test_pred = shadow_clf.predict_proba(x_shadow[shadow_size:shadow_size*2])\n",
    "y_shadow_train = y_shadow[:shadow_size]\n",
    "y_shadow_test = y_shadow[shadow_size:shadow_size*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ab2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# members\n",
    "labels = np.ones(shadow_train_pred.shape[0])\n",
    "# non-members\n",
    "test_labels = np.zeros(shadow_test_pred.shape[0])\n",
    "\n",
    "x_1 = np.concatenate((shadow_train_pred, shadow_test_pred))\n",
    "x_2 = np.concatenate((y_shadow_train, y_shadow_test)).reshape((-1, 1))\n",
    "y_new = np.concatenate((labels, test_labels))\n",
    "\n",
    "attack_train_data = np.concatenate((x_1,x_2),axis=1)\n",
    "attack_train_target = y_new\n",
    "df = pd.DataFrame(attack_train_data)\n",
    "df['a_target'] = attack_train_target\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "attack_train_data = np.array(df.drop(['a_target'], axis=1))\n",
    "attack_train_target = np.array(df['a_target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b1987",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)   \n",
    "attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)\n",
    "attack_train_args = Train_args(learning_rate=0.001, weight_decay=1e-3, epoch=200)\n",
    "attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86acb25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49914cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)   \n",
    "attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)\n",
    "attack_train_args = Train_args(learning_rate=0.001, weight_decay=1e-3, epoch=500)\n",
    "attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "for epoch in range(attack_train_args.epoch):\n",
    "            \n",
    "    attack_model = train_model(attack_model, attack_train_data, attack_train_target, attack_train_args)\n",
    "    train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, train_pre, train_rec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2234ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(x_target_train, y_target_train)\n",
    "test_pred = model.predict(x_target_test, y_target_test)\n",
    "\n",
    "y_target_train_ohe = OneHotEncoder(sparse=False).fit_transform(y_target_train.reshape(-1,1)) #encoode the target values\n",
    "y_target_test_ohe = OneHotEncoder(sparse=False).fit_transform(y_target_test.reshape(-1,1)) #encoode the target values\n",
    "    \n",
    "# members\n",
    "labels = np.ones(train_pred.shape[0])\n",
    "# non-members\n",
    "test_labels = np.zeros(test_pred.shape[0])\n",
    "\n",
    "x_1 = np.concatenate((train_pred, test_pred))\n",
    "x_2 = np.concatenate((y_target_train_ohe, y_target_test_ohe))#.reshape((-1, 1))\n",
    "y_new = np.concatenate((labels, test_labels))\n",
    "\n",
    "attack_test_data = np.concatenate((x_1,x_2),axis=1)\n",
    "attack_test_target = y_new\n",
    "df = pd.DataFrame(attack_test_data)\n",
    "df['a_target'] = attack_test_target\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "attack_test_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)\n",
    "attack_test_target = torch.tensor(np.array(df['a_target']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_pre, test_rec = attack_evaluation(a_model, attack_test_data, attack_test_target)\n",
    "test_acc, test_pre, test_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2bcabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_model = Net_attack(h_neurons=64, do=0, input_size=100)\n",
    "a_model = torch.load('attack_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41bca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "# random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "\n",
    "lr = [0.001]\n",
    "wd = [1e-4, 1e-6]\n",
    "tms = {}\n",
    "for l_r in lr:\n",
    "    for w_d in wd:\n",
    "        model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "        model.n_classes      = 100\n",
    "        model.alpha          = 0.001\n",
    "        model.max_iter       = 100*X_train_size \n",
    "        model.lambda_        = 1e-3\n",
    "        model.tolerance      = 10e-5\n",
    "        model.DP             = True\n",
    "        model.epsilon\n",
    "\n",
    "        X,y = model.init_theta(x_target_train, y_target_train)\n",
    "        model.SGD(X,y)\n",
    "        model.evaluate(x_target_train, y_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import algo\n",
    "# import attack\n",
    "\n",
    "# from torch import nn,optim\n",
    "import torch\n",
    "\n",
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "        \n",
    "\n",
    "raw_data_path = '../datasets/dataset_purchase'\n",
    "raw_data = pd.read_csv(raw_data_path)\n",
    "y=raw_data['63']\n",
    "X=raw_data.drop('63', axis=1)\n",
    "y =  y.replace(100, 0)\n",
    "print('Dataset: ', raw_data_path)\n",
    "print('Classes in classification task: ', y.nunique())\n",
    "n_classes = y.nunique()\n",
    "\n",
    "X_train, x_shadow, y_train, y_shadow = train_test_split(X, y, train_size=0.2, random_state=rand_seed)\n",
    "print(X_train.shape, x_shadow.shape)\n",
    "\n",
    "#Target model\n",
    "X_train_size = 10000\n",
    "X_test_size = 10000\n",
    "x_target_train = np.array(X_train[:X_train_size])\n",
    "y_target_train = np.array(y_train[:X_train_size])\n",
    "x_target_test = np.array(X_train[X_train_size:X_train_size+X_test_size])\n",
    "y_target_test = np.array(y_train[X_train_size:X_train_size+X_test_size])\n",
    "if y_target_test.shape[0]<X_test_size or y_target_train.shape[0]<X_train_size:\n",
    "    raise ValueError(\n",
    "            \"Not enough traning or test data for the target model\")\n",
    "\n",
    "for L in [1,10,100]:\n",
    "    for epsilon in np.arange(0,1,0.1):\n",
    "        model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "        model.n_classes      = n_classes\n",
    "        model.alpha          = 0.001\n",
    "        model.max_iter       = 100*X_train_size\n",
    "        model.lambda_        = 1e-3\n",
    "        model.tolerance      = 10e-5\n",
    "        model.DP             = True\n",
    "        model.L              = L\n",
    "        model.epsilon        = epsilon\n",
    "\n",
    "\n",
    "        X,y = model.init_theta(x_target_train, y_target_train)\n",
    "        model.train(X,y)\n",
    "        model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "        model.evaluate(x_target_test, y_target_test, acc=True)\n",
    "\n",
    "        tm_path = f'tm/lr{model.alpha}_iter{int(model.max_iter/X_train_size)}_reg{model.lambda_}_DP{model.DP}'\n",
    "        if model.DP:\n",
    "            tm_path += f'_eps{model.epsilon}_L{model.L}'\n",
    "        np.save(tm_path+'_target_model', model.theta)\n",
    "\n",
    "        print(tm_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee18d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shadow models\n",
    "# s_ms = {}\n",
    "# number_of_sms = 10\n",
    "# shadow_size = 50000\n",
    "# shadow_batch_size = int(shadow_size/number_of_sms)\n",
    "\n",
    "# x_shadow_train = np.array(x_shadow[:shadow_size])\n",
    "# y_shadow_train = np.array(y_shadow[:shadow_size])\n",
    "# x_shadow_test = np.array(x_shadow[shadow_size:2*shadow_size])\n",
    "# y_shadow_test = np.array(y_shadow[shadow_size:2*shadow_size])\n",
    "\n",
    "# attack.train_shadow_models(number_of_sms,)\n",
    "\n",
    "# for i in range(number_of_sms):  \n",
    "#     batch_start = i*shadow_batch_size\n",
    "#     batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "#     shadow_model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "#     shadow_model.n_classes      = n_classes\n",
    "#     shadow_model.alpha          = 0.001\n",
    "#     shadow_model.max_iter       = 100*shadow_batch_size\n",
    "#     shadow_model.lambda_        = 10e-3\n",
    "#     shadow_model.tolerance      = 10e-5\n",
    "#     shadow_model.DP             = False\n",
    "\n",
    "#     X,y = shadow_model.init_theta(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end] )\n",
    "#     shadow_model.SGD(X,y)\n",
    "#     print('Shadow model: ', i)\n",
    "#     shadow_model.evaluate(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end])\n",
    "#     shadow_model.evaluate(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end])\n",
    "#     s_ms[i] = shadow_model\n",
    "\n",
    "# #Attack model\n",
    "\n",
    "# shadow_train_pred = []\n",
    "# shadow_test_pred = []\n",
    "\n",
    "# for i in range(number_of_sms): \n",
    "#     batch_start = i*shadow_batch_size\n",
    "#     batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "#     train_prediciton = s_ms[i].predict(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end])\n",
    "#     test_prediciton = s_ms[i].predict(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end])\n",
    "    \n",
    "#     shadow_train_pred.append(train_prediciton)\n",
    "#     shadow_test_pred.append(test_prediciton)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f530940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/10000 == 1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7622903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0,1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca6fe0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
