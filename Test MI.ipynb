{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf2f0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 15:08:12.480907: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from torch import nn,optim\n",
    "import torch\n",
    "\n",
    "import algo\n",
    "\n",
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import attack\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa0caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_attack(nn.Module):\n",
    "\n",
    "    def __init__(self, h_neurons, do, input_size):\n",
    "        super(Net_attack, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.h_neurons = h_neurons\n",
    "        self.do = do\n",
    "        self.fc1 = nn.Linear(input_size, h_neurons)\n",
    "        self.fc2 = nn.Linear(h_neurons, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(do)\n",
    "        self.softmax = nn.Softmax(dim=1)   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class Train_args():\n",
    "\n",
    "    def __init__(self, learning_rate, weight_decay, epoch):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epoch = epoch\n",
    "         \n",
    "def train_attack_model(model, train_data, train_target, train_args):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=train_args.learning_rate, weight_decay=train_args.weight_decay)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_data)\n",
    "    loss = nn.CrossEntropyLoss()(output, train_target.to(torch.long))\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "def attack_evaluation(model, x, y, dev=\"cpu\", extended=False):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output =  model(x)\n",
    "        out_target = output.argmax(1, keepdim=True)\n",
    "        correct = out_target.to(dev).eq(y.to(dev).view_as(out_target.to(dev))).sum().item()\n",
    "        acc = correct/y.shape[0]\n",
    "\n",
    "        predicted_positive = output.argmax(1, keepdim=True) == 1\n",
    "        labeled_positive = y == 1\n",
    "        tp = predicted_positive.to(dev) * labeled_positive.to(dev).view_as(out_target)\n",
    "        tp_count = tp.to(dev).sum().item()\n",
    "        \n",
    "        if predicted_positive.to(dev).sum().item() != 0:\n",
    "            pre = tp_count / predicted_positive.to(dev).sum().item()\n",
    "        else:\n",
    "            pre = 0\n",
    "        if labeled_positive.to(dev).sum().item() !=0:\n",
    "            rec = tp_count / labeled_positive.to(dev).sum().item()\n",
    "        else:\n",
    "            rec = 0\n",
    "    if extended:\n",
    "        predicted_negative = output.argmax(1, keepdim=True) == 0\n",
    "        labeled_negative = y == 0\n",
    "        tn = predicted_negative.to(dev) * labeled_negative.to(dev).view_as(out_target)\n",
    "        tn_count = tn.to(dev).sum().item()\n",
    "\n",
    "        fp_count = predicted_positive.to(dev).sum().item() - tp.to(dev).sum().item()\n",
    "        fn_count = labeled_positive.to(dev).sum().item() - tp.to(dev).sum().item()\n",
    "        \n",
    "        return acc, pre, rec, tp_count, tn_count, fp_count, fn_count\n",
    "    else:\n",
    "        return acc, pre, rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "684c6361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "(19732, 600) (177591, 600)\n"
     ]
    }
   ],
   "source": [
    "# raw_data = pd.read_csv('../datasets/dataset_purchase', )\n",
    "# y=raw_data['63']\n",
    "# X=raw_data.drop('63', axis=1)\n",
    "# y =  y.replace(100, 0)\n",
    "# print(y.nunique())\n",
    "\n",
    "# X_train, x_shadow, y_train, y_shadow = train_test_split(X, y, train_size=0.1, random_state=42)\n",
    "# print(X_train.shape, x_shadow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa3395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../DP-UTIL.nosync/loan_preprocessed.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "523b87ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661283, 167)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7b1c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loan\n",
    "y = data['grade']\n",
    "X = data.drop('grade', axis=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "tr_size = 10000\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[0:100000], y[0:100000], train_size=0.5, random_state=0)\n",
    "x_target_train = np.array(X_train)[:tr_size]\n",
    "y_target_train = np.array(y_train)[:tr_size]\n",
    "x_target_test = np.array(X_test)[:tr_size]\n",
    "y_target_test = np.array(y_test)[:tr_size]\n",
    "\n",
    "x_shadow = X[100000:]\n",
    "y_shadow = y[100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77aad817",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "model.n_classes      = len(np.unique(y_target_test))\n",
    "model.alpha          = 0.001\n",
    "model.max_iter       = 100\n",
    "model.lambda_        = 1e-5\n",
    "model.tolerance      = 1e-5\n",
    "model.DP             = False\n",
    "model.L              = 1\n",
    "model.epsilon        = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc6fa43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model : 88.0 %\n",
      "The accuracy of the model : 83.7 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8371"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "random.seed(rand_seed)\n",
    "\n",
    "X,y = model.init_theta(x_target_train, y_target_train)\n",
    "model.train(X,y)\n",
    "model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "model.evaluate(x_target_test, y_target_test, acc=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3727bc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model : 84.39999999999999 %\n",
      "The accuracy of the model : 57.99999999999999 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.58"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "model.evaluate(x_target_test, y_target_test, acc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b14812a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/0p0lnj8d6zj532q0yvm1zt580000gn/T/ipykernel_47583/701540644.py:7: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_shadow_train = np.array(y_shadow[:shadow_size])\n",
      "/var/folders/66/0p0lnj8d6zj532q0yvm1zt580000gn/T/ipykernel_47583/701540644.py:9: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_shadow_test = np.array(y_shadow[shadow_size:2*shadow_size])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shadow model:  0\n",
      "The accuracy of the model : 84.1 %\n",
      "The accuracy of the model : 59.4 %\n",
      "Shadow model:  1\n",
      "The accuracy of the model : 84.8 %\n",
      "The accuracy of the model : 60.5 %\n",
      "Shadow model:  2\n",
      "The accuracy of the model : 85.7 %\n",
      "The accuracy of the model : 57.4 %\n",
      "Shadow model:  3\n",
      "The accuracy of the model : 81.0 %\n",
      "The accuracy of the model : 62.3 %\n",
      "Shadow model:  4\n",
      "The accuracy of the model : 86.5 %\n",
      "The accuracy of the model : 56.00000000000001 %\n",
      "Shadow model:  5\n",
      "The accuracy of the model : 83.7 %\n",
      "The accuracy of the model : 61.4 %\n",
      "Shadow model:  6\n",
      "The accuracy of the model : 85.0 %\n",
      "The accuracy of the model : 56.39999999999999 %\n",
      "Shadow model:  7\n",
      "The accuracy of the model : 85.3 %\n",
      "The accuracy of the model : 61.9 %\n",
      "Shadow model:  8\n",
      "The accuracy of the model : 83.2 %\n",
      "The accuracy of the model : 57.599999999999994 %\n",
      "Shadow model:  9\n",
      "The accuracy of the model : 84.2 %\n",
      "The accuracy of the model : 55.60000000000001 %\n"
     ]
    }
   ],
   "source": [
    "s_ms = {}\n",
    "number_of_sms = 10\n",
    "shadow_size = 10000\n",
    "shadow_batch_size = int(shadow_size/number_of_sms)\n",
    "\n",
    "x_shadow_train = np.array(x_shadow[:shadow_size])\n",
    "y_shadow_train = np.array(y_shadow[:shadow_size])\n",
    "x_shadow_test = np.array(x_shadow[shadow_size:2*shadow_size])\n",
    "y_shadow_test = np.array(y_shadow[shadow_size:2*shadow_size])\n",
    "\n",
    "for i in range(number_of_sms):\n",
    "    batch_start = i*shadow_batch_size\n",
    "    batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "    shadow_model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "    shadow_model.n_classes      = len(np.unique(y_target_test))\n",
    "    shadow_model.alpha          = 0.001\n",
    "    shadow_model.max_iter       = 100\n",
    "    shadow_model.lambda_        = 10e-5\n",
    "    shadow_model.tolerance      = 10e-5\n",
    "    shadow_model.DP             = False\n",
    "\n",
    "    X,y = shadow_model.init_theta(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end] )\n",
    "    shadow_model.SGD(X,y)\n",
    "    print('Shadow model: ', i)\n",
    "    shadow_model.evaluate(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end], acc=True)\n",
    "    shadow_model.evaluate(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end], acc=True)\n",
    "    s_ms[i] = shadow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "600e22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_train_pred = []\n",
    "shadow_test_pred = []\n",
    "\n",
    "for i in range(number_of_sms): \n",
    "    batch_start = i*shadow_batch_size\n",
    "    batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "    train_prediciton = s_ms[i].predict(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end])\n",
    "    test_prediciton = s_ms[i].predict(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end])\n",
    "    \n",
    "    shadow_train_pred.append(train_prediciton)\n",
    "    shadow_test_pred.append(test_prediciton)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49a69604",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_shadow_train_ohe = OneHotEncoder(sparse=False).fit_transform(y_shadow_train.reshape(-1,1)) #encoode the target values\n",
    "y_shadow_test_ohe = OneHotEncoder(sparse=False).fit_transform(y_shadow_test.reshape(-1,1)) #encoode the target values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d261d0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shadow_train_pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41420b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_train_pred = np.concatenate(shadow_train_pred)\n",
    "sh_test_pred = np.concatenate(shadow_test_pred)\n",
    "\n",
    "# members\n",
    "labels = np.ones(sh_train_pred.shape[0])\n",
    "# non-members\n",
    "test_labels = np.zeros(sh_test_pred.shape[0])\n",
    "\n",
    "x_1 = np.concatenate((sh_train_pred, sh_test_pred))\n",
    "x_2 = np.concatenate((y_shadow_train_ohe, y_shadow_test_ohe))#.reshape((-1, 1))\n",
    "y_new = np.concatenate((labels, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e11035f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_train_data = np.concatenate((x_1,x_2),axis=1)\n",
    "attack_train_target = y_new\n",
    "df = pd.DataFrame(attack_train_data)\n",
    "df['a_target'] = attack_train_target\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)\n",
    "attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "399bf1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: (0.5168, 0.5436590436590436, 0.2092)\n",
      "epoch 1: (0.5153, 0.5384422110552763, 0.2143)\n",
      "epoch 2: (0.51035, 0.5248977628097186, 0.2182)\n",
      "epoch 3: (0.50225, 0.5050732807215332, 0.224)\n",
      "epoch 4: (0.49565, 0.4910585817060637, 0.2389)\n",
      "epoch 5: (0.5034, 0.5048158640226629, 0.3564)\n",
      "epoch 6: (0.52905, 0.526639156350298, 0.5743)\n",
      "epoch 7: (0.5241, 0.5193449991973029, 0.647)\n",
      "epoch 8: (0.51985, 0.5150755677071467, 0.6782)\n",
      "epoch 9: (0.5226, 0.517168034032209, 0.6808)\n",
      "epoch 10: (0.52385, 0.5183815028901734, 0.6726)\n",
      "epoch 11: (0.5248, 0.5193146417445483, 0.6668)\n",
      "epoch 12: (0.5319, 0.5243846506650359, 0.686)\n",
      "epoch 13: (0.5418, 0.5304044224614489, 0.7292)\n",
      "epoch 14: (0.55215, 0.5366453516969996, 0.7637)\n",
      "epoch 15: (0.5606, 0.541558085310657, 0.7897)\n",
      "epoch 16: (0.5696, 0.5469065911847958, 0.8115)\n",
      "epoch 17: (0.5743, 0.5493687707641196, 0.8268)\n",
      "epoch 18: (0.57885, 0.5524373212741903, 0.8307)\n",
      "epoch 19: (0.5813, 0.5547179970386323, 0.8242)\n",
      "epoch 20: (0.58375, 0.5564924114671164, 0.825)\n",
      "epoch 21: (0.5869, 0.559643102264928, 0.8154)\n",
      "epoch 22: (0.5866, 0.560105496946141, 0.807)\n",
      "epoch 23: (0.58725, 0.561232367183662, 0.7997)\n",
      "epoch 24: (0.5875, 0.5620127569099929, 0.793)\n",
      "epoch 25: (0.58695, 0.5617060535093322, 0.7915)\n",
      "epoch 26: (0.58605, 0.5620627479264335, 0.7793)\n",
      "epoch 27: (0.5856, 0.5621731551423591, 0.774)\n",
      "epoch 28: (0.58385, 0.5614060783595752, 0.7666)\n",
      "epoch 29: (0.5809, 0.5597930524759793, 0.7574)\n",
      "epoch 30: (0.5793, 0.5590205418279249, 0.7511)\n",
      "epoch 31: (0.57915, 0.5592484467400255, 0.7471)\n",
      "epoch 32: (0.57815, 0.5587373167981962, 0.7434)\n",
      "epoch 33: (0.57755, 0.5583126550868487, 0.7425)\n",
      "epoch 34: (0.5776, 0.5586102719033232, 0.7396)\n",
      "epoch 35: (0.5777, 0.5587035358114234, 0.7395)\n",
      "epoch 36: (0.5784, 0.5593489780469342, 0.7389)\n",
      "epoch 37: (0.57815, 0.5592359584628211, 0.7378)\n",
      "epoch 38: (0.5785, 0.5595689786006981, 0.7374)\n",
      "epoch 39: (0.5791, 0.5600972496581067, 0.7372)\n",
      "epoch 40: (0.5794, 0.5605459813939302, 0.7351)\n",
      "epoch 41: (0.5799, 0.5608993902439025, 0.7359)\n",
      "epoch 42: (0.58105, 0.5622264875239923, 0.7323)\n",
      "epoch 43: (0.58155, 0.5623518617631318, 0.7355)\n",
      "epoch 44: (0.5822, 0.5632697044334976, 0.7318)\n",
      "epoch 45: (0.58255, 0.5634951157603262, 0.7326)\n",
      "epoch 46: (0.5837, 0.5644341801385682, 0.7332)\n",
      "epoch 47: (0.585, 0.5652140555470309, 0.7367)\n",
      "epoch 48: (0.58665, 0.5664035558280328, 0.7391)\n",
      "epoch 49: (0.5884, 0.5676151139666513, 0.7421)\n",
      "epoch 50: (0.5892, 0.5683106141828764, 0.7421)\n",
      "epoch 51: (0.5907, 0.5690889701401585, 0.7471)\n",
      "epoch 52: (0.5921, 0.5705638982531412, 0.7447)\n",
      "epoch 53: (0.59335, 0.5711997559301351, 0.7489)\n",
      "epoch 54: (0.59445, 0.5720167746854746, 0.7502)\n",
      "epoch 55: (0.5955, 0.5728118328758768, 0.7513)\n",
      "epoch 56: (0.59675, 0.5736693824716363, 0.7534)\n",
      "epoch 57: (0.59795, 0.5743566385789114, 0.7566)\n",
      "epoch 58: (0.5993, 0.575616813889735, 0.7559)\n",
      "epoch 59: (0.59965, 0.5756356736242885, 0.7584)\n",
      "epoch 60: (0.6006, 0.5765601217656012, 0.7576)\n",
      "epoch 61: (0.6019, 0.5773258461071483, 0.7608)\n",
      "epoch 62: (0.60225, 0.5779047619047619, 0.7585)\n",
      "epoch 63: (0.60225, 0.5775267268178027, 0.7617)\n",
      "epoch 64: (0.60335, 0.5789353089437104, 0.758)\n",
      "epoch 65: (0.6042, 0.5793723339427178, 0.7606)\n",
      "epoch 66: (0.6055, 0.5806081907090465, 0.7599)\n",
      "epoch 67: (0.6064, 0.5808264965056213, 0.7646)\n",
      "epoch 68: (0.6072, 0.5822085889570552, 0.7592)\n",
      "epoch 69: (0.60745, 0.5818043395508185, 0.7642)\n",
      "epoch 70: (0.6079, 0.583, 0.7579)\n",
      "epoch 71: (0.60875, 0.5831612755219087, 0.7626)\n",
      "epoch 72: (0.6105, 0.5863011558887847, 0.7507)\n",
      "epoch 73: (0.60995, 0.5841690270228891, 0.7631)\n",
      "epoch 74: (0.61165, 0.587328901055925, 0.7509)\n",
      "epoch 75: (0.6112, 0.5857230958988591, 0.7598)\n",
      "epoch 76: (0.6138, 0.58950762938493, 0.7495)\n",
      "epoch 77: (0.6151, 0.5902461972714442, 0.7528)\n",
      "epoch 78: (0.6154, 0.5919375398342893, 0.743)\n",
      "epoch 79: (0.61545, 0.5901600937133932, 0.7557)\n",
      "epoch 80: (0.61655, 0.593561852773541, 0.7394)\n",
      "epoch 81: (0.61775, 0.5930315240578337, 0.7506)\n",
      "epoch 82: (0.61935, 0.5966788173349534, 0.7366)\n",
      "epoch 83: (0.61945, 0.5947940639631776, 0.7495)\n",
      "epoch 84: (0.62025, 0.5978835978835979, 0.7345)\n",
      "epoch 85: (0.6221, 0.5980093112859207, 0.745)\n",
      "epoch 86: (0.62295, 0.6015025179559151, 0.7286)\n",
      "epoch 87: (0.62315, 0.5993385496491087, 0.743)\n",
      "epoch 88: (0.6233, 0.601348019069538, 0.7316)\n",
      "epoch 89: (0.6245, 0.6014835343984349, 0.7379)\n",
      "epoch 90: (0.62355, 0.6021665426279665, 0.7282)\n",
      "epoch 91: (0.62595, 0.6021575148024981, 0.7424)\n",
      "epoch 92: (0.6237, 0.6023667659715326, 0.7279)\n",
      "epoch 93: (0.62535, 0.6025861363450364, 0.7363)\n",
      "epoch 94: (0.62465, 0.6039703061139378, 0.7241)\n",
      "epoch 95: (0.62575, 0.60264468206677, 0.7383)\n",
      "epoch 96: (0.6244, 0.6036321226257914, 0.7246)\n",
      "epoch 97: (0.6259, 0.6036555244524946, 0.7332)\n",
      "epoch 98: (0.6246, 0.6047410894418291, 0.7194)\n",
      "epoch 99: (0.62695, 0.604185473943373, 0.7362)\n",
      "epoch 100: (0.6249, 0.6049756261556564, 0.7198)\n",
      "epoch 101: (0.6269, 0.6044272547728768, 0.7345)\n",
      "epoch 102: (0.62515, 0.6053894736842105, 0.7189)\n",
      "epoch 103: (0.62715, 0.6054224359505845, 0.7302)\n",
      "epoch 104: (0.6261, 0.6069369063772049, 0.7157)\n",
      "epoch 105: (0.6273, 0.605398244742507, 0.7312)\n",
      "epoch 106: (0.62565, 0.6062399594148982, 0.717)\n",
      "epoch 107: (0.62755, 0.6058769818212003, 0.7299)\n",
      "epoch 108: (0.6259, 0.6066045723962743, 0.7164)\n",
      "epoch 109: (0.6273, 0.6062249666221629, 0.7265)\n",
      "epoch 110: (0.6259, 0.6067130022037633, 0.7158)\n",
      "epoch 111: (0.6282, 0.606602361549975, 0.7295)\n",
      "epoch 112: (0.62555, 0.6065156528378722, 0.7149)\n",
      "epoch 113: (0.6275, 0.606694560669456, 0.725)\n",
      "epoch 114: (0.62645, 0.6074067782213539, 0.7151)\n",
      "epoch 115: (0.62705, 0.6064516129032258, 0.7238)\n",
      "epoch 116: (0.6263, 0.6075992502981769, 0.7132)\n",
      "epoch 117: (0.62755, 0.6064069408525903, 0.7269)\n",
      "epoch 118: (0.626, 0.607051826677995, 0.7145)\n",
      "epoch 119: (0.62755, 0.6066917607695524, 0.7253)\n",
      "epoch 120: (0.6259, 0.6072950400545424, 0.7126)\n",
      "epoch 121: (0.62815, 0.607229520542214, 0.7257)\n",
      "epoch 122: (0.6265, 0.6079167377580618, 0.7126)\n",
      "epoch 123: (0.62805, 0.6070920799531655, 0.7259)\n",
      "epoch 124: (0.62635, 0.6075960146470237, 0.7135)\n",
      "epoch 125: (0.62855, 0.6076362722933936, 0.7257)\n",
      "epoch 126: (0.6272, 0.6084029316516107, 0.7139)\n",
      "epoch 127: (0.6287, 0.6078612135434127, 0.7253)\n",
      "epoch 128: (0.628, 0.6091591335493775, 0.7143)\n",
      "epoch 129: (0.6289, 0.6077759197324415, 0.7269)\n",
      "epoch 130: (0.62795, 0.6089214267472546, 0.7153)\n",
      "epoch 131: (0.6287, 0.6081149193548387, 0.7239)\n",
      "epoch 132: (0.6281, 0.6094310609943618, 0.7134)\n",
      "epoch 133: (0.62905, 0.6078202021889882, 0.7275)\n",
      "epoch 134: (0.62855, 0.6098811864261903, 0.7135)\n",
      "epoch 135: (0.6291, 0.6082145850796312, 0.7256)\n",
      "epoch 136: (0.6287, 0.6099436186570989, 0.714)\n",
      "epoch 137: (0.6296, 0.6087248322147651, 0.7256)\n",
      "epoch 138: (0.62915, 0.6105641640270525, 0.7132)\n",
      "epoch 139: (0.6295, 0.6084226389819156, 0.7267)\n",
      "epoch 140: (0.62875, 0.6101839965768079, 0.713)\n",
      "epoch 141: (0.6298, 0.6091123066577001, 0.7246)\n",
      "epoch 142: (0.62945, 0.6106126634196359, 0.7146)\n",
      "epoch 143: (0.6298, 0.6089474567735438, 0.7255)\n",
      "epoch 144: (0.62965, 0.6112016467964663, 0.7126)\n",
      "epoch 145: (0.6302, 0.6088628762541806, 0.7282)\n",
      "epoch 146: (0.62995, 0.6109830045264326, 0.7154)\n",
      "epoch 147: (0.6303, 0.6096063257065949, 0.7247)\n",
      "epoch 148: (0.63045, 0.6117153378436242, 0.7143)\n",
      "epoch 149: (0.6307, 0.6097028705724358, 0.7264)\n",
      "epoch 150: (0.6306, 0.6117767887709689, 0.7148)\n",
      "epoch 151: (0.631, 0.6103250800067375, 0.7247)\n",
      "epoch 152: (0.6304, 0.6115101761587138, 0.7151)\n",
      "epoch 153: (0.63075, 0.6099386193559236, 0.7254)\n",
      "epoch 154: (0.6305, 0.6119787197528745, 0.7132)\n",
      "epoch 155: (0.63105, 0.6097111762243617, 0.7283)\n",
      "epoch 156: (0.6303, 0.6114437222032159, 0.7149)\n",
      "epoch 157: (0.63095, 0.6103666245259166, 0.7242)\n",
      "epoch 158: (0.63025, 0.6115058642239535, 0.7143)\n",
      "epoch 159: (0.63115, 0.6101267948610295, 0.7266)\n",
      "epoch 160: (0.6303, 0.611577324884398, 0.7142)\n",
      "epoch 161: (0.6312, 0.6100855848296695, 0.7271)\n",
      "epoch 162: (0.63045, 0.6116579645638962, 0.7146)\n",
      "epoch 163: (0.6315, 0.6100234270414994, 0.7291)\n",
      "epoch 164: (0.63025, 0.6112392176957896, 0.7157)\n",
      "epoch 165: (0.63165, 0.6108444893491622, 0.7255)\n",
      "epoch 166: (0.63085, 0.611789833404528, 0.7161)\n",
      "epoch 167: (0.63165, 0.610010863207153, 0.73)\n",
      "epoch 168: (0.63095, 0.6117034888680372, 0.7171)\n",
      "epoch 169: (0.63205, 0.6110316993189271, 0.7267)\n",
      "epoch 170: (0.6309, 0.6118421052631579, 0.7161)\n",
      "epoch 171: (0.6315, 0.6098947016546883, 0.7298)\n",
      "epoch 172: (0.63115, 0.6117978006990027, 0.7177)\n",
      "epoch 173: (0.63165, 0.6102503977891299, 0.7287)\n",
      "epoch 174: (0.63115, 0.6120461341307134, 0.7164)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 175: (0.6315, 0.609674728940784, 0.731)\n",
      "epoch 176: (0.6312, 0.611678583588696, 0.7186)\n",
      "epoch 177: (0.63205, 0.6107894957630674, 0.728)\n",
      "epoch 178: (0.6314, 0.6119631901840491, 0.7182)\n",
      "epoch 179: (0.6318, 0.6098150308281953, 0.7319)\n",
      "epoch 180: (0.6311, 0.6115175229669956, 0.7189)\n",
      "epoch 181: (0.63185, 0.6099391311598432, 0.7315)\n",
      "epoch 182: (0.63145, 0.6117297067573311, 0.7197)\n",
      "epoch 183: (0.63175, 0.6096911164765632, 0.7323)\n",
      "epoch 184: (0.63105, 0.6112762163539102, 0.7199)\n",
      "epoch 185: (0.63195, 0.609784507862551, 0.7329)\n",
      "epoch 186: (0.63115, 0.6113233172056701, 0.7202)\n",
      "epoch 187: (0.63185, 0.6096830546543549, 0.7329)\n",
      "epoch 188: (0.63105, 0.6110875646350767, 0.7209)\n",
      "epoch 189: (0.63205, 0.609648758614963, 0.7342)\n",
      "epoch 190: (0.631, 0.6110734271663558, 0.7207)\n",
      "epoch 191: (0.6322, 0.6098554096725943, 0.7339)\n",
      "epoch 192: (0.63085, 0.610861645344404, 0.721)\n",
      "epoch 193: (0.63195, 0.6094929881337648, 0.7345)\n",
      "epoch 194: (0.63105, 0.6109371031913993, 0.7217)\n",
      "epoch 195: (0.6319, 0.6093335543766578, 0.7351)\n",
      "epoch 196: (0.63105, 0.6108432715892751, 0.7222)\n",
      "epoch 197: (0.63215, 0.6094409937888199, 0.7359)\n",
      "epoch 198: (0.6314, 0.6110547667342799, 0.723)\n",
      "epoch 199: (0.632, 0.6091992058239576, 0.7364)\n",
      "epoch 200: (0.6312, 0.6109232330064255, 0.7226)\n",
      "epoch 201: (0.6322, 0.6091659785301404, 0.7377)\n",
      "epoch 202: (0.63105, 0.6104136827028394, 0.7245)\n",
      "epoch 203: (0.6326, 0.6096411443691087, 0.7373)\n",
      "epoch 204: (0.63105, 0.6104881544557794, 0.7241)\n",
      "epoch 205: (0.6324, 0.6093130779392338, 0.738)\n",
      "epoch 206: (0.63125, 0.6105449338836014, 0.7249)\n",
      "epoch 207: (0.6328, 0.6092824226464779, 0.7404)\n",
      "epoch 208: (0.6316, 0.6105696521593009, 0.7267)\n",
      "epoch 209: (0.63285, 0.6096755551886403, 0.7385)\n",
      "epoch 210: (0.63135, 0.6104431178003867, 0.726)\n",
      "epoch 211: (0.6328, 0.6091207888249794, 0.7413)\n",
      "epoch 212: (0.63135, 0.6103503318491137, 0.7265)\n",
      "epoch 213: (0.6328, 0.6092464626521882, 0.7406)\n",
      "epoch 214: (0.6311, 0.6102051109616677, 0.7259)\n",
      "epoch 215: (0.63255, 0.6087277499794931, 0.7421)\n",
      "epoch 216: (0.63145, 0.6102675950004194, 0.7275)\n",
      "epoch 217: (0.6325, 0.6087491792514773, 0.7417)\n",
      "epoch 218: (0.63175, 0.6104636538945251, 0.7281)\n",
      "epoch 219: (0.6325, 0.6084110620193094, 0.7436)\n",
      "epoch 220: (0.6319, 0.6104135275405994, 0.7292)\n",
      "epoch 221: (0.63275, 0.6086778550961932, 0.7435)\n",
      "epoch 222: (0.63195, 0.6103537676674751, 0.7298)\n",
      "epoch 223: (0.63275, 0.6084115965700286, 0.745)\n",
      "epoch 224: (0.6322, 0.6104427736006683, 0.7307)\n",
      "epoch 225: (0.6328, 0.608372776236331, 0.7455)\n",
      "epoch 226: (0.63235, 0.6103744475022934, 0.7319)\n",
      "epoch 227: (0.63315, 0.6086495308037536, 0.7459)\n",
      "epoch 228: (0.6325, 0.6105087572977481, 0.732)\n",
      "epoch 229: (0.63325, 0.6085539714867617, 0.747)\n",
      "epoch 230: (0.6326, 0.6104079933388843, 0.7331)\n",
      "epoch 231: (0.63355, 0.6087098087098087, 0.7478)\n",
      "epoch 232: (0.63285, 0.6104965482824586, 0.734)\n",
      "epoch 233: (0.6337, 0.6087699316628702, 0.7483)\n",
      "epoch 234: (0.633, 0.6104834690147866, 0.7349)\n",
      "epoch 235: (0.6337, 0.608646188850967, 0.749)\n",
      "epoch 236: (0.63285, 0.6103496968186727, 0.7348)\n",
      "epoch 237: (0.6344, 0.6091440636673705, 0.7501)\n",
      "epoch 238: (0.633, 0.6103734439834025, 0.7355)\n",
      "epoch 239: (0.6344, 0.6091086215294691, 0.7503)\n",
      "epoch 240: (0.63305, 0.610332531719048, 0.736)\n",
      "epoch 241: (0.63465, 0.6092672238902864, 0.7508)\n",
      "epoch 242: (0.6331, 0.6102368726188504, 0.7368)\n",
      "epoch 243: (0.63495, 0.6093332253098922, 0.7521)\n",
      "epoch 244: (0.63295, 0.6100670585313354, 0.7369)\n",
      "epoch 245: (0.63505, 0.6093787964687778, 0.7524)\n",
      "epoch 246: (0.63295, 0.6099760112498966, 0.7374)\n",
      "epoch 247: (0.6351, 0.6093395921010035, 0.7529)\n",
      "epoch 248: (0.63295, 0.6099032818054063, 0.7378)\n",
      "epoch 249: (0.63525, 0.609363629012695, 0.7536)\n",
      "epoch 250: (0.63305, 0.6098951020071033, 0.7384)\n",
      "epoch 251: (0.63535, 0.6093914167946335, 0.754)\n",
      "epoch 252: (0.6333, 0.610056142668428, 0.7389)\n",
      "epoch 253: (0.6353, 0.6092891760904685, 0.7543)\n",
      "epoch 254: (0.6331, 0.6098910171730515, 0.7387)\n",
      "epoch 255: (0.6354, 0.6093522855758359, 0.7545)\n",
      "epoch 256: (0.6331, 0.6098184818481848, 0.7391)\n",
      "epoch 257: (0.6354, 0.6092464095530096, 0.7551)\n",
      "epoch 258: (0.6331, 0.6097460422163589, 0.7395)\n",
      "epoch 259: (0.63535, 0.6092325074650956, 0.7549)\n",
      "epoch 260: (0.6332, 0.6098103874690849, 0.7397)\n",
      "epoch 261: (0.63545, 0.6092603049124788, 0.7553)\n",
      "epoch 262: (0.63345, 0.6099892854199291, 0.7401)\n",
      "epoch 263: (0.6357, 0.6094531376028391, 0.7556)\n",
      "epoch 264: (0.6337, 0.6100592690154758, 0.7411)\n",
      "epoch 265: (0.63585, 0.6095299524308635, 0.756)\n",
      "epoch 266: (0.6338, 0.6101415870925255, 0.7412)\n",
      "epoch 267: (0.6358, 0.6094631629856521, 0.7561)\n",
      "epoch 268: (0.63395, 0.6101471918427761, 0.742)\n",
      "epoch 269: (0.63545, 0.6091018928715264, 0.7562)\n",
      "epoch 270: (0.63415, 0.6103297968582942, 0.7421)\n",
      "epoch 271: (0.6354, 0.6090177133655394, 0.7564)\n",
      "epoch 272: (0.63425, 0.6104120404638539, 0.7422)\n",
      "epoch 273: (0.63565, 0.6091399147155845, 0.7571)\n",
      "epoch 274: (0.63455, 0.6104951958610495, 0.7434)\n",
      "epoch 275: (0.6356, 0.6090032154340836, 0.7576)\n",
      "epoch 276: (0.63505, 0.6108693867498564, 0.7441)\n",
      "epoch 277: (0.63575, 0.6090273873584451, 0.7583)\n",
      "epoch 278: (0.63505, 0.6107784431137725, 0.7446)\n",
      "epoch 279: (0.63565, 0.6089295752027624, 0.7583)\n",
      "epoch 280: (0.63515, 0.6107696090484387, 0.7452)\n",
      "epoch 281: (0.63555, 0.6087444845567589, 0.7588)\n",
      "epoch 282: (0.63535, 0.6108790038502498, 0.7457)\n",
      "epoch 283: (0.6358, 0.6088315435165892, 0.7597)\n",
      "epoch 284: (0.6354, 0.6108745496233213, 0.746)\n",
      "epoch 285: (0.63595, 0.6088732281572836, 0.7603)\n",
      "epoch 286: (0.6355, 0.6109201047806155, 0.7463)\n",
      "epoch 287: (0.63585, 0.6087583059803058, 0.7604)\n",
      "epoch 288: (0.6357, 0.6109748119070985, 0.7471)\n",
      "epoch 289: (0.636, 0.6088348271446863, 0.7608)\n",
      "epoch 290: (0.6357, 0.610938521909745, 0.7473)\n",
      "epoch 291: (0.63605, 0.6088138846676797, 0.7612)\n",
      "epoch 292: (0.6355, 0.610756906980546, 0.7472)\n",
      "epoch 293: (0.6362, 0.6088903102014711, 0.7616)\n",
      "epoch 294: (0.6357, 0.6107935989549315, 0.7481)\n",
      "epoch 295: (0.63615, 0.6087894526568118, 0.7619)\n",
      "epoch 296: (0.6359, 0.6109025624285948, 0.7486)\n",
      "epoch 297: (0.63625, 0.6088867577719173, 0.7619)\n",
      "epoch 298: (0.63565, 0.6106173040854603, 0.7488)\n",
      "epoch 299: (0.6362, 0.6087685673215142, 0.7623)\n",
      "epoch 300: (0.6358, 0.6107486543793834, 0.7489)\n",
      "epoch 301: (0.6361, 0.6086887078741415, 0.7622)\n",
      "epoch 302: (0.6357, 0.6105949470252648, 0.7492)\n",
      "epoch 303: (0.63625, 0.6087824351297405, 0.7625)\n",
      "epoch 304: (0.63575, 0.6105906313645622, 0.7495)\n",
      "epoch 305: (0.6363, 0.6087442157332057, 0.763)\n",
      "epoch 306: (0.6358, 0.6106223525578365, 0.7496)\n",
      "epoch 307: (0.6363, 0.6087442157332057, 0.763)\n",
      "epoch 308: (0.63575, 0.6105546054238945, 0.7497)\n",
      "epoch 309: (0.6362, 0.608629765512841, 0.7631)\n",
      "epoch 310: (0.63585, 0.6105820105820106, 0.7501)\n",
      "epoch 311: (0.6361, 0.6084980867346939, 0.7633)\n",
      "epoch 312: (0.6357, 0.6104329427083334, 0.7501)\n",
      "epoch 313: (0.6361, 0.6084807906902598, 0.7634)\n",
      "epoch 314: (0.6359, 0.6105417276720352, 0.7506)\n",
      "epoch 315: (0.6361, 0.6084116616217938, 0.7638)\n",
      "epoch 316: (0.6362, 0.6107317073170732, 0.7512)\n",
      "epoch 317: (0.63625, 0.6084533948897556, 0.7644)\n",
      "epoch 318: (0.63595, 0.6104655886893637, 0.7513)\n",
      "epoch 319: (0.63615, 0.6083220622165646, 0.7646)\n",
      "epoch 320: (0.63565, 0.6101860125091382, 0.7512)\n",
      "epoch 321: (0.63615, 0.6082531605311282, 0.765)\n",
      "epoch 322: (0.63585, 0.6102947146220671, 0.7517)\n",
      "epoch 323: (0.63615, 0.6082015417626957, 0.7653)\n",
      "epoch 324: (0.6358, 0.6102630724261123, 0.7516)\n",
      "epoch 325: (0.63615, 0.6081843464441796, 0.7654)\n",
      "epoch 326: (0.63595, 0.6103579836025651, 0.7519)\n",
      "epoch 327: (0.6365, 0.6084021601016518, 0.7661)\n",
      "epoch 328: (0.6359, 0.6102905372504464, 0.752)\n",
      "epoch 329: (0.6364, 0.60828834550651, 0.7662)\n",
      "epoch 330: (0.6358, 0.6101914962674456, 0.752)\n",
      "epoch 331: (0.6365, 0.6083505318304493, 0.7664)\n",
      "epoch 332: (0.63585, 0.6102052405289202, 0.7522)\n",
      "epoch 333: (0.6365, 0.6083161402951912, 0.7666)\n",
      "epoch 334: (0.6357, 0.6100032425421531, 0.7525)\n",
      "epoch 335: (0.63685, 0.6085508051082732, 0.7672)\n",
      "epoch 336: (0.63585, 0.6101159114857745, 0.7527)\n",
      "epoch 337: (0.6369, 0.6085818527918782, 0.7673)\n",
      "epoch 338: (0.63575, 0.6099457358062688, 0.7531)\n",
      "epoch 339: (0.6371, 0.6086715282181357, 0.7679)\n",
      "epoch 340: (0.63575, 0.6099635479951397, 0.753)\n",
      "epoch 341: (0.63705, 0.6086232860426409, 0.7679)\n",
      "epoch 342: (0.6358, 0.6100307891751742, 0.7529)\n",
      "epoch 343: (0.63685, 0.6084647697550923, 0.7677)\n",
      "epoch 344: (0.63575, 0.6099635479951397, 0.753)\n",
      "epoch 345: (0.63685, 0.608413213974491, 0.768)\n",
      "epoch 346: (0.63585, 0.6100089076038545, 0.7533)\n",
      "epoch 347: (0.6369, 0.6084442332065906, 0.7681)\n",
      "epoch 348: (0.6359, 0.6100226683937824, 0.7535)\n",
      "epoch 349: (0.6369, 0.6084098828001268, 0.7683)\n",
      "epoch 350: (0.63585, 0.6100089076038545, 0.7533)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 351: (0.63685, 0.6083788706739527, 0.7682)\n",
      "epoch 352: (0.636, 0.610032362459547, 0.754)\n",
      "epoch 353: (0.6371, 0.6085338822039266, 0.7687)\n",
      "epoch 354: (0.63595, 0.6100542378369627, 0.7536)\n",
      "epoch 355: (0.63705, 0.6085544554455445, 0.7683)\n",
      "epoch 356: (0.6363, 0.6101859337105902, 0.7548)\n",
      "epoch 357: (0.6372, 0.6085614812470328, 0.7691)\n",
      "epoch 358: (0.6366, 0.6104999191069406, 0.7547)\n",
      "epoch 359: (0.6372, 0.6085786641342197, 0.769)\n",
      "epoch 360: (0.6367, 0.610491432266408, 0.7553)\n",
      "epoch 361: (0.63705, 0.6083999050858182, 0.7692)\n",
      "epoch 362: (0.6367, 0.6105271668822768, 0.7551)\n",
      "epoch 363: (0.637, 0.6083346512731298, 0.7693)\n",
      "epoch 364: (0.6365, 0.6102228682170543, 0.7557)\n",
      "epoch 365: (0.63705, 0.6082628959633463, 0.77)\n",
      "epoch 366: (0.6368, 0.6105008077544426, 0.7558)\n",
      "epoch 367: (0.6371, 0.6083109495970928, 0.77)\n",
      "epoch 368: (0.63665, 0.6102816560406746, 0.7562)\n",
      "epoch 369: (0.63715, 0.608273466487724, 0.7705)\n",
      "epoch 370: (0.6368, 0.6104294478527608, 0.7562)\n",
      "epoch 371: (0.63705, 0.6081945211968106, 0.7704)\n",
      "epoch 372: (0.63675, 0.6103089457126725, 0.7566)\n",
      "epoch 373: (0.6372, 0.6083214906047687, 0.7705)\n",
      "epoch 374: (0.63695, 0.6104881000403388, 0.7567)\n",
      "epoch 375: (0.6373, 0.6084175615919141, 0.7705)\n",
      "epoch 376: (0.6369, 0.6103854217061764, 0.757)\n",
      "epoch 377: (0.6373, 0.6083833280707294, 0.7707)\n",
      "epoch 378: (0.63665, 0.6101571946795646, 0.7569)\n",
      "epoch 379: (0.63745, 0.6084760476679031, 0.771)\n",
      "epoch 380: (0.63685, 0.6102650874224478, 0.7574)\n",
      "epoch 381: (0.63735, 0.6083629191321499, 0.7711)\n",
      "epoch 382: (0.63685, 0.6102473213566423, 0.7575)\n",
      "epoch 383: (0.63755, 0.6084693636148568, 0.7716)\n",
      "epoch 384: (0.6368, 0.6101804123711341, 0.7576)\n",
      "epoch 385: (0.6374, 0.6083425327235452, 0.7715)\n",
      "epoch 386: (0.63685, 0.6101763143064165, 0.7579)\n",
      "epoch 387: (0.6374, 0.608274231678487, 0.7719)\n",
      "epoch 388: (0.63675, 0.6100603621730383, 0.758)\n",
      "epoch 389: (0.63745, 0.6082198252106134, 0.7725)\n",
      "epoch 390: (0.6369, 0.6102254428341385, 0.7579)\n",
      "epoch 391: (0.63725, 0.6080793763288448, 0.7722)\n",
      "epoch 392: (0.6372, 0.6104492030268878, 0.7583)\n",
      "epoch 393: (0.63705, 0.607955888144939, 0.7718)\n",
      "epoch 394: (0.6371, 0.6102976669348351, 0.7586)\n",
      "epoch 395: (0.63715, 0.6080006299708638, 0.7721)\n",
      "epoch 396: (0.63695, 0.6102390726877566, 0.7581)\n",
      "epoch 397: (0.63735, 0.6081240651814532, 0.7725)\n",
      "epoch 398: (0.63705, 0.6103018108651912, 0.7583)\n",
      "epoch 399: (0.63745, 0.608202786743289, 0.7726)\n",
      "epoch 400: (0.6371, 0.6103864734299517, 0.7581)\n",
      "epoch 401: (0.63745, 0.6082198252106134, 0.7725)\n",
      "epoch 402: (0.6371, 0.6102976669348351, 0.7586)\n",
      "epoch 403: (0.6374, 0.6081549118387909, 0.7726)\n",
      "epoch 404: (0.63705, 0.6102840589039994, 0.7584)\n",
      "epoch 405: (0.6376, 0.6082952935620967, 0.7729)\n",
      "epoch 406: (0.63695, 0.6102213279678068, 0.7582)\n",
      "epoch 407: (0.6376, 0.6082782499213094, 0.773)\n",
      "epoch 408: (0.63715, 0.6102935263369521, 0.7589)\n",
      "epoch 409: (0.63775, 0.6083536537402658, 0.7734)\n",
      "epoch 410: (0.6372, 0.6102893890675242, 0.7592)\n",
      "epoch 411: (0.63755, 0.6081282918009591, 0.7736)\n",
      "epoch 412: (0.63725, 0.6103029815960781, 0.7594)\n",
      "epoch 413: (0.6374, 0.6079170593779454, 0.774)\n",
      "epoch 414: (0.6371, 0.6101912875743449, 0.7592)\n",
      "epoch 415: (0.6372, 0.6077599748664781, 0.7738)\n",
      "epoch 416: (0.63725, 0.6102675343456254, 0.7596)\n",
      "epoch 417: (0.6374, 0.6078831658291457, 0.7742)\n",
      "epoch 418: (0.63715, 0.6101871937012935, 0.7595)\n",
      "epoch 419: (0.6375, 0.6079447322970639, 0.7744)\n",
      "epoch 420: (0.6372, 0.6102008032128514, 0.7597)\n",
      "epoch 421: (0.63745, 0.6078462142016477, 0.7747)\n",
      "epoch 422: (0.63715, 0.6101517950365433, 0.7597)\n",
      "epoch 423: (0.63735, 0.6077846660911873, 0.7745)\n",
      "epoch 424: (0.63715, 0.6100987396644457, 0.76)\n",
      "epoch 425: (0.63725, 0.607672393504354, 0.7746)\n",
      "epoch 426: (0.637, 0.6099165596919127, 0.7602)\n",
      "epoch 427: (0.6373, 0.6077200690412679, 0.7746)\n",
      "epoch 428: (0.6369, 0.6098363286264442, 0.7601)\n",
      "epoch 429: (0.6372, 0.6076416130550761, 0.7745)\n",
      "epoch 430: (0.6369, 0.609818706882721, 0.7602)\n",
      "epoch 431: (0.63715, 0.6075601913575406, 0.7747)\n",
      "epoch 432: (0.63675, 0.6096719865265859, 0.7602)\n",
      "epoch 433: (0.6371, 0.607495687627411, 0.7748)\n",
      "epoch 434: (0.63665, 0.6095917876333307, 0.7601)\n",
      "epoch 435: (0.63725, 0.6076048608388868, 0.775)\n",
      "epoch 436: (0.6366, 0.6094902212247515, 0.7604)\n",
      "epoch 437: (0.6372, 0.6075403668286565, 0.7751)\n",
      "epoch 438: (0.6368, 0.6096681096681097, 0.7605)\n",
      "epoch 439: (0.63715, 0.6075433231396534, 0.7748)\n",
      "epoch 440: (0.6369, 0.6097306829111895, 0.7607)\n",
      "epoch 441: (0.6372, 0.6075572279711509, 0.775)\n",
      "epoch 442: (0.6369, 0.6097130950472832, 0.7608)\n",
      "epoch 443: (0.63725, 0.6076048608388868, 0.775)\n",
      "epoch 444: (0.63675, 0.6095489866218057, 0.7609)\n",
      "epoch 445: (0.63725, 0.607587990906953, 0.7751)\n",
      "epoch 446: (0.63685, 0.6095939777368463, 0.7612)\n",
      "epoch 447: (0.6371, 0.6074451410658307, 0.7751)\n",
      "epoch 448: (0.63665, 0.6094688776736361, 0.7608)\n",
      "epoch 449: (0.637, 0.6073499451496631, 0.7751)\n",
      "epoch 450: (0.6366, 0.6094726719025485, 0.7605)\n",
      "epoch 451: (0.6371, 0.60742830277386, 0.7752)\n",
      "epoch 452: (0.6368, 0.6095802627363025, 0.761)\n",
      "epoch 453: (0.637, 0.6073163089456368, 0.7753)\n",
      "epoch 454: (0.6366, 0.6094551282051283, 0.7606)\n",
      "epoch 455: (0.63695, 0.6072687397195896, 0.7753)\n",
      "epoch 456: (0.6364, 0.6093299134337928, 0.7602)\n",
      "epoch 457: (0.6371, 0.60739464201786, 0.7754)\n",
      "epoch 458: (0.6367, 0.6095703751202308, 0.7605)\n",
      "epoch 459: (0.63705, 0.6073134445227468, 0.7756)\n",
      "epoch 460: (0.6368, 0.6095978208620413, 0.7609)\n",
      "epoch 461: (0.6372, 0.6074056677626428, 0.7759)\n",
      "epoch 462: (0.6367, 0.6095703751202308, 0.7605)\n",
      "epoch 463: (0.63715, 0.6073749314961246, 0.7758)\n",
      "epoch 464: (0.6368, 0.6095978208620413, 0.7609)\n",
      "epoch 465: (0.6373, 0.6074671258609894, 0.7761)\n",
      "epoch 466: (0.63675, 0.6095841012901675, 0.7607)\n",
      "epoch 467: (0.63735, 0.6074978476950771, 0.7762)\n",
      "epoch 468: (0.63675, 0.6095665411425366, 0.7608)\n",
      "epoch 469: (0.6375, 0.6075899843505478, 0.7765)\n",
      "epoch 470: (0.6368, 0.6096153846153847, 0.7608)\n",
      "epoch 471: (0.63735, 0.6074810235542687, 0.7763)\n",
      "epoch 472: (0.6371, 0.6098029793368572, 0.7614)\n",
      "epoch 473: (0.63755, 0.6075701884726675, 0.7769)\n",
      "epoch 474: (0.6369, 0.6096779362281686, 0.761)\n",
      "epoch 475: (0.6375, 0.6075226775101658, 0.7769)\n",
      "epoch 476: (0.6371, 0.6098205703300225, 0.7613)\n",
      "epoch 477: (0.6374, 0.6074276778733385, 0.7769)\n",
      "epoch 478: (0.637, 0.6097580515942957, 0.7611)\n",
      "epoch 479: (0.63745, 0.607491984046297, 0.7768)\n",
      "epoch 480: (0.63705, 0.6097365681799984, 0.7615)\n",
      "epoch 481: (0.6375, 0.6074890556597874, 0.7771)\n",
      "epoch 482: (0.6372, 0.6098830690373218, 0.7615)\n",
      "epoch 483: (0.63765, 0.6076147291064029, 0.7772)\n",
      "epoch 484: (0.63715, 0.6097990553198303, 0.7617)\n",
      "epoch 485: (0.6375, 0.6074554548296343, 0.7773)\n",
      "epoch 486: (0.6371, 0.6097853939782191, 0.7615)\n",
      "epoch 487: (0.6375, 0.6074890556597874, 0.7771)\n",
      "epoch 488: (0.63705, 0.6096487719017522, 0.762)\n",
      "epoch 489: (0.6375, 0.6074890556597874, 0.7771)\n",
      "epoch 490: (0.63725, 0.6098263583259982, 0.7621)\n",
      "epoch 491: (0.63765, 0.6076147291064029, 0.7772)\n",
      "epoch 492: (0.6372, 0.6097775644103056, 0.7621)\n",
      "epoch 493: (0.6378, 0.6077235772357723, 0.7774)\n",
      "epoch 494: (0.63695, 0.6095863007121709, 0.7618)\n",
      "epoch 495: (0.63775, 0.6076929090766946, 0.7773)\n",
      "epoch 496: (0.6371, 0.6097151088348272, 0.7619)\n",
      "epoch 497: (0.6378, 0.6077235772357723, 0.7774)\n",
      "epoch 498: (0.63715, 0.6097639055622249, 0.7619)\n",
      "epoch 499: (0.6377, 0.6076454033771107, 0.7773)\n"
     ]
    }
   ],
   "source": [
    "# attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)   \n",
    "# attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)\n",
    "attack_train_args = Train_args(learning_rate=0.001, weight_decay=1e-5, epoch=500)\n",
    "attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "for epoch in range(attack_train_args.epoch):\n",
    "    attack_model = attack.train_attack_model(attack_model, attack_train_data, attack_train_target, attack_train_args)\n",
    "    print(f'epoch {epoch}: {attack_evaluation(attack_model, attack_train_data, attack_train_target)}' )        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba78e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = attack_evaluation(attack_model, attack_train_data, attack_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c46ca467",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = attack.mi_attack_test(model, attack_model, x_target_train, y_target_train, x_target_test, y_target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2392f52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6377, 0.6076454033771107, 0.7773)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5247, 0.5145208700764257, 0.8752)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_res)\n",
    "test_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b459d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_path = f'mia/shms{number_of_sms}_shtrsize{shadow_size}_shlr{shadow_model.alpha}_shiter{int(shadow_model.max_iter/shadow_batch_size)}_shreg{shadow_model.lambda_}/'      \n",
    "os.mkdir(sh_path)\n",
    "\n",
    "torch.save(attack_train_data, sh_path+'attack_train_data.pt')\n",
    "torch.save(attack_train_target, sh_path+'attack_train_target.pt')\n",
    "\n",
    "at_path = sh_path+f'attack_model_aneur{attack_model.h_neurons}_ado{attack_model.do}_alr{attack_train_args.learning_rate}_alreg{attack_train_args.weight_decay}_aepoch{attack_train_args.epoch}'\n",
    "torch.save(attack_model, at_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6a77dac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.59217, 0.5930014328093153, 0.5877)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)\n",
    "train_acc, train_pre, train_rec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "61f91b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6260389215487533, 0.5935737236701178, 0.8313)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, test_pre, test_rec = attack_evaluation(attack_model, attack_test_data, attack_test_target)\n",
    "test_acc, test_pre, test_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc286901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ffcaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac16b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "# random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "\n",
    "epo = [200]\n",
    "wd = [1e-5, 1e-6]\n",
    "ams = {}\n",
    "for ep in epo:\n",
    "    for w_d in wd:\n",
    "        attack_train_args = Train_args(learning_rate=l_r, weight_decay=w_d, epoch=ep)\n",
    "        attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "        for epoch in range(attack_train_args.epoch):\n",
    "\n",
    "            attack_model = train_model(attack_model, attack_train_data, attack_train_target, attack_train_args)\n",
    "            ams[(l_r,w_d)] = attack_model\n",
    "        at_path = sh_path+f'attack_model_aneur{attack_model.h_neurons}_ado{attack_model.do}_alr{attack_train_args.learning_rate}_alreg{attack_train_args.weight_decay}_aepoch{attack_train_args.epoch}'\n",
    "        torch.save(attack_model, at_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "df4d74e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 1e-05\n",
      "Train:  0.8081 0.8207236842105263 0.78842\n",
      "Test:  0.6895398337725522 0.6209717711716213 0.9943\n",
      "200 1e-06\n",
      "Train:  0.80328 0.8273606493674712 0.7665\n",
      "Test:  0.7008919521589296 0.6304929308368361 0.99\n",
      "500 1e-05\n",
      "Train:  0.8081 0.8207236842105263 0.78842\n",
      "Test:  0.6895398337725522 0.6209717711716213 0.9943\n",
      "500 1e-06\n",
      "Train:  0.80328 0.8273606493674712 0.7665\n",
      "Test:  0.7008919521589296 0.6304929308368361 0.99\n"
     ]
    }
   ],
   "source": [
    "epo = [200,500]\n",
    "wd = [1e-5, 1e-6]\n",
    "\n",
    "for ep in epo:\n",
    "    for w_d in wd:\n",
    "        print(ep, w_d)\n",
    "        attack_train_args = Train_args(learning_rate=l_r, weight_decay=w_d, epoch=500)\n",
    "        attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "        at_path = sh_path+f'attack_model_aneur{attack_model.h_neurons}_ado{attack_model.do}_alr{attack_train_args.learning_rate}_alreg{attack_train_args.weight_decay}_aepoch{attack_train_args.epoch}'\n",
    "        attack_model = torch.load(at_path)\n",
    "        \n",
    "        train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)\n",
    "        print('Train: ', train_acc, train_pre, train_rec)\n",
    "        test_acc, test_pre, test_rec = attack_evaluation(attack_model, attack_test_data, attack_test_target)\n",
    "        print('Test: ' , test_acc, test_pre, test_rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l_r in lr:\n",
    "    for w_d in wd:\n",
    "        ams[(l_r,w_d)]\n",
    "        train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)\n",
    "        print(\"Train acc pre rec: \", train_acc, train_pre, train_rec)\n",
    "        test_acc, test_pre, test_rec = attack_evaluation(attack_model, attack_test_data, attack_test_target)\n",
    "        print(\"Test acc pre rec: \", test_acc, test_pre, test_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf9f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_size = 20000\n",
    "shadow_clf = LogisticRegression(random_state=1).fit(x_shadow[:shadow_size], y_shadow[:shadow_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eca16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shadow_clf.score(x_shadow[:shadow_size], y_shadow[:shadow_size]))\n",
    "print(shadow_clf.score(x_shadow[shadow_size:shadow_size*2], y_shadow[shadow_size:shadow_size*2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee911ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_train_pred = shadow_clf.predict_proba(x_shadow[:shadow_size])\n",
    "shadow_test_pred = shadow_clf.predict_proba(x_shadow[shadow_size:shadow_size*2])\n",
    "y_shadow_train = y_shadow[:shadow_size]\n",
    "y_shadow_test = y_shadow[shadow_size:shadow_size*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ab2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# members\n",
    "labels = np.ones(shadow_train_pred.shape[0])\n",
    "# non-members\n",
    "test_labels = np.zeros(shadow_test_pred.shape[0])\n",
    "\n",
    "x_1 = np.concatenate((shadow_train_pred, shadow_test_pred))\n",
    "x_2 = np.concatenate((y_shadow_train, y_shadow_test)).reshape((-1, 1))\n",
    "y_new = np.concatenate((labels, test_labels))\n",
    "\n",
    "attack_train_data = np.concatenate((x_1,x_2),axis=1)\n",
    "attack_train_target = y_new\n",
    "df = pd.DataFrame(attack_train_data)\n",
    "df['a_target'] = attack_train_target\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "attack_train_data = np.array(df.drop(['a_target'], axis=1))\n",
    "attack_train_target = np.array(df['a_target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b1987",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)   \n",
    "attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)\n",
    "attack_train_args = Train_args(learning_rate=0.001, weight_decay=1e-3, epoch=200)\n",
    "attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86acb25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49914cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_train_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)   \n",
    "attack_train_target = torch.tensor(np.array(df['a_target']), dtype=torch.float)\n",
    "attack_train_args = Train_args(learning_rate=0.001, weight_decay=1e-3, epoch=500)\n",
    "attack_model = Net_attack(h_neurons=64, do=0, input_size=attack_train_data.shape[1])\n",
    "for epoch in range(attack_train_args.epoch):\n",
    "            \n",
    "    attack_model = train_model(attack_model, attack_train_data, attack_train_target, attack_train_args)\n",
    "    train_acc, train_pre, train_rec = attack_evaluation(attack_model, attack_train_data, attack_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, train_pre, train_rec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2234ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(x_target_train, y_target_train)\n",
    "test_pred = model.predict(x_target_test, y_target_test)\n",
    "\n",
    "y_target_train_ohe = OneHotEncoder(sparse=False).fit_transform(y_target_train.reshape(-1,1)) #encoode the target values\n",
    "y_target_test_ohe = OneHotEncoder(sparse=False).fit_transform(y_target_test.reshape(-1,1)) #encoode the target values\n",
    "    \n",
    "# members\n",
    "labels = np.ones(train_pred.shape[0])\n",
    "# non-members\n",
    "test_labels = np.zeros(test_pred.shape[0])\n",
    "\n",
    "x_1 = np.concatenate((train_pred, test_pred))\n",
    "x_2 = np.concatenate((y_target_train_ohe, y_target_test_ohe))#.reshape((-1, 1))\n",
    "y_new = np.concatenate((labels, test_labels))\n",
    "\n",
    "attack_test_data = np.concatenate((x_1,x_2),axis=1)\n",
    "attack_test_target = y_new\n",
    "df = pd.DataFrame(attack_test_data)\n",
    "df['a_target'] = attack_test_target\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "attack_test_data = torch.tensor(np.array(df.drop(['a_target'], axis=1)), dtype=torch.float, requires_grad=True)\n",
    "attack_test_target = torch.tensor(np.array(df['a_target']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_pre, test_rec = attack_evaluation(a_model, attack_test_data, attack_test_target)\n",
    "test_acc, test_pre, test_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2bcabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_model = Net_attack(h_neurons=64, do=0, input_size=100)\n",
    "a_model = torch.load('attack_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41bca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "# random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "\n",
    "lr = [0.001]\n",
    "wd = [1e-4, 1e-6]\n",
    "tms = {}\n",
    "for l_r in lr:\n",
    "    for w_d in wd:\n",
    "        model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "        model.n_classes      = 100\n",
    "        model.alpha          = 0.001\n",
    "        model.max_iter       = 100*X_train_size \n",
    "        model.lambda_        = 1e-3\n",
    "        model.tolerance      = 10e-5\n",
    "        model.DP             = True\n",
    "        model.epsilon\n",
    "\n",
    "        X,y = model.init_theta(x_target_train, y_target_train)\n",
    "        model.SGD(X,y)\n",
    "        model.evaluate(x_target_train, y_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import algo\n",
    "# import attack\n",
    "\n",
    "# from torch import nn,optim\n",
    "import torch\n",
    "\n",
    "rand_seed=42\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "        \n",
    "\n",
    "raw_data_path = '../datasets/dataset_purchase'\n",
    "raw_data = pd.read_csv(raw_data_path)\n",
    "y=raw_data['63']\n",
    "X=raw_data.drop('63', axis=1)\n",
    "y =  y.replace(100, 0)\n",
    "print('Dataset: ', raw_data_path)\n",
    "print('Classes in classification task: ', y.nunique())\n",
    "n_classes = y.nunique()\n",
    "\n",
    "X_train, x_shadow, y_train, y_shadow = train_test_split(X, y, train_size=0.2, random_state=rand_seed)\n",
    "print(X_train.shape, x_shadow.shape)\n",
    "\n",
    "#Target model\n",
    "X_train_size = 10000\n",
    "X_test_size = 10000\n",
    "x_target_train = np.array(X_train[:X_train_size])\n",
    "y_target_train = np.array(y_train[:X_train_size])\n",
    "x_target_test = np.array(X_train[X_train_size:X_train_size+X_test_size])\n",
    "y_target_test = np.array(y_train[X_train_size:X_train_size+X_test_size])\n",
    "if y_target_test.shape[0]<X_test_size or y_target_train.shape[0]<X_train_size:\n",
    "    raise ValueError(\n",
    "            \"Not enough traning or test data for the target model\")\n",
    "\n",
    "for L in [1,10,100]:\n",
    "    for epsilon in np.arange(0,1,0.1):\n",
    "        model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "        model.n_classes      = n_classes\n",
    "        model.alpha          = 0.001\n",
    "        model.max_iter       = 100*X_train_size\n",
    "        model.lambda_        = 1e-3\n",
    "        model.tolerance      = 10e-5\n",
    "        model.DP             = True\n",
    "        model.L              = L\n",
    "        model.epsilon        = epsilon\n",
    "\n",
    "\n",
    "        X,y = model.init_theta(x_target_train, y_target_train)\n",
    "        model.train(X,y)\n",
    "        model.evaluate(x_target_train, y_target_train, acc=True)\n",
    "        model.evaluate(x_target_test, y_target_test, acc=True)\n",
    "\n",
    "        tm_path = f'tm/lr{model.alpha}_iter{int(model.max_iter/X_train_size)}_reg{model.lambda_}_DP{model.DP}'\n",
    "        if model.DP:\n",
    "            tm_path += f'_eps{model.epsilon}_L{model.L}'\n",
    "        np.save(tm_path+'_target_model', model.theta)\n",
    "\n",
    "        print(tm_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee18d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shadow models\n",
    "# s_ms = {}\n",
    "# number_of_sms = 10\n",
    "# shadow_size = 50000\n",
    "# shadow_batch_size = int(shadow_size/number_of_sms)\n",
    "\n",
    "# x_shadow_train = np.array(x_shadow[:shadow_size])\n",
    "# y_shadow_train = np.array(y_shadow[:shadow_size])\n",
    "# x_shadow_test = np.array(x_shadow[shadow_size:2*shadow_size])\n",
    "# y_shadow_test = np.array(y_shadow[shadow_size:2*shadow_size])\n",
    "\n",
    "# attack.train_shadow_models(number_of_sms,)\n",
    "\n",
    "# for i in range(number_of_sms):  \n",
    "#     batch_start = i*shadow_batch_size\n",
    "#     batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "#     shadow_model = algo.LogisticRegression_DPSGD()\n",
    "\n",
    "#     shadow_model.n_classes      = n_classes\n",
    "#     shadow_model.alpha          = 0.001\n",
    "#     shadow_model.max_iter       = 100*shadow_batch_size\n",
    "#     shadow_model.lambda_        = 10e-3\n",
    "#     shadow_model.tolerance      = 10e-5\n",
    "#     shadow_model.DP             = False\n",
    "\n",
    "#     X,y = shadow_model.init_theta(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end] )\n",
    "#     shadow_model.SGD(X,y)\n",
    "#     print('Shadow model: ', i)\n",
    "#     shadow_model.evaluate(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end])\n",
    "#     shadow_model.evaluate(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end])\n",
    "#     s_ms[i] = shadow_model\n",
    "\n",
    "# #Attack model\n",
    "\n",
    "# shadow_train_pred = []\n",
    "# shadow_test_pred = []\n",
    "\n",
    "# for i in range(number_of_sms): \n",
    "#     batch_start = i*shadow_batch_size\n",
    "#     batch_end = (i+1)*shadow_batch_size\n",
    "    \n",
    "#     train_prediciton = s_ms[i].predict(x_shadow_train[batch_start:batch_end], y_shadow_train[batch_start:batch_end])\n",
    "#     test_prediciton = s_ms[i].predict(x_shadow_test[batch_start:batch_end], y_shadow_test[batch_start:batch_end])\n",
    "    \n",
    "#     shadow_train_pred.append(train_prediciton)\n",
    "#     shadow_test_pred.append(test_prediciton)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f530940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/10000 == 1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7622903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0,1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca6fe0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
